{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use `AutoRater` to Evaluate Answer Completeness and Accuracy for Given Questions\n",
    "\n",
    "In this example, we will show you how to use AutoRater to verify the correctness of an answer to a given question and context pairs.\n",
    "\n",
    "### Before running the code\n",
    "\n",
    "You will need to `uniflow` conda environment to run this notebook. You can set up the environment following the instruction: https://github.com/CambioML/uniflow/tree/main#installation.\n",
    "\n",
    "Next, you will need a valid [OpenAI API key](https://platform.openai.com/api-keys) to run the code. Once you have the key, set it as the environment variable `OPENAI_API_KEY` within a `.env` file in the root directory of this repository. For more details, see this [instruction](https://github.com/CambioML/uniflow/tree/main#api-keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the dependency\n",
    "First, we set system paths and import libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayn/miniconda3/envs/uniflow/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\".\")\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "import pprint\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display\n",
    "\n",
    "from uniflow.flow.client import RaterClient\n",
    "from uniflow.flow.config  import RaterClassificationConfig\n",
    "from uniflow.op.model.model_config  import OpenAIModelConfig\n",
    "from uniflow.op.prompt_schema import Context\n",
    "from uniflow.op.op import OpScope\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the input data\n",
    "\n",
    "We use three example raw inputs. Each one is a tuple consisting of context, question, and answer to be labeled. The ground truth label of the first one is 'correct', and the others are 'incorrect'. Then, we use the `Context` class to wrap them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_input = [\n",
    "    (\"The Pacific Ocean is the largest and deepest of Earth's oceanic divisions. It extends from the Arctic Ocean in the north to the Southern Ocean in the south.\",\n",
    "     \"What is the largest ocean on Earth?\",\n",
    "     \"The largest ocean on Earth is the Pacific Ocean.\"), # correct\n",
    "    (\"Shakespeare, a renowned English playwright and poet, wrote 39 plays during his lifetime. His works include famous plays like 'Hamlet' and 'Romeo and Juliet'.\",\n",
    "     \"How many plays did Shakespeare write?\",\n",
    "     \"Shakespeare wrote 31 plays.\"), # incorrect\n",
    "    (\"The human brain is an intricate organ responsible for intelligence, memory, and emotions. It is made up of approximately 86 billion neurons.\",\n",
    "     \"What is the human brain responsible for?\",\n",
    "     \"The human brain is responsible for physical movement.\"), # incorrect\n",
    "]\n",
    "\n",
    "data = [\n",
    "    Context(context=c[0], question=c[1], answer=c[2])\n",
    "    for c in raw_input\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the config: JSON format\n",
    "\n",
    "In this example, we will use the [`OpenAIModelConfig`](https://github.com/CambioML/uniflow/blob/main/uniflow/model/config.py#L17) as the default LLM to generate questions and answers. If you want to use open-source models, you can replace the `OpenAIConfig` and `OpenAIModelConfig` with `HuggingfaceConfig` and [`HuggingfaceModelConfig`](https://github.com/CambioML/uniflow/blob/main/uniflow/model/config.py#L27).\n",
    "\n",
    "We use the default `guided_prompt` in `RaterClassificationConfig`, which includes two examples, labeled 'Yes' and 'No'. The default examples are also encapsulated within the `Context` class, which has fields for context, question, answer (and label), aligning with the input data format.\n",
    "\n",
    "The response format is JSON, enabling the model to return a JSON object as output rather than plain text. This facilitates more convenient processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RaterConfig(flow_name='RaterFlow', model_config={'model_name': 'gpt-3.5-turbo-1106', 'model_server': 'OpenAIModelServer', 'num_call': 3, 'temperature': 0.9, 'response_format': {'type': 'json_object'}}, label2score={'Yes': 1.0, 'No': 0.0}, guided_prompt_template=GuidedPrompt(instruction='\\n        Task: Answer Evaluation\\n        Objective:\\n        You are required to evaluate whether a given answer is appropriate in relation to a specific context and question. \\n        Input:\\n        1. Context: This is a brief text, usually a couple of sentences or a paragraph, providing key information or facts.\\n        2. Question: This is a query related to the information given in the context. It is designed to test knowledge that can be inferred or directly obtained from the context.\\n        3. Answer: This is a response to the question provided.\\n        Evaluation Criteria:\\n        Based on these, you need to judge if the answer is correct or incorrect in relation to the context and the question. Use the following labels to categorize your judgment:\\n        1. Yes: The answer is good.\\n        2. No: The answer is not bad.\\n        Response Format:\\n        Your response should include:\\n        1. label: Your judgment (one of the two labels mentioned above).\\n        2. explanatoin: A clear and concise thought for your judgment, detailing why you think the answer is good or bad.\\n        Note: Only use the example below as a few shot demonstrate but not including them in the final response.\\n        ', examples=[Context(context='The Eiffel Tower, located in Paris, France, is one of the most famous landmarks in the world. It was constructed in 1889 and stands at a height of 324 meters.', question='When was the Eiffel Tower constructed?', answer='The Eiffel Tower was constructed in 1889.', explanation='The context explicitly mentions that the Eiffel Tower was constructed in 1889, so the answer is correct.', label='Yes'), Context(context='Photosynthesis is a process used by plants to convert light energy into chemical energy. This process primarily occurs in the chloroplasts of plant cells.', question='Where does photosynthesis primarily occur in plant cells?', answer='Photosynthesis primarily occurs in the mitochondria of plant cells.', explanation='The context mentions that photosynthesis primarily occurs in the chloroplasts of plant cells, so the answer is incorrect.', label='No')]), num_thread=1)\n"
     ]
    }
   ],
   "source": [
    "config = RaterClassificationConfig(\n",
    "    flow_name=\"RaterFlow\",\n",
    "    model_config=OpenAIModelConfig(num_call=3, response_format={\"type\": \"json_object\"}),\n",
    "    label2score={\"Yes\": 1.0, \"No\": 0.0})\n",
    "\n",
    "with OpScope(name=\"JSONFlow\"):\n",
    "    client = RaterClient(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the client\n",
    "\n",
    "Then we can run the client. For each item in the `raw_input`, the Client will generate an explanation and a final label, either `Yes` or `No`. The label is determined by taking the majority vote from three samples of the LLM's output, which improves stability and self-consistency compared to generating a single output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:14<00:00,  4.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'output': [{'average_score': 1.0,\n",
      "              'error': 'No errors.',\n",
      "              'majority_vote': 'yes',\n",
      "              'response': [{'answer': 'The largest ocean on Earth is the '\n",
      "                                      'Pacific Ocean.',\n",
      "                            'context': 'The Pacific Ocean is the largest and '\n",
      "                                       \"deepest of Earth's oceanic divisions. \"\n",
      "                                       'It extends from the Arctic Ocean in '\n",
      "                                       'the north to the Southern Ocean in the '\n",
      "                                       'south.',\n",
      "                            'explanation': 'The answer correctly identifies '\n",
      "                                           'the Pacific Ocean as the largest '\n",
      "                                           'ocean on Earth, which is '\n",
      "                                           'consistent with the information '\n",
      "                                           'provided in the context. '\n",
      "                                           'Therefore, the answer is good.',\n",
      "                            'label': 'Yes',\n",
      "                            'question': 'What is the largest ocean on Earth?'},\n",
      "                           {'explanation': 'The answer correctly identifies '\n",
      "                                           'the Pacific Ocean as the largest '\n",
      "                                           'ocean on Earth, which is '\n",
      "                                           'explicitly mentioned in the '\n",
      "                                           'context.',\n",
      "                            'label': 'Yes'},\n",
      "                           {'explanation': 'The answer correctly identifies '\n",
      "                                           'the Pacific Ocean as the largest '\n",
      "                                           'ocean on Earth, which is '\n",
      "                                           'consistent with the information '\n",
      "                                           'provided in the context.',\n",
      "                            'label': 'Yes'}],\n",
      "              'scores': [1.0, 1.0, 1.0],\n",
      "              'votes': ['yes', 'yes', 'yes']}],\n",
      "  'root': <uniflow.node.Node object at 0x7fa09e6aef20>},\n",
      " {'output': [{'average_score': 0.0,\n",
      "              'error': 'No errors.',\n",
      "              'majority_vote': 'no',\n",
      "              'response': [{'explanation': 'The context explicitly states that '\n",
      "                                           'Shakespeare wrote 39 plays, so the '\n",
      "                                           'answer is incorrect.',\n",
      "                            'label': 'No'},\n",
      "                           {'explanation': 'The context explicitly mentions '\n",
      "                                           'that Shakespeare wrote 39 plays, '\n",
      "                                           'so the answer is incorrect.',\n",
      "                            'label': 'No'},\n",
      "                           {'explanation': 'The context explicitly states that '\n",
      "                                           'Shakespeare wrote 39 plays, so the '\n",
      "                                           'answer is incorrect.',\n",
      "                            'label': 'No'}],\n",
      "              'scores': [0.0, 0.0, 0.0],\n",
      "              'votes': ['no', 'no', 'no']}],\n",
      "  'root': <uniflow.node.Node object at 0x7fa0c0a57880>},\n",
      " {'output': [{'average_score': 0.0,\n",
      "              'error': 'No errors.',\n",
      "              'majority_vote': 'no',\n",
      "              'response': [{'explanation': 'The context explicitly mentions '\n",
      "                                           'that the human brain is '\n",
      "                                           'responsible for intelligence, '\n",
      "                                           'memory, and emotions, not physical '\n",
      "                                           'movement. Therefore, the answer is '\n",
      "                                           'incorrect.',\n",
      "                            'label': 'No'},\n",
      "                           {'explanation': 'The context clearly states that '\n",
      "                                           'the human brain is responsible for '\n",
      "                                           'intelligence, memory, and '\n",
      "                                           'emotions, not just physical '\n",
      "                                           'movement.',\n",
      "                            'label': 'No'},\n",
      "                           {'explanation': 'The context mentions that the '\n",
      "                                           'human brain is responsible for '\n",
      "                                           'intelligence, memory, and '\n",
      "                                           'emotions, not just physical '\n",
      "                                           'movement. The answer is incorrect.',\n",
      "                            'label': 'No'}],\n",
      "              'scores': [0.0, 0.0, 0.0],\n",
      "              'votes': ['no', 'no', 'no']}],\n",
      "  'root': <uniflow.node.Node object at 0x7fa09e6d8130>}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output = client.run(data)\n",
    "pprint.pprint(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that model response is a JSON object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'The largest ocean on Earth is the Pacific Ocean.',\n",
      " 'context': \"The Pacific Ocean is the largest and deepest of Earth's oceanic \"\n",
      "            'divisions. It extends from the Arctic Ocean in the north to the '\n",
      "            'Southern Ocean in the south.',\n",
      " 'explanation': 'The answer correctly identifies the Pacific Ocean as the '\n",
      "                'largest ocean on Earth, which is consistent with the '\n",
      "                'information provided in the context. Therefore, the answer is '\n",
      "                'good.',\n",
      " 'label': 'Yes',\n",
      " 'question': 'What is the largest ocean on Earth?'}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(output[0][\"output\"][0][\"response\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model's responses can be distilled into majority votes, as shown below. Given the non-deterministic nature of the LLM (where each inference could yield a different output), we've enhanced stability and self-consistency by averaging results from three LLM output samplings, a notable improvement over single-output scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data 0 has majority vote \u001b[31myes\u001b[0m and average score \u001b[34m1.0\u001b[0m\n",
      "data 1 has majority vote \u001b[31mno\u001b[0m and average score \u001b[34m0.0\u001b[0m\n",
      "data 2 has majority vote \u001b[31mno\u001b[0m and average score \u001b[34m0.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "for idx, o in enumerate(output):\n",
    "    majority_vote = o['output'][0]['majority_vote']\n",
    "    average_score = o['output'][0]['average_score']\n",
    "    print(f\"data {idx} has majority vote \\033[31m{majority_vote}\\033[0m and average score \\033[34m{average_score}\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the config: Text format\n",
    "\n",
    "Following the previous settings, we changed `response_format={\"type\": \"text\"}` passed to `OpenAIModelConfig`, so the model will output plain text instead of a JSON object. In this case, AutoRater will use a regex to match the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RaterConfig(flow_name='RaterFlow', model_config={'model_name': 'gpt-3.5-turbo-1106', 'model_server': 'OpenAIModelServer', 'num_call': 3, 'temperature': 0.9, 'response_format': {'type': 'text'}}, label2score={'Yes': 1.0, 'No': 0.0}, guided_prompt_template=GuidedPrompt(instruction='\\n        Task: Answer Evaluation\\n        Objective:\\n        You are required to evaluate whether a given answer is appropriate in relation to a specific context and question. \\n        Input:\\n        1. Context: This is a brief text, usually a couple of sentences or a paragraph, providing key information or facts.\\n        2. Question: This is a query related to the information given in the context. It is designed to test knowledge that can be inferred or directly obtained from the context.\\n        3. Answer: This is a response to the question provided.\\n        Evaluation Criteria:\\n        Based on these, you need to judge if the answer is correct or incorrect in relation to the context and the question. Use the following labels to categorize your judgment:\\n        1. Yes: The answer is good.\\n        2. No: The answer is not bad.\\n        Response Format:\\n        Your response should include:\\n        1. label: Your judgment (one of the two labels mentioned above).\\n        2. explanatoin: A clear and concise thought for your judgment, detailing why you think the answer is good or bad.\\n        Note: Only use the example below as a few shot demonstrate but not including them in the final response.\\n        ', examples=[Context(context='The Eiffel Tower, located in Paris, France, is one of the most famous landmarks in the world. It was constructed in 1889 and stands at a height of 324 meters.', question='When was the Eiffel Tower constructed?', answer='The Eiffel Tower was constructed in 1889.', explanation='The context explicitly mentions that the Eiffel Tower was constructed in 1889, so the answer is correct.', label='Yes'), Context(context='Photosynthesis is a process used by plants to convert light energy into chemical energy. This process primarily occurs in the chloroplasts of plant cells.', question='Where does photosynthesis primarily occur in plant cells?', answer='Photosynthesis primarily occurs in the mitochondria of plant cells.', explanation='The context mentions that photosynthesis primarily occurs in the chloroplasts of plant cells, so the answer is incorrect.', label='No')]), num_thread=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:07<00:00,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'output': [{'average_score': 1.0,\n",
      "              'error': 'No errors.',\n",
      "              'majority_vote': 'yes',\n",
      "              'response': ['explanation: The context states that the Pacific '\n",
      "                           \"Ocean is the largest and deepest of Earth's \"\n",
      "                           'oceanic divisions, so the answer is correct.\\n'\n",
      "                           'label: Yes',\n",
      "                           'explanation: The context explicitly states that '\n",
      "                           'the Pacific Ocean is the largest ocean on Earth, '\n",
      "                           'so the answer is correct.\\n'\n",
      "                           'label: Yes',\n",
      "                           'explanation: The context explicitly states that '\n",
      "                           'the Pacific Ocean is the largest ocean on Earth, '\n",
      "                           'so the answer is correct.\\n'\n",
      "                           'label: Yes'],\n",
      "              'scores': [1.0, 1.0, 1.0],\n",
      "              'votes': ['yes', 'yes', 'yes']}],\n",
      "  'root': <uniflow.node.Node object at 0x7fa0bf0f33a0>},\n",
      " {'output': [{'average_score': 0.0,\n",
      "              'error': 'No errors.',\n",
      "              'majority_vote': 'no',\n",
      "              'response': ['explanation: The context explicitly states that '\n",
      "                           'Shakespeare wrote 39 plays, so the answer is '\n",
      "                           'incorrect.\\n'\n",
      "                           'label: No',\n",
      "                           'explanation: The context explicitly states that '\n",
      "                           'Shakespeare wrote 39 plays during his lifetime, so '\n",
      "                           'the answer is incorrect.\\n'\n",
      "                           'label: No',\n",
      "                           'explanation: The context explicitly states that '\n",
      "                           'Shakespeare wrote 39 plays, so the answer is '\n",
      "                           'incorrect.\\n'\n",
      "                           'label: No'],\n",
      "              'scores': [0.0, 0.0, 0.0],\n",
      "              'votes': ['no', 'no', 'no']}],\n",
      "  'root': <uniflow.node.Node object at 0x7fa0bf0f3be0>},\n",
      " {'output': [{'average_score': 0.0,\n",
      "              'error': 'No errors.',\n",
      "              'majority_vote': 'no',\n",
      "              'response': ['explanation: The context states that the human '\n",
      "                           'brain is responsible for intelligence, memory, and '\n",
      "                           'emotions, not physical movement, so the answer is '\n",
      "                           'incorrect.\\n'\n",
      "                           'label: No',\n",
      "                           'explanation: The context mentions that the human '\n",
      "                           'brain is responsible for intelligence, memory, and '\n",
      "                           'emotions, not just physical movement. The answer '\n",
      "                           'is incorrect.\\n'\n",
      "                           'label: No',\n",
      "                           'explanation: The context mentions that the human '\n",
      "                           'brain is responsible for intelligence, memory, and '\n",
      "                           'emotions, not physical movement, so the answer is '\n",
      "                           'incorrect.\\n'\n",
      "                           'label: No'],\n",
      "              'scores': [0.0, 0.0, 0.0],\n",
      "              'votes': ['no', 'no', 'no']}],\n",
      "  'root': <uniflow.node.Node object at 0x7fa0bf0f2b60>}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "config = RaterClassificationConfig(\n",
    "    flow_name=\"RaterFlow\",\n",
    "    model_config=OpenAIModelConfig(num_call=3, response_format={\"type\": \"text\"}),\n",
    "    label2score={\"Yes\": 1.0, \"No\": 0.0})\n",
    "\n",
    "with OpScope(name=\"TextFlow\"):\n",
    "    client = RaterClient(config)\n",
    "\n",
    "output = client.run(data)\n",
    "\n",
    "pprint.pprint(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model's responses can be distilled into majority votes, as shown below. Given the non-deterministic nature of the LLM (where each inference could yield a different output), we've enhanced stability and self-consistency by averaging results from three LLM output samplings, a notable improvement over single-output scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data 0 has majority vote \u001b[31myes\u001b[0m and average score \u001b[34m1.0\u001b[0m\n",
      "data 1 has majority vote \u001b[31mno\u001b[0m and average score \u001b[34m0.0\u001b[0m\n",
      "data 2 has majority vote \u001b[31mno\u001b[0m and average score \u001b[34m0.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "for idx, o in enumerate(output):\n",
    "    majority_vote = o['output'][0]['majority_vote']\n",
    "    average_score = o['output'][0]['average_score']\n",
    "    print(f\"data {idx} has majority vote \\033[31m{majority_vote}\\033[0m and average score \\033[34m{average_score}\\033[0m\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uniflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
