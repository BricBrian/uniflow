{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use `AutoRater` to Evaluate Answer Completeness and Accuracy for Given Questions\n",
    "\n",
    "Do you need to evaluate the completeness and accuracy of an answer generated by a Large Language Model (LLM)? In this example, we demonstrate how to use AutoRater for verifying the correctness of an answer to a specific question and its context.\n",
    "\n",
    "### Before running the code\n",
    "\n",
    "You will need to create a `uniflow` conda environment to run this notebook. You can set up the environment following the instruction: https://github.com/CambioML/uniflow/tree/main#installation.\n",
    "\n",
    "Next, you will need a valid [OpenAI API key](https://platform.openai.com/api-keys) to run the code. Once you have the key, set it as the environment variable `OPENAI_API_KEY` within a `.env` file in the root directory of this repository. For more details, see this [instruction](https://github.com/CambioML/uniflow/tree/main#api-keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the dependency\n",
    "First, we set system paths and import libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joseortiz/anaconda3/envs/uniflow/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\".\")\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "import pprint\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display\n",
    "\n",
    "from uniflow.flow.client import RaterClient\n",
    "from uniflow.flow.config  import RaterClassificationOpenAIGPT4Config\n",
    "from uniflow.op.prompt_schema import Context\n",
    "from uniflow.op.op import OpScope\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the input data\n",
    "\n",
    "We use three example raw inputs. Each one is a tuple consisting of context, question, and answer to be labeled. The ground truth label of the first one is 'correct', and the others are 'incorrect'. Then, we use the `Context` class to wrap them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_input = [\n",
    "    (\"The Pacific Ocean is the largest and deepest of Earth's oceanic divisions. It extends from the Arctic Ocean in the north to the Southern Ocean in the south.\",\n",
    "     \"What is the largest ocean on Earth?\",\n",
    "     \"The largest ocean on Earth is the Pacific Ocean.\"), # correct\n",
    "    (\"Shakespeare, a renowned English playwright and poet, wrote 39 plays during his lifetime. His works include famous plays like 'Hamlet' and 'Romeo and Juliet'.\",\n",
    "     \"How many plays did Shakespeare write?\",\n",
    "     \"Shakespeare wrote 31 plays.\"), # incorrect\n",
    "    (\"The human brain is an intricate organ responsible for intelligence, memory, and emotions. It is made up of approximately 86 billion neurons.\",\n",
    "     \"What is the human brain responsible for?\",\n",
    "     \"The human brain is responsible for physical movement.\"), # incorrect\n",
    "]\n",
    "\n",
    "data = [\n",
    "    Context(context=c[0], question=c[1], answer=c[2])\n",
    "    for c in raw_input\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Output JSON format using GPT4\n",
    "\n",
    "In this example, we will use the OpenAI GPT4 Model as the default LLM to generate questions and answers. If you want to use open-source models, you can replace with Huggingface models in the Uniflow.\n",
    "\n",
    "We use the default `guided_prompt` in `RaterClassificationOpenAIGPT4Config`, which includes the four attributes:\n",
    "- `flow_name` (str): Name of the rating flow, default is \"RaterFlow\".\n",
    "- `model_config` (ModelConfig): Configuration for the GPT-4 model. Includes model name (\"gpt-4\"),\n",
    "                            the server (\"OpenAIModelServer\"), number of calls (1), temperature (0.2),\n",
    "                            and the response format (plain text).\n",
    "- `label2score` (Dict[str, float]): Mapping of labels to scores, default is {\"Yes\": 1.0, \"No\": 0.0}.\n",
    "- `guided_prompt_template` (GuidedPrompt): Template for guided prompts used in rating. Includes instructions\n",
    "                                        for rating, along with examples that detail the context, question,\n",
    "                                        answer, label, and explanation for each case.\n",
    "\n",
    "The configuration primarily focuses on setting up the parameters for utilizing GPT-4 to evaluate the correctness of answers in relation to given questions and contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RaterClassificationOpenAIGPT4Config(flow_name='RaterFlow',\n",
      "                                    model_config=OpenAIModelConfig(model_name='gpt-4',\n",
      "                                                                   model_server='OpenAIModelServer',\n",
      "                                                                   num_call=1,\n",
      "                                                                   temperature=0.2,\n",
      "                                                                   response_format={'type': 'text'}),\n",
      "                                    label2score={'No': 0.0, 'Yes': 1.0},\n",
      "                                    guided_prompt_template=GuidedPrompt(instruction=\"Rate the answer based on the question and the context.\\n        Follow the format of the examples below to include context, question, answer, and label in the response.\\n        The response should not include examples in the prompt. The response label should be one of the following: ['Yes', 'No'].\", examples=[Context(context='The Eiffel Tower, located in Paris, France, is one of the most famous landmarks in the world. It was constructed in 1889 and stands at a height of 324 meters.', question='When was the Eiffel Tower constructed?', answer='The Eiffel Tower was constructed in 1889.', explanation='The context explicitly mentions that the Eiffel Tower was constructed in 1889, so the answer is correct.', label='Yes'), Context(context='Photosynthesis is a process used by plants to convert light energy into chemical energy. This process primarily occurs in the chloroplasts of plant cells.', question='Where does photosynthesis primarily occur in plant cells?', answer='Photosynthesis primarily occurs in the mitochondria of plant cells.', explanation='The context mentions that photosynthesis primarily occurs in the chloroplasts of plant cells, so the answer is incorrect.', label='No')]),\n",
      "                                    num_thread=1)\n"
     ]
    }
   ],
   "source": [
    "config = RaterClassificationOpenAIGPT4Config()\n",
    "pprint.pprint(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want the response format to be JSON, we need to update two aspects of the default config:\n",
    "1. Change the `model_name` to \"gpt-4-1106-preview\", which is the only model that supports the JSON format.\n",
    "1. Change the `response_format` to a `json_object`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.model_config.model_name = \"gpt-4-1106-preview\"\n",
    "config.model_config.response_format = {\"type\": \"json_object\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can initialize a client. Since we will demonstrate multiple raters in the notebook, we will initialize them under different operation name scopes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RaterConfig(flow_name='RaterFlow', model_config={'model_name': 'gpt-4-1106-preview', 'model_server': 'OpenAIModelServer', 'num_call': 1, 'temperature': 0.2, 'response_format': {'type': 'json_object'}}, label2score={'Yes': 1.0, 'No': 0.0}, guided_prompt_template=GuidedPrompt(instruction=\"Rate the answer based on the question and the context.\\n        Follow the format of the examples below to include context, question, answer, and label in the response.\\n        The response should not include examples in the prompt. The response label should be one of the following: ['Yes', 'No'].\", examples=[Context(context='The Eiffel Tower, located in Paris, France, is one of the most famous landmarks in the world. It was constructed in 1889 and stands at a height of 324 meters.', question='When was the Eiffel Tower constructed?', answer='The Eiffel Tower was constructed in 1889.', explanation='The context explicitly mentions that the Eiffel Tower was constructed in 1889, so the answer is correct.', label='Yes'), Context(context='Photosynthesis is a process used by plants to convert light energy into chemical energy. This process primarily occurs in the chloroplasts of plant cells.', question='Where does photosynthesis primarily occur in plant cells?', answer='Photosynthesis primarily occurs in the mitochondria of plant cells.', explanation='The context mentions that photosynthesis primarily occurs in the chloroplasts of plant cells, so the answer is incorrect.', label='No')]), num_thread=1)\n"
     ]
    }
   ],
   "source": [
    "with OpScope(name=\"JSONFlow\"):\n",
    "    client = RaterClient(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the client\n",
    "\n",
    "Then we can run the client. For each item in the `raw_input`, the Client will generate an explanation and a final label, either `Yes` or `No`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:27<00:00,  9.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'output': [{'average_score': 1.0,\n",
      "              'error': 'No errors.',\n",
      "              'majority_vote': 'yes',\n",
      "              'response': [{'answer': 'The largest ocean on Earth is the '\n",
      "                                      'Pacific Ocean.',\n",
      "                            'context': 'The Pacific Ocean is the largest and '\n",
      "                                       \"deepest of Earth's oceanic divisions. \"\n",
      "                                       'It extends from the Arctic Ocean in '\n",
      "                                       'the north to the Southern Ocean in the '\n",
      "                                       'south.',\n",
      "                            'explanation': 'The context clearly states that '\n",
      "                                           'the Pacific Ocean is the largest '\n",
      "                                           \"and deepest of Earth's oceanic \"\n",
      "                                           'divisions, which directly answers '\n",
      "                                           'the question.',\n",
      "                            'label': 'Yes',\n",
      "                            'question': 'What is the largest ocean on Earth?'}],\n",
      "              'scores': [1.0],\n",
      "              'votes': ['yes']}],\n",
      "  'root': <uniflow.node.Node object at 0x10d4c3700>},\n",
      " {'output': [{'average_score': 0.0,\n",
      "              'error': 'No errors.',\n",
      "              'majority_vote': 'no',\n",
      "              'response': [{'answer': 'Shakespeare wrote 31 plays.',\n",
      "                            'context': 'Shakespeare, a renowned English '\n",
      "                                       'playwright and poet, wrote 39 plays '\n",
      "                                       'during his lifetime. His works include '\n",
      "                                       \"famous plays like 'Hamlet' and 'Romeo \"\n",
      "                                       \"and Juliet'.\",\n",
      "                            'explanation': 'The context states that '\n",
      "                                           'Shakespeare wrote 39 plays during '\n",
      "                                           'his lifetime, but the answer '\n",
      "                                           'incorrectly states that he wrote '\n",
      "                                           '31 plays.',\n",
      "                            'label': 'No',\n",
      "                            'question': 'How many plays did Shakespeare '\n",
      "                                        'write?'}],\n",
      "              'scores': [0.0],\n",
      "              'votes': ['no']}],\n",
      "  'root': <uniflow.node.Node object at 0x10d4c3b50>},\n",
      " {'output': [{'average_score': 0.0,\n",
      "              'error': 'No errors.',\n",
      "              'majority_vote': 'no',\n",
      "              'response': [{'answer': 'The human brain is responsible for '\n",
      "                                      'physical movement.',\n",
      "                            'context': 'The human brain is an intricate organ '\n",
      "                                       'responsible for intelligence, memory, '\n",
      "                                       'and emotions. It is made up of '\n",
      "                                       'approximately 86 billion neurons.',\n",
      "                            'explanation': 'The context lists intelligence, '\n",
      "                                           'memory, and emotions as functions '\n",
      "                                           'of the human brain, but does not '\n",
      "                                           'mention physical movement. '\n",
      "                                           'Physical movement is generally '\n",
      "                                           'controlled by the brain but is not '\n",
      "                                           'highlighted in the given context.',\n",
      "                            'label': 'No',\n",
      "                            'question': 'What is the human brain responsible '\n",
      "                                        'for?'}],\n",
      "              'scores': [0.0],\n",
      "              'votes': ['no']}],\n",
      "  'root': <uniflow.node.Node object at 0x10d4c3e20>}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output = client.run(data)\n",
    "pprint.pprint(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that model response is a JSON object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'The largest ocean on Earth is the Pacific Ocean.',\n",
      " 'context': \"The Pacific Ocean is the largest and deepest of Earth's oceanic \"\n",
      "            'divisions. It extends from the Arctic Ocean in the north to the '\n",
      "            'Southern Ocean in the south.',\n",
      " 'explanation': 'The context clearly states that the Pacific Ocean is the '\n",
      "                \"largest and deepest of Earth's oceanic divisions, which \"\n",
      "                'directly answers the question.',\n",
      " 'label': 'Yes',\n",
      " 'question': 'What is the largest ocean on Earth?'}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(output[0][\"output\"][0][\"response\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model's responses can be distilled into majority votes, as shown below. Given the non-deterministic nature of the LLM (where each inference could yield a different output), we've enhanced stability and self-consistency by averaging results from three LLM output samplings, a notable improvement over single-output scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data 0 has majority vote \u001b[31myes\u001b[0m and average score \u001b[34m1.0\u001b[0m\n",
      "data 1 has majority vote \u001b[31mno\u001b[0m and average score \u001b[34m0.0\u001b[0m\n",
      "data 2 has majority vote \u001b[31mno\u001b[0m and average score \u001b[34m0.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "for idx, o in enumerate(output):\n",
    "    majority_vote = o['output'][0]['majority_vote']\n",
    "    average_score = o['output'][0]['average_score']\n",
    "    print(f\"data {idx} has majority vote \\033[31m{majority_vote}\\033[0m and average score \\033[34m{average_score}\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Output text format using GPT3.5\n",
    "\n",
    "Following the previous settings, we will keep the default config `response_format={\"type\": \"text\"}`, so the model will output plain text instead of a JSON object. In this case, AutoRater will use a regex to match the label. Furthermore, we will change `num_call` to 3. This means the model will perform inference on each example three times, allowing us to take the majority vote of the ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RaterConfig(flow_name='RaterFlow', model_config={'model_name': 'gpt-4', 'model_server': 'OpenAIModelServer', 'num_call': 3, 'temperature': 0.2, 'response_format': {'type': 'text'}}, label2score={'Yes': 1.0, 'No': 0.0}, guided_prompt_template=GuidedPrompt(instruction=\"Rate the answer based on the question and the context.\\n        Follow the format of the examples below to include context, question, answer, and label in the response.\\n        The response should not include examples in the prompt. The response label should be one of the following: ['Yes', 'No'].\", examples=[Context(context='The Eiffel Tower, located in Paris, France, is one of the most famous landmarks in the world. It was constructed in 1889 and stands at a height of 324 meters.', question='When was the Eiffel Tower constructed?', answer='The Eiffel Tower was constructed in 1889.', explanation='The context explicitly mentions that the Eiffel Tower was constructed in 1889, so the answer is correct.', label='Yes'), Context(context='Photosynthesis is a process used by plants to convert light energy into chemical energy. This process primarily occurs in the chloroplasts of plant cells.', question='Where does photosynthesis primarily occur in plant cells?', answer='Photosynthesis primarily occurs in the mitochondria of plant cells.', explanation='The context mentions that photosynthesis primarily occurs in the chloroplasts of plant cells, so the answer is incorrect.', label='No')]), num_thread=1)\n"
     ]
    }
   ],
   "source": [
    "config2 = RaterClassificationOpenAIGPT4Config()\n",
    "config2.model_config.num_call = 3\n",
    "\n",
    "with OpScope(name=\"TextFlow\"):\n",
    "    client2 = RaterClient(config2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the client\n",
    "\n",
    "Then we can run the client. For each item in the `raw_input`, the Client will generate an explanation and a final label, either `Yes` or `No`. The label is determined by taking the majority vote from three samples of the LLM's output, which improves stability and self-consistency compared to generating a single output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:11<00:00,  3.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'output': [{'average_score': 1.0,\n",
      "              'error': 'No errors.',\n",
      "              'majority_vote': 'yes',\n",
      "              'response': ['explanation: The context clearly states that the '\n",
      "                           \"Pacific Ocean is the largest of Earth's oceanic \"\n",
      "                           'divisions, so the answer is correct.\\n'\n",
      "                           'label: Yes',\n",
      "                           'explanation: The context clearly states that the '\n",
      "                           \"Pacific Ocean is the largest of Earth's oceanic \"\n",
      "                           'divisions, so the answer is correct.\\n'\n",
      "                           'label: Yes',\n",
      "                           'explanation: The context clearly states that the '\n",
      "                           \"Pacific Ocean is the largest of Earth's oceanic \"\n",
      "                           'divisions, so the answer is correct.\\n'\n",
      "                           'label: Yes'],\n",
      "              'scores': [1.0, 1.0, 1.0],\n",
      "              'votes': ['yes', 'yes', 'yes']}],\n",
      "  'root': <uniflow.node.Node object at 0x10d791090>},\n",
      " {'output': [{'average_score': 0.0,\n",
      "              'error': 'No errors.',\n",
      "              'majority_vote': 'no',\n",
      "              'response': ['explanation: The context mentions that Shakespeare '\n",
      "                           'wrote 39 plays during his lifetime, so the answer '\n",
      "                           'is incorrect.\\n'\n",
      "                           'label: No',\n",
      "                           'explanation: The context mentions that Shakespeare '\n",
      "                           'wrote 39 plays, so the answer is incorrect.\\n'\n",
      "                           'label: No',\n",
      "                           'explanation: The context clearly states that '\n",
      "                           'Shakespeare wrote 39 plays during his lifetime, so '\n",
      "                           'the answer is incorrect.\\n'\n",
      "                           'label: No'],\n",
      "              'scores': [0.0, 0.0, 0.0],\n",
      "              'votes': ['no', 'no', 'no']}],\n",
      "  'root': <uniflow.node.Node object at 0x10d71a470>},\n",
      " {'output': [{'average_score': 0.0,\n",
      "              'error': 'No errors.',\n",
      "              'majority_vote': 'no',\n",
      "              'response': ['explanation: The context mentions that the human '\n",
      "                           'brain is responsible for intelligence, memory, and '\n",
      "                           'emotions. It does not mention physical movement, '\n",
      "                           'so the answer is partially correct but '\n",
      "                           'incomplete.\\n'\n",
      "                           'label: No',\n",
      "                           'explanation: The context mentions that the human '\n",
      "                           'brain is responsible for intelligence, memory, and '\n",
      "                           'emotions. It does not mention physical movement, '\n",
      "                           'so the answer is not completely correct.\\n'\n",
      "                           'label: No',\n",
      "                           'explanation: The context does not mention that the '\n",
      "                           'human brain is responsible for physical movement. '\n",
      "                           'It states that the brain is responsible for '\n",
      "                           'intelligence, memory, and emotions. Therefore, the '\n",
      "                           'answer is not fully correct.\\n'\n",
      "                           'label: No'],\n",
      "              'scores': [0.0, 0.0, 0.0],\n",
      "              'votes': ['no', 'no', 'no']}],\n",
      "  'root': <uniflow.node.Node object at 0x10d4e1840>}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output = client2.run(data)\n",
    "pprint.pprint(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model's responses can be distilled into majority votes, as shown below. Given the non-deterministic nature of the LLM (where each inference could yield a different output), we've enhanced stability and self-consistency by averaging results from three LLM output samplings, a notable improvement over single-output scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data 0 has majority vote \u001b[31myes\u001b[0m and average score \u001b[34m1.0\u001b[0m\n",
      "data 1 has majority vote \u001b[31mno\u001b[0m and average score \u001b[34m0.0\u001b[0m\n",
      "data 2 has majority vote \u001b[31mno\u001b[0m and average score \u001b[34m0.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "for idx, o in enumerate(output):\n",
    "    majority_vote = o['output'][0]['majority_vote']\n",
    "    average_score = o['output'][0]['average_score']\n",
    "    print(f\"data {idx} has majority vote \\033[31m{majority_vote}\\033[0m and average score \\033[34m{average_score}\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of the notebook\n",
    "\n",
    "Check more Uniflow use cases in the [example folder](https://github.com/CambioML/uniflow/tree/main/example/model#examples)!\n",
    "\n",
    "<a href=\"https://www.cambioml.com/\" title=\"Title\">\n",
    "    <img src=\"../image/cambioml_logo_large.png\" style=\"height: 100px; display: block; margin-left: auto; margin-right: auto;\"/>\n",
    "</a>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uniflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
