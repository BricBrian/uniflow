{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use `AutoRater` to Evaluate Answer Completeness and Accuracy for Given Questions\n",
    "\n",
    "In this example, we will show you how to use AutoRater to verify the correctness of an answer to a given question and context pairs.\n",
    "\n",
    "### Before running the code\n",
    "\n",
    "You will need to `uniflow` conda environment to run this notebook. You can set up the environment following the instruction: https://github.com/CambioML/uniflow/tree/main#installation.\n",
    "\n",
    "Next, you will need a valid [OpenAI API key](https://platform.openai.com/api-keys) to run the code. Once you have the key, set it as the environment variable `OPENAI_API_KEY` within a `.env` file in the root directory of this repository. For more details, see this [instruction](https://github.com/CambioML/uniflow/tree/main#api-keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the dependency\n",
    "First, we set system paths and import libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lingjiekong/anaconda3/envs/uniflow/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\".\")\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "import pprint\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display\n",
    "\n",
    "from uniflow.flow.client import RaterClient\n",
    "from uniflow.flow.config  import RaterOpenAIGPT4ClassificationConfig\n",
    "from uniflow.op.prompt_schema import Context\n",
    "from uniflow.op.op import OpScope\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the input data\n",
    "\n",
    "We use three example raw inputs. Each one is a tuple consisting of context, question, and answer to be labeled. The ground truth label of the first one is 'correct', and the others are 'incorrect'. Then, we use the `Context` class to wrap them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_input = [\n",
    "    (\"The Pacific Ocean is the largest and deepest of Earth's oceanic divisions. It extends from the Arctic Ocean in the north to the Southern Ocean in the south.\",\n",
    "     \"What is the largest ocean on Earth?\",\n",
    "     \"The largest ocean on Earth is the Pacific Ocean.\"), # correct\n",
    "    (\"Shakespeare, a renowned English playwright and poet, wrote 39 plays during his lifetime. His works include famous plays like 'Hamlet' and 'Romeo and Juliet'.\",\n",
    "     \"How many plays did Shakespeare write?\",\n",
    "     \"Shakespeare wrote 31 plays.\"), # incorrect\n",
    "    (\"The human brain is an intricate organ responsible for intelligence, memory, and emotions. It is made up of approximately 86 billion neurons.\",\n",
    "     \"What is the human brain responsible for?\",\n",
    "     \"The human brain is responsible for physical movement.\"), # incorrect\n",
    "]\n",
    "\n",
    "data = [\n",
    "    Context(context=c[0], question=c[1], answer=c[2])\n",
    "    for c in raw_input\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the config: JSON format\n",
    "\n",
    "In this example, we will use the [`OpenAIModelConfig`](https://github.com/CambioML/uniflow/blob/main/uniflow/model/config.py#L17) as the default LLM to generate questions and answers. If you want to use open-source models, you can replace the `OpenAIConfig` and `OpenAIModelConfig` with `HuggingfaceConfig` and [`HuggingfaceModelConfig`](https://github.com/CambioML/uniflow/blob/main/uniflow/model/config.py#L27).\n",
    "\n",
    "We use the default `guided_prompt` in `RaterClassificationConfig`, which includes two examples, labeled 'Yes' and 'No'. The default examples are also encapsulated within the `Context` class, which has fields for context, question, answer (and label), aligning with the input data format.\n",
    "\n",
    "The response format is JSON, enabling the model to return a JSON object as output rather than plain text. This facilitates more convenient processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RaterConfig(flow_name='RaterFlow', model_config={'model_name': 'gpt-4-1106-preview', 'model_server': 'OpenAIModelServer', 'num_call': 1, 'temperature': 0.2, 'response_format': {'type': 'json_object'}}, label2score={'Yes': 1.0, 'No': 0.0}, guided_prompt_template=GuidedPrompt(instruction=\"Rate the answer based on the question and the context.\\n        Follow the format of the examples below to include context, question, answer, and label in the response.\\n        The response should not include examples in the prompt. The response label should be one of the following: ['Yes', 'No'].\", examples=[Context(context='The Eiffel Tower, located in Paris, France, is one of the most famous landmarks in the world. It was constructed in 1889 and stands at a height of 324 meters.', question='When was the Eiffel Tower constructed?', answer='The Eiffel Tower was constructed in 1889.', explanation='The context explicitly mentions that the Eiffel Tower was constructed in 1889, so the answer is correct.', label='Yes'), Context(context='Photosynthesis is a process used by plants to convert light energy into chemical energy. This process primarily occurs in the chloroplasts of plant cells.', question='Where does photosynthesis primarily occur in plant cells?', answer='Photosynthesis primarily occurs in the mitochondria of plant cells.', explanation='The context mentions that photosynthesis primarily occurs in the chloroplasts of plant cells, so the answer is incorrect.', label='No')]), num_thread=1)\n",
      "RaterOpenAIGPT4ClassificationConfig(flow_name='RaterFlow',\n",
      "                                    model_config=OpenAIModelConfig(model_name='gpt-4-1106-preview',\n",
      "                                                                   model_server='OpenAIModelServer',\n",
      "                                                                   num_call=1,\n",
      "                                                                   temperature=0.2,\n",
      "                                                                   response_format={'type': 'json_object'}),\n",
      "                                    label2score={'No': 0.0, 'Yes': 1.0},\n",
      "                                    guided_prompt_template=GuidedPrompt(instruction=\"Rate the answer based on the question and the context.\\n        Follow the format of the examples below to include context, question, answer, and label in the response.\\n        The response should not include examples in the prompt. The response label should be one of the following: ['Yes', 'No'].\", examples=[Context(context='The Eiffel Tower, located in Paris, France, is one of the most famous landmarks in the world. It was constructed in 1889 and stands at a height of 324 meters.', question='When was the Eiffel Tower constructed?', answer='The Eiffel Tower was constructed in 1889.', explanation='The context explicitly mentions that the Eiffel Tower was constructed in 1889, so the answer is correct.', label='Yes'), Context(context='Photosynthesis is a process used by plants to convert light energy into chemical energy. This process primarily occurs in the chloroplasts of plant cells.', question='Where does photosynthesis primarily occur in plant cells?', answer='Photosynthesis primarily occurs in the mitochondria of plant cells.', explanation='The context mentions that photosynthesis primarily occurs in the chloroplasts of plant cells, so the answer is incorrect.', label='No')]),\n",
      "                                    num_thread=1)\n"
     ]
    }
   ],
   "source": [
    "config = RaterOpenAIGPT4ClassificationConfig()\n",
    "config.model_config.model_name = \"gpt-4-1106-preview\"\n",
    "config.model_config.response_format = {\"type\": \"json_object\"}\n",
    "with OpScope(name=\"JSONFlow\"):\n",
    "    client = RaterClient(config)\n",
    "\n",
    "pprint.pprint(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the client\n",
    "\n",
    "Then we can run the client. For each item in the `raw_input`, the Client will generate an explanation and a final label, either `Yes` or `No`. The label is determined by taking the majority vote from three samples of the LLM's output, which improves stability and self-consistency compared to generating a single output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:38<00:00, 12.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'output': [{'average_score': 1.0,\n",
      "              'error': 'No errors.',\n",
      "              'majority_vote': 'yes',\n",
      "              'response': [{'answer': 'The largest ocean on Earth is the '\n",
      "                                      'Pacific Ocean.',\n",
      "                            'context': 'The Pacific Ocean is the largest and '\n",
      "                                       \"deepest of Earth's oceanic divisions. \"\n",
      "                                       'It extends from the Arctic Ocean in '\n",
      "                                       'the north to the Southern Ocean in the '\n",
      "                                       'south.',\n",
      "                            'explanation': 'The context states that the '\n",
      "                                           'Pacific Ocean is the largest and '\n",
      "                                           \"deepest of Earth's oceanic \"\n",
      "                                           'divisions, which directly answers '\n",
      "                                           'the question affirming that the '\n",
      "                                           'Pacific Ocean is the largest ocean '\n",
      "                                           'on Earth.',\n",
      "                            'label': 'Yes',\n",
      "                            'question': 'What is the largest ocean on Earth?'}],\n",
      "              'scores': [1.0],\n",
      "              'votes': ['yes']}],\n",
      "  'root': <uniflow.node.Node object at 0x12036fd90>},\n",
      " {'output': [{'average_score': 0.0,\n",
      "              'error': 'No errors.',\n",
      "              'majority_vote': 'no',\n",
      "              'response': [{'answer': 'Shakespeare wrote 31 plays.',\n",
      "                            'context': 'Shakespeare, a renowned English '\n",
      "                                       'playwright and poet, wrote 39 plays '\n",
      "                                       'during his lifetime. His works include '\n",
      "                                       \"famous plays like 'Hamlet' and 'Romeo \"\n",
      "                                       \"and Juliet'.\",\n",
      "                            'explanation': 'The context states that '\n",
      "                                           'Shakespeare wrote 39 plays during '\n",
      "                                           'his lifetime, but the answer '\n",
      "                                           'incorrectly states that he wrote '\n",
      "                                           '31 plays.',\n",
      "                            'label': 'No',\n",
      "                            'question': 'How many plays did Shakespeare '\n",
      "                                        'write?'}],\n",
      "              'scores': [0.0],\n",
      "              'votes': ['no']}],\n",
      "  'root': <uniflow.node.Node object at 0x1071a5690>},\n",
      " {'output': [{'average_score': 0.0,\n",
      "              'error': 'No errors.',\n",
      "              'majority_vote': 'no',\n",
      "              'response': [{'answer': 'The human brain is responsible for '\n",
      "                                      'physical movement.',\n",
      "                            'context': 'The human brain is an intricate organ '\n",
      "                                       'responsible for intelligence, memory, '\n",
      "                                       'and emotions. It is made up of '\n",
      "                                       'approximately 86 billion neurons.',\n",
      "                            'explanation': 'The context lists intelligence, '\n",
      "                                           'memory, and emotions as the '\n",
      "                                           'responsibilities of the human '\n",
      "                                           'brain, not physical movement. '\n",
      "                                           'Physical movement is controlled by '\n",
      "                                           'the brain but is not mentioned in '\n",
      "                                           'the context provided.',\n",
      "                            'label': 'No',\n",
      "                            'question': 'What is the human brain responsible '\n",
      "                                        'for?'}],\n",
      "              'scores': [0.0],\n",
      "              'votes': ['no']}],\n",
      "  'root': <uniflow.node.Node object at 0x12036fd00>}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output = client.run(data)\n",
    "pprint.pprint(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that model response is a JSON object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'The largest ocean on Earth is the Pacific Ocean.',\n",
      " 'context': \"The Pacific Ocean is the largest and deepest of Earth's oceanic \"\n",
      "            'divisions. It extends from the Arctic Ocean in the north to the '\n",
      "            'Southern Ocean in the south.',\n",
      " 'explanation': 'The context states that the Pacific Ocean is the largest and '\n",
      "                \"deepest of Earth's oceanic divisions, which directly answers \"\n",
      "                'the question affirming that the Pacific Ocean is the largest '\n",
      "                'ocean on Earth.',\n",
      " 'label': 'Yes',\n",
      " 'question': 'What is the largest ocean on Earth?'}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(output[0][\"output\"][0][\"response\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model's responses can be distilled into majority votes, as shown below. Given the non-deterministic nature of the LLM (where each inference could yield a different output), we've enhanced stability and self-consistency by averaging results from three LLM output samplings, a notable improvement over single-output scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data 0 has majority vote \u001b[31myes\u001b[0m and average score \u001b[34m1.0\u001b[0m\n",
      "data 1 has majority vote \u001b[31mno\u001b[0m and average score \u001b[34m0.0\u001b[0m\n",
      "data 2 has majority vote \u001b[31mno\u001b[0m and average score \u001b[34m0.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "for idx, o in enumerate(output):\n",
    "    majority_vote = o['output'][0]['majority_vote']\n",
    "    average_score = o['output'][0]['average_score']\n",
    "    print(f\"data {idx} has majority vote \\033[31m{majority_vote}\\033[0m and average score \\033[34m{average_score}\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the config: Text format\n",
    "\n",
    "Following the previous settings, we changed `response_format={\"type\": \"text\"}` passed to `OpenAIModelConfig`, so the model will output plain text instead of a JSON object. In this case, AutoRater will use a regex to match the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RaterConfig(flow_name='RaterFlow', model_config={'model_name': 'gpt-4', 'model_server': 'OpenAIModelServer', 'num_call': 3, 'temperature': 0.2, 'response_format': {'type': 'text'}}, label2score={'Yes': 1.0, 'No': 0.0}, guided_prompt_template=GuidedPrompt(instruction=\"Rate the answer based on the question and the context.\\n        Follow the format of the examples below to include context, question, answer, and label in the response.\\n        The response should not include examples in the prompt. The response label should be one of the following: ['Yes', 'No'].\", examples=[Context(context='The Eiffel Tower, located in Paris, France, is one of the most famous landmarks in the world. It was constructed in 1889 and stands at a height of 324 meters.', question='When was the Eiffel Tower constructed?', answer='The Eiffel Tower was constructed in 1889.', explanation='The context explicitly mentions that the Eiffel Tower was constructed in 1889, so the answer is correct.', label='Yes'), Context(context='Photosynthesis is a process used by plants to convert light energy into chemical energy. This process primarily occurs in the chloroplasts of plant cells.', question='Where does photosynthesis primarily occur in plant cells?', answer='Photosynthesis primarily occurs in the mitochondria of plant cells.', explanation='The context mentions that photosynthesis primarily occurs in the chloroplasts of plant cells, so the answer is incorrect.', label='No')]), num_thread=1)\n",
      "RaterOpenAIGPT4ClassificationConfig(flow_name='RaterFlow',\n",
      "                                    model_config=OpenAIModelConfig(model_name='gpt-4',\n",
      "                                                                   model_server='OpenAIModelServer',\n",
      "                                                                   num_call=3,\n",
      "                                                                   temperature=0.2,\n",
      "                                                                   response_format={'type': 'text'}),\n",
      "                                    label2score={'No': 0.0, 'Yes': 1.0},\n",
      "                                    guided_prompt_template=GuidedPrompt(instruction=\"Rate the answer based on the question and the context.\\n        Follow the format of the examples below to include context, question, answer, and label in the response.\\n        The response should not include examples in the prompt. The response label should be one of the following: ['Yes', 'No'].\", examples=[Context(context='The Eiffel Tower, located in Paris, France, is one of the most famous landmarks in the world. It was constructed in 1889 and stands at a height of 324 meters.', question='When was the Eiffel Tower constructed?', answer='The Eiffel Tower was constructed in 1889.', explanation='The context explicitly mentions that the Eiffel Tower was constructed in 1889, so the answer is correct.', label='Yes'), Context(context='Photosynthesis is a process used by plants to convert light energy into chemical energy. This process primarily occurs in the chloroplasts of plant cells.', question='Where does photosynthesis primarily occur in plant cells?', answer='Photosynthesis primarily occurs in the mitochondria of plant cells.', explanation='The context mentions that photosynthesis primarily occurs in the chloroplasts of plant cells, so the answer is incorrect.', label='No')]),\n",
      "                                    num_thread=1)\n"
     ]
    }
   ],
   "source": [
    "config2 = RaterOpenAIGPT4ClassificationConfig()\n",
    "config2.model_config.num_call = 3\n",
    "with OpScope(name=\"TextFlow\"):\n",
    "    client2 = RaterClient(config2)\n",
    "\n",
    "pprint.pprint(config2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:10<00:00,  3.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'output': [{'average_score': 1.0,\n",
      "              'error': 'No errors.',\n",
      "              'majority_vote': 'yes',\n",
      "              'response': ['explanation: The context explicitly mentions that '\n",
      "                           \"the Pacific Ocean is the largest of Earth's \"\n",
      "                           'oceanic divisions, so the answer is correct.\\n'\n",
      "                           'label: Yes',\n",
      "                           'explanation: The context clearly states that the '\n",
      "                           \"Pacific Ocean is the largest of Earth's oceanic \"\n",
      "                           'divisions, so the answer is correct.\\n'\n",
      "                           'label: Yes',\n",
      "                           'explanation: The context clearly states that the '\n",
      "                           \"Pacific Ocean is the largest of Earth's oceanic \"\n",
      "                           'divisions, so the answer is correct.\\n'\n",
      "                           'label: Yes'],\n",
      "              'scores': [1.0, 1.0, 1.0],\n",
      "              'votes': ['yes', 'yes', 'yes']}],\n",
      "  'root': <uniflow.node.Node object at 0x12061cdc0>},\n",
      " {'output': [{'average_score': 0.0,\n",
      "              'error': 'No errors.',\n",
      "              'majority_vote': 'no',\n",
      "              'response': ['explanation: The context clearly states that '\n",
      "                           'Shakespeare wrote 39 plays, so the answer is '\n",
      "                           'incorrect.\\n'\n",
      "                           'label: No',\n",
      "                           'explanation: The context mentions that Shakespeare '\n",
      "                           'wrote 39 plays, so the answer is incorrect.\\n'\n",
      "                           'label: No',\n",
      "                           'explanation: The context mentions that Shakespeare '\n",
      "                           'wrote 39 plays during his lifetime, so the answer '\n",
      "                           'is incorrect.\\n'\n",
      "                           'label: No'],\n",
      "              'scores': [0.0, 0.0, 0.0],\n",
      "              'votes': ['no', 'no', 'no']}],\n",
      "  'root': <uniflow.node.Node object at 0x1203c6ce0>},\n",
      " {'output': [{'average_score': 0.0,\n",
      "              'error': 'No errors.',\n",
      "              'majority_vote': 'no',\n",
      "              'response': ['explanation: The context mentions that the human '\n",
      "                           'brain is responsible for intelligence, memory, and '\n",
      "                           'emotions. It does not mention physical movement, '\n",
      "                           'so the answer is partially correct but lacks '\n",
      "                           'completeness.\\n'\n",
      "                           'label: No',\n",
      "                           'explanation: The context does not mention that the '\n",
      "                           'human brain is responsible for physical movement. '\n",
      "                           'It mentions that the human brain is responsible '\n",
      "                           'for intelligence, memory, and emotions. Therefore, '\n",
      "                           'the answer is not fully correct.\\n'\n",
      "                           'label: No',\n",
      "                           'explanation: The context mentions that the human '\n",
      "                           'brain is responsible for intelligence, memory, and '\n",
      "                           'emotions. It does not mention physical movement, '\n",
      "                           'so the answer is partially correct.\\n'\n",
      "                           'label: No'],\n",
      "              'scores': [0.0, 0.0, 0.0],\n",
      "              'votes': ['no', 'no', 'no']}],\n",
      "  'root': <uniflow.node.Node object at 0x12061cf40>}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output = client2.run(data)\n",
    "pprint.pprint(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model's responses can be distilled into majority votes, as shown below. Given the non-deterministic nature of the LLM (where each inference could yield a different output), we've enhanced stability and self-consistency by averaging results from three LLM output samplings, a notable improvement over single-output scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data 0 has majority vote \u001b[31myes\u001b[0m and average score \u001b[34m1.0\u001b[0m\n",
      "data 1 has majority vote \u001b[31mno\u001b[0m and average score \u001b[34m0.0\u001b[0m\n",
      "data 2 has majority vote \u001b[31mno\u001b[0m and average score \u001b[34m0.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "for idx, o in enumerate(output):\n",
    "    majority_vote = o['output'][0]['majority_vote']\n",
    "    average_score = o['output'][0]['average_score']\n",
    "    print(f\"data {idx} has majority vote \\033[31m{majority_vote}\\033[0m and average score \\033[34m{average_score}\\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uniflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
