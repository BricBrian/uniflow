{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use `AutoRater` to Evaluate Answer Completeness and Accuracy for Given Questions\n",
    "\n",
    "In this example, we will show you how to use AutoRater to verify the correctness of an answer to a given question and context pairs.\n",
    "\n",
    "### Before running the code\n",
    "\n",
    "You will need to `uniflow` conda environment to run this notebook. You can set up the environment following the instruction: https://github.com/CambioML/uniflow/tree/main#installation.\n",
    "\n",
    "Next, you will need a valid [OpenAI API key](https://platform.openai.com/api-keys) to run the code. Once you have the key, set it as the environment variable `OPENAI_API_KEY` within a `.env` file in the root directory of this repository. For more details, see this [instruction](https://github.com/CambioML/uniflow/tree/main#api-keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the dependency\n",
    "First, we set system paths and import libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayn/miniconda3/envs/uniflow/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\".\")\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "import pprint\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display\n",
    "\n",
    "from uniflow.flow.client import RaterClient\n",
    "from uniflow.flow.config  import RaterClassificationConfig\n",
    "from uniflow.op.model.model_config  import OpenAIModelConfig\n",
    "from uniflow.op.prompt_schema import Context\n",
    "from uniflow.op.op import OpScope\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the input data\n",
    "\n",
    "We use 3 example raw inputs. Each one is a tuple with context, question and answer to be labeled. The grounding truth label of first one is correct and other are incorrect. Then we use `Context` class to wrap them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_input = [\n",
    "    (\"The Pacific Ocean is the largest and deepest of Earth's oceanic divisions. It extends from the Arctic Ocean in the north to the Southern Ocean in the south.\",\n",
    "     \"What is the largest ocean on Earth?\",\n",
    "     \"The largest ocean on Earth is the Pacific Ocean.\"), # correct\n",
    "    (\"Shakespeare, a renowned English playwright and poet, wrote 39 plays during his lifetime. His works include famous plays like 'Hamlet' and 'Romeo and Juliet'.\",\n",
    "     \"How many plays did Shakespeare write?\",\n",
    "     \"Shakespeare wrote 31 plays.\"), # incorrect\n",
    "    (\"The human brain is an intricate organ responsible for intelligence, memory, and emotions. It is made up of approximately 86 billion neurons.\",\n",
    "     \"What is the human brain responsible for?\",\n",
    "     \"The human brain is responsible for physical movement.\"), # incorrect\n",
    "]\n",
    "\n",
    "data = [\n",
    "    Context(context=c[0], question=c[1], answer=c[2])\n",
    "    for c in raw_input\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the config: JSON format\n",
    "\n",
    "In this example, we will use the [`OpenAIModelConfig`](https://github.com/CambioML/uniflow/blob/main/uniflow/model/config.py#L17) as the default LLM to generate questions and answers. If you want to use open-source models, you can replace the `OpenAIConfig` and `OpenAIModelConfig` with `HuggingfaceConfig` and [`HuggingfaceModelConfig`](https://github.com/CambioML/uniflow/blob/main/uniflow/model/config.py#L27).\n",
    "\n",
    "We use the default `guided_prompt` in `RaterClassificationConfig`, which contains two examples, labeled by Yes and No. The default examples are also wrap by `Context` class with fileds of context, question, answer (and label), consistent with input data.\n",
    "\n",
    "The response format is `json`, so the model returns json object as output instead of plain text, which can be processed more conveniently. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RaterConfig(flow_name='RaterFlow', model_config={'model_name': 'gpt-3.5-turbo-1106', 'model_server': 'OpenAIModelServer', 'num_call': 3, 'temperature': 0.9, 'response_format': {'type': 'json_object'}}, label2score={'Yes': 1.0, 'No': 0.0}, guided_prompt_template=GuidedPrompt(instruction='Rate the answer based on the question and the context.\\n        Follow the format of the examples below to include context, question, answer, and label in the response.\\n        The response should not include examples in the prompt.', examples=[Context(context='The Eiffel Tower, located in Paris, France, is one of the most famous landmarks in the world. It was constructed in 1889 and stands at a height of 324 meters.', question='When was the Eiffel Tower constructed?', answer='The Eiffel Tower was constructed in 1889.', explanation='The context explicitly mentions that the Eiffel Tower was constructed in 1889, so the answer is correct.', label='Yes'), Context(context='Photosynthesis is a process used by plants to convert light energy into chemical energy. This process primarily occurs in the chloroplasts of plant cells.', question='Where does photosynthesis primarily occur in plant cells?', answer='Photosynthesis primarily occurs in the mitochondria of plant cells.', explanation='The context mentions that photosynthesis primarily occurs in the chloroplasts of plant cells, so the answer is incorrect.', label='No')]), num_thread=1)\n"
     ]
    }
   ],
   "source": [
    "config = RaterClassificationConfig(\n",
    "    flow_name=\"RaterFlow\",\n",
    "    model_config=OpenAIModelConfig(num_call=3, response_format={\"type\": \"json_object\"}),\n",
    "    label2score={\"Yes\": 1.0, \"No\": 0.0})\n",
    "\n",
    "with OpScope(name=\"JSONFlow\"):\n",
    "    client = RaterClient(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the client\n",
    "\n",
    "Then we can run the client. For each item in the `raw_input`, the Client will generate an explanation and a final label `Yes` or `No`. The label is decided by taking the majority votes from sampling the LLM output 3 times, which improved stability and self-consistency compared with outputting only one time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:05<00:00,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'output': [{'average_score': 1.0,\n",
      "              'error': 'No errors.',\n",
      "              'majority_vote': 'yes',\n",
      "              'response': [{'answer': 'The largest ocean on Earth is the '\n",
      "                                      'Pacific Ocean.',\n",
      "                            'context': 'The Pacific Ocean is the largest and '\n",
      "                                       \"deepest of Earth's oceanic divisions. \"\n",
      "                                       'It extends from the Arctic Ocean in '\n",
      "                                       'the north to the Southern Ocean in the '\n",
      "                                       'south.',\n",
      "                            'label': 'Yes',\n",
      "                            'question': 'What is the largest ocean on Earth?'},\n",
      "                           {'answer': 'The largest ocean on Earth is the '\n",
      "                                      'Pacific Ocean.',\n",
      "                            'context': 'The Pacific Ocean is the largest and '\n",
      "                                       \"deepest of Earth's oceanic divisions. \"\n",
      "                                       'It extends from the Arctic Ocean in '\n",
      "                                       'the north to the Southern Ocean in the '\n",
      "                                       'south.',\n",
      "                            'label': 'Yes',\n",
      "                            'question': 'What is the largest ocean on Earth?'},\n",
      "                           {'answer': 'The largest ocean on Earth is the '\n",
      "                                      'Pacific Ocean.',\n",
      "                            'context': 'The Pacific Ocean is the largest and '\n",
      "                                       \"deepest of Earth's oceanic divisions. \"\n",
      "                                       'It extends from the Arctic Ocean in '\n",
      "                                       'the north to the Southern Ocean in the '\n",
      "                                       'south.',\n",
      "                            'label': 'Yes',\n",
      "                            'question': 'What is the largest ocean on Earth?'}],\n",
      "              'scores': [1.0, 1.0, 1.0],\n",
      "              'votes': ['yes', 'yes', 'yes']}],\n",
      "  'root': <uniflow.node.Node object at 0x7f7e95f229b0>},\n",
      " {'output': [{'average_score': 0.3333333333333333,\n",
      "              'error': 'No errors.',\n",
      "              'majority_vote': 'no',\n",
      "              'response': [{'answer': 'Shakespeare wrote 31 plays.',\n",
      "                            'context': 'Shakespeare, a renowned English '\n",
      "                                       'playwright and poet, wrote 39 plays '\n",
      "                                       'during his lifetime. His works include '\n",
      "                                       \"famous plays like 'Hamlet' and 'Romeo \"\n",
      "                                       \"and Juliet'.\",\n",
      "                            'explanation': 'The context explicitly states that '\n",
      "                                           'Shakespeare wrote 39 plays, so the '\n",
      "                                           'answer is incorrect.',\n",
      "                            'label': 'No',\n",
      "                            'question': 'How many plays did Shakespeare '\n",
      "                                        'write?'},\n",
      "                           {'answer': 'Shakespeare wrote 31 plays.',\n",
      "                            'context': 'Shakespeare, a renowned English '\n",
      "                                       'playwright and poet, wrote 39 plays '\n",
      "                                       'during his lifetime. His works include '\n",
      "                                       \"famous plays like 'Hamlet' and 'Romeo \"\n",
      "                                       \"and Juliet'.\",\n",
      "                            'explanation': 'The context explicitly states that '\n",
      "                                           'Shakespeare wrote 39 plays, so the '\n",
      "                                           'answer is incorrect.',\n",
      "                            'label': 'No',\n",
      "                            'question': 'How many plays did Shakespeare '\n",
      "                                        'write?'},\n",
      "                           {'answer': 'The Eiffel Tower was constructed in '\n",
      "                                      '1889.',\n",
      "                            'context': 'The Eiffel Tower, located in Paris, '\n",
      "                                       'France, is one of the most famous '\n",
      "                                       'landmarks in the world. It was '\n",
      "                                       'constructed in 1889 and stands at a '\n",
      "                                       'height of 324 meters.',\n",
      "                            'explanation': 'The context explicitly mentions '\n",
      "                                           'that the Eiffel Tower was '\n",
      "                                           'constructed in 1889, so the answer '\n",
      "                                           'is correct.',\n",
      "                            'label': 'Yes',\n",
      "                            'question': 'When was the Eiffel Tower '\n",
      "                                        'constructed?'}],\n",
      "              'scores': [0.0, 0.0, 1.0],\n",
      "              'votes': ['no', 'no', 'yes']}],\n",
      "  'root': <uniflow.node.Node object at 0x7f7e95f235e0>},\n",
      " {'output': [{'average_score': 0.0,\n",
      "              'error': 'No errors.',\n",
      "              'majority_vote': 'no',\n",
      "              'response': [{'answer': 'The human brain is responsible for '\n",
      "                                      'physical movement.',\n",
      "                            'context': 'The human brain is an intricate organ '\n",
      "                                       'responsible for intelligence, memory, '\n",
      "                                       'and emotions. It is made up of '\n",
      "                                       'approximately 86 billion neurons.',\n",
      "                            'explanation': 'The answer is incorrect as the '\n",
      "                                           'context explicitly mentions that '\n",
      "                                           'the human brain is responsible for '\n",
      "                                           'intelligence, memory, and '\n",
      "                                           'emotions, not just physical '\n",
      "                                           'movement.',\n",
      "                            'label': 'No',\n",
      "                            'question': 'What is the human brain responsible '\n",
      "                                        'for?'},\n",
      "                           {'answer': 'The human brain is responsible for '\n",
      "                                      'physical movement.',\n",
      "                            'context': 'The human brain is an intricate organ '\n",
      "                                       'responsible for intelligence, memory, '\n",
      "                                       'and emotions. It is made up of '\n",
      "                                       'approximately 86 billion neurons.',\n",
      "                            'explanation': 'The answer is incorrect as the '\n",
      "                                           'context clearly states that the '\n",
      "                                           'human brain is responsible for '\n",
      "                                           'intelligence, memory, and '\n",
      "                                           'emotions, not physical movement.',\n",
      "                            'label': 'No',\n",
      "                            'question': 'What is the human brain responsible '\n",
      "                                        'for?'},\n",
      "                           {'answer': 'The human brain is responsible for '\n",
      "                                      'physical movement.',\n",
      "                            'context': 'The human brain is an intricate organ '\n",
      "                                       'responsible for intelligence, memory, '\n",
      "                                       'and emotions. It is made up of '\n",
      "                                       'approximately 86 billion neurons.',\n",
      "                            'explanation': 'The answer is incorrect as the '\n",
      "                                           'context explicitly states that the '\n",
      "                                           'human brain is responsible for '\n",
      "                                           'intelligence, memory, and '\n",
      "                                           'emotions, not just physical '\n",
      "                                           'movement.',\n",
      "                            'label': 'No',\n",
      "                            'question': 'What is the human brain responsible '\n",
      "                                        'for?'}],\n",
      "              'scores': [0.0, 0.0, 0.0],\n",
      "              'votes': ['no', 'no', 'no']}],\n",
      "  'root': <uniflow.node.Node object at 0x7f7e945c80a0>}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output = client.run(data)\n",
    "pprint.pprint(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that model response is a json object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'The largest ocean on Earth is the Pacific Ocean.',\n",
      " 'context': \"The Pacific Ocean is the largest and deepest of Earth's oceanic \"\n",
      "            'divisions. It extends from the Arctic Ocean in the north to the '\n",
      "            'Southern Ocean in the south.',\n",
      " 'label': 'Yes',\n",
      " 'question': 'What is the largest ocean on Earth?'}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(output[0][\"output\"][0][\"response\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data 0 has majority vote \u001b[31myes\u001b[0m and average score \u001b[34m1.0\u001b[0m\n",
      "data 1 has majority vote \u001b[31mno\u001b[0m and average score \u001b[34m0.3333333333333333\u001b[0m\n",
      "data 2 has majority vote \u001b[31mno\u001b[0m and average score \u001b[34m0.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "for idx, o in enumerate(output):\n",
    "    majority_vote = o['output'][0]['majority_vote']\n",
    "    average_score = o['output'][0]['average_score']\n",
    "    print(f\"data {idx} has majority vote \\033[31m{majority_vote}\\033[0m and average score \\033[34m{average_score}\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up config: Text format\n",
    "\n",
    "Follow the previous setting we change `response_format={\"type\": \"text\"}` passed to `OpenAIModelConfig`, so model will output plain text instead of json object. In this case, AutoRater will use a regex to match label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RaterConfig(flow_name='RaterFlow', model_config={'model_name': 'gpt-3.5-turbo-1106', 'model_server': 'OpenAIModelServer', 'num_call': 3, 'temperature': 0.9, 'response_format': {'type': 'text'}}, label2score={'Yes': 1.0, 'No': 0.0}, guided_prompt_template=GuidedPrompt(instruction='Rate the answer based on the question and the context.\\n        Follow the format of the examples below to include context, question, answer, and label in the response.\\n        The response should not include examples in the prompt.', examples=[Context(context='The Eiffel Tower, located in Paris, France, is one of the most famous landmarks in the world. It was constructed in 1889 and stands at a height of 324 meters.', question='When was the Eiffel Tower constructed?', answer='The Eiffel Tower was constructed in 1889.', explanation='The context explicitly mentions that the Eiffel Tower was constructed in 1889, so the answer is correct.', label='Yes'), Context(context='Photosynthesis is a process used by plants to convert light energy into chemical energy. This process primarily occurs in the chloroplasts of plant cells.', question='Where does photosynthesis primarily occur in plant cells?', answer='Photosynthesis primarily occurs in the mitochondria of plant cells.', explanation='The context mentions that photosynthesis primarily occurs in the chloroplasts of plant cells, so the answer is incorrect.', label='No')]), num_thread=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:02<00:00,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'output': [{'average_score': 1.0,\n",
      "              'error': 'No errors.',\n",
      "              'majority_vote': 'yes',\n",
      "              'response': ['\\n'\n",
      "                           'explanation: The answer directly addresses the '\n",
      "                           'question and provides the correct information '\n",
      "                           'based on the context.\\n'\n",
      "                           'label: Yes',\n",
      "                           'label: Yes',\n",
      "                           'explanation: The context explicitly states that '\n",
      "                           'the Pacific Ocean is the largest ocean on Earth, '\n",
      "                           'so the answer is correct.\\n'\n",
      "                           'label: Yes'],\n",
      "              'scores': [1.0, 1.0, 1.0],\n",
      "              'votes': ['yes', 'yes', 'yes']}],\n",
      "  'root': <uniflow.node.Node object at 0x7f7e95f23310>},\n",
      " {'output': [{'average_score': 0.0,\n",
      "              'error': 'No errors.',\n",
      "              'majority_vote': 'no',\n",
      "              'response': ['explanation: The context explicitly states that '\n",
      "                           'Shakespeare wrote 39 plays, so the answer is '\n",
      "                           'incorrect.\\n'\n",
      "                           'label: No',\n",
      "                           'explanation: The context explicitly mentions that '\n",
      "                           'Shakespeare wrote 39 plays, so the answer is '\n",
      "                           'incorrect.\\n'\n",
      "                           'label: No',\n",
      "                           'explanation: The context explicitly states that '\n",
      "                           'Shakespeare wrote 39 plays, so the answer is '\n",
      "                           'incorrect.\\n'\n",
      "                           'label: No'],\n",
      "              'scores': [0.0, 0.0, 0.0],\n",
      "              'votes': ['no', 'no', 'no']}],\n",
      "  'root': <uniflow.node.Node object at 0x7f7e95f23550>},\n",
      " {'output': [{'average_score': 0.0,\n",
      "              'error': 'No errors.',\n",
      "              'majority_vote': 'no',\n",
      "              'response': ['explanation: The context mentions that the human '\n",
      "                           'brain is responsible for intelligence, memory, and '\n",
      "                           'emotions, not just physical movement. So the '\n",
      "                           'answer is incorrect.\\n'\n",
      "                           'label: No',\n",
      "                           'explanation: The context mentions that the human '\n",
      "                           'brain is responsible for intelligence, memory, and '\n",
      "                           'emotions, not just physical movement, so the '\n",
      "                           'answer is incorrect.\\n'\n",
      "                           'label: No',\n",
      "                           'explanation: The context mentions that the human '\n",
      "                           'brain is responsible for intelligence, memory, and '\n",
      "                           'emotions, not physical movement. The answer is '\n",
      "                           'incorrect.\\n'\n",
      "                           'label: No'],\n",
      "              'scores': [0.0, 0.0, 0.0],\n",
      "              'votes': ['no', 'no', 'no']}],\n",
      "  'root': <uniflow.node.Node object at 0x7f7e9467a620>}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "config = RaterClassificationConfig(\n",
    "    flow_name=\"RaterFlow\",\n",
    "    model_config=OpenAIModelConfig(num_call=3, response_format={\"type\": \"text\"}),\n",
    "    label2score={\"Yes\": 1.0, \"No\": 0.0})\n",
    "\n",
    "with OpScope(name=\"TextFlow\"):\n",
    "    client = RaterClient(config)\n",
    "\n",
    "output = client.run(data)\n",
    "\n",
    "pprint.pprint(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model's responses can be distilled into majority votes, as shown below. Given the non-deterministic nature of the LLM (where each inference could yield a different output), we've enhanced stability and self-consistency by averaging results from three LLM output samplings, a notable improvement over single-output scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data 0 has majority vote \u001b[31myes\u001b[0m and average score \u001b[34m1.0\u001b[0m\n",
      "data 1 has majority vote \u001b[31mno\u001b[0m and average score \u001b[34m0.0\u001b[0m\n",
      "data 2 has majority vote \u001b[31mno\u001b[0m and average score \u001b[34m0.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "for idx, o in enumerate(output):\n",
    "    majority_vote = o['output'][0]['majority_vote']\n",
    "    average_score = o['output'][0]['average_score']\n",
    "    print(f\"data {idx} has majority vote \\033[31m{majority_vote}\\033[0m and average score \\033[34m{average_score}\\033[0m\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uniflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
