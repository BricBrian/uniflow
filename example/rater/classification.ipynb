{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use `AutoRater` to Evaluate Answer Completeness and Accuracy for Given Questions\n",
    "\n",
    "In this example, we will show you how to use AutoRater to verify the correctness of an answer to a given question and context pairs.\n",
    "\n",
    "### Before running the code\n",
    "\n",
    "You will need to `uniflow` conda environment to run this notebook. You can set up the environment following the instruction: https://github.com/CambioML/uniflow/tree/main#installation.\n",
    "\n",
    "Next, you will need a valid [OpenAI API key](https://platform.openai.com/api-keys) to run the code. Once you have the key, set it as the environment variable `OPENAI_API_KEY` within a `.env` file in the root directory of this repository. For more details, see this [instruction](https://github.com/CambioML/uniflow/tree/main#api-keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the dependency\n",
    "First, we set system paths and import libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayn/miniconda3/envs/uniflow/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\".\")\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "import pprint\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display\n",
    "\n",
    "from uniflow.flow.client import RaterClient\n",
    "from uniflow.flow.config  import RaterClassificationConfig\n",
    "from uniflow.op.model.model_config  import OpenAIModelConfig\n",
    "from uniflow.op.prompt_schema import Context\n",
    "from uniflow.op.op import OpScope\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the input data\n",
    "\n",
    "We use three example raw inputs. Each one is a tuple consisting of context, question, and answer to be labeled. The ground truth label of the first one is 'correct', and the others are 'incorrect'. Then, we use the `Context` class to wrap them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_input = [\n",
    "    (\"The Pacific Ocean is the largest and deepest of Earth's oceanic divisions. It extends from the Arctic Ocean in the north to the Southern Ocean in the south.\",\n",
    "     \"What is the largest ocean on Earth?\",\n",
    "     \"The largest ocean on Earth is the Pacific Ocean.\"), # correct\n",
    "    (\"Shakespeare, a renowned English playwright and poet, wrote 39 plays during his lifetime. His works include famous plays like 'Hamlet' and 'Romeo and Juliet'.\",\n",
    "     \"How many plays did Shakespeare write?\",\n",
    "     \"Shakespeare wrote 31 plays.\"), # incorrect\n",
    "    (\"The human brain is an intricate organ responsible for intelligence, memory, and emotions. It is made up of approximately 86 billion neurons.\",\n",
    "     \"What is the human brain responsible for?\",\n",
    "     \"The human brain is responsible for physical movement.\"), # incorrect\n",
    "]\n",
    "\n",
    "data = [\n",
    "    Context(context=c[0], question=c[1], answer=c[2])\n",
    "    for c in raw_input\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the config: JSON format\n",
    "\n",
    "In this example, we will use the [`OpenAIModelConfig`](https://github.com/CambioML/uniflow/blob/main/uniflow/model/config.py#L17) as the default LLM to generate questions and answers. If you want to use open-source models, you can replace the `OpenAIConfig` and `OpenAIModelConfig` with `HuggingfaceConfig` and [`HuggingfaceModelConfig`](https://github.com/CambioML/uniflow/blob/main/uniflow/model/config.py#L27).\n",
    "\n",
    "We use the default `guided_prompt` in `RaterClassificationConfig`, which includes two examples, labeled 'Yes' and 'No'. The default examples are also encapsulated within the `Context` class, which has fields for context, question, answer (and label), aligning with the input data format.\n",
    "\n",
    "The response format is JSON, enabling the model to return a JSON object as output rather than plain text. This facilitates more convenient processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RaterConfig(flow_name='RaterFlow', model_config={'model_name': 'gpt-3.5-turbo-1106', 'model_server': 'OpenAIModelServer', 'num_call': 3, 'temperature': 0.9, 'response_format': {'type': 'json_object'}}, label2score={'Yes': 1.0, 'No': 0.0}, guided_prompt_template=GuidedPrompt(instruction='\\n        # Task: Evaluate the appropriateness of a given answer based on a provided context and question.\\n        ## Input:\\n        1. context: A brief text containing key information.\\n        2. question: A query related to the context, testing knowledge that can be inferred or directly obtained from it.\\n        3. answer: A response to the question.\\n        ## Evaluation Criteria: Decide if the answer correctly addresses the context and question. Use these labels: \"Yes\" or \"No\".\\n        ## Response Format:\\n        1. label: Your judgment (Yes or No).\\n        2. explanation: Reasoning behind your judgment, explaining why the answer is appropriate or not.\\n        ## Note: Use the below example only for demonstration, do not include in the final response. And your response should only include two fields: \"explanation\" and \"label\".\\n        ', examples=[Context(context='The Eiffel Tower, located in Paris, France, is one of the most famous landmarks in the world. It was constructed in 1889 and stands at a height of 324 meters.', question='When was the Eiffel Tower constructed?', answer='The Eiffel Tower was constructed in 1889.', explanation='The context explicitly mentions that the Eiffel Tower was constructed in 1889, so the answer is correct.', label='Yes'), Context(context='Photosynthesis is a process used by plants to convert light energy into chemical energy. This process primarily occurs in the chloroplasts of plant cells.', question='Where does photosynthesis primarily occur in plant cells?', answer='Photosynthesis primarily occurs in the mitochondria of plant cells.', explanation='The context mentions that photosynthesis primarily occurs in the chloroplasts of plant cells, so the answer is incorrect.', label='No')]), num_thread=1)\n"
     ]
    }
   ],
   "source": [
    "config = RaterClassificationConfig(\n",
    "    flow_name=\"RaterFlow\",\n",
    "    model_config=OpenAIModelConfig(num_call=3, response_format={\"type\": \"json_object\"}),\n",
    "    label2score={\"Yes\": 1.0, \"No\": 0.0})\n",
    "\n",
    "with OpScope(name=\"JSONFlow\"):\n",
    "    client = RaterClient(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the client\n",
    "\n",
    "Then we can run the client. For each item in the `raw_input`, the Client will generate an explanation and a final label, either `Yes` or `No`. The label is determined by taking the majority vote from three samples of the LLM's output, which improves stability and self-consistency compared to generating a single output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:31<00:00, 10.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'output': [{'average_score': 1.0,\n",
      "              'error': 'No errors.',\n",
      "              'majority_vote': 'yes',\n",
      "              'response': [{'explanation': 'The answer directly addresses the '\n",
      "                                           'question by correctly identifying '\n",
      "                                           'the Pacific Ocean as the largest '\n",
      "                                           'ocean on Earth, matching the '\n",
      "                                           'information provided in the '\n",
      "                                           'context.',\n",
      "                            'label': 'Yes'},\n",
      "                           {'explanation': 'The context clearly states that '\n",
      "                                           'the Pacific Ocean is the largest '\n",
      "                                           \"and deepest of Earth's oceanic \"\n",
      "                                           'divisions, so the answer is '\n",
      "                                           'correct.',\n",
      "                            'label': 'Yes'},\n",
      "                           {'explanation': 'The answer correctly addresses the '\n",
      "                                           'question by stating that the '\n",
      "                                           'largest ocean on Earth is the '\n",
      "                                           'Pacific Ocean, which is directly '\n",
      "                                           'supported by the context.',\n",
      "                            'label': 'Yes'}],\n",
      "              'scores': [1.0, 1.0, 1.0],\n",
      "              'votes': ['yes', 'yes', 'yes']}],\n",
      "  'root': <uniflow.node.Node object at 0x7f45f1922f80>},\n",
      " {'output': [{'average_score': 0.0,\n",
      "              'error': 'No errors.',\n",
      "              'majority_vote': 'no',\n",
      "              'response': [{'explanation': 'The context explicitly mentions '\n",
      "                                           'that Shakespeare wrote 39 plays, '\n",
      "                                           'so the answer of 31 plays is '\n",
      "                                           'incorrect.',\n",
      "                            'label': 'No'},\n",
      "                           {'explanation': 'The context explicitly states that '\n",
      "                                           'Shakespeare wrote 39 plays, so the '\n",
      "                                           'answer is incorrect.',\n",
      "                            'label': 'No'},\n",
      "                           {'explanation': 'The context explicitly states that '\n",
      "                                           'Shakespeare wrote 39 plays, so the '\n",
      "                                           'answer is incorrect.',\n",
      "                            'label': 'No'}],\n",
      "              'scores': [0.0, 0.0, 0.0],\n",
      "              'votes': ['no', 'no', 'no']}],\n",
      "  'root': <uniflow.node.Node object at 0x7f460a07f5e0>},\n",
      " {'output': [{'average_score': 0.0,\n",
      "              'error': 'No errors.',\n",
      "              'majority_vote': 'no',\n",
      "              'response': [{'explanation': 'The answer is incorrect because '\n",
      "                                           'the context explicitly states that '\n",
      "                                           'the human brain is responsible for '\n",
      "                                           'intelligence, memory, and '\n",
      "                                           'emotions, not just physical '\n",
      "                                           'movement.',\n",
      "                            'label': 'No'},\n",
      "                           {'explanation': 'The answer does not correctly '\n",
      "                                           'address the context and question '\n",
      "                                           'as it mentions physical movement '\n",
      "                                           'instead of intelligence, memory, '\n",
      "                                           'and emotions. Therefore, the '\n",
      "                                           'answer is inappropriate.',\n",
      "                            'label': 'No'},\n",
      "                           {'explanation': 'The answer is incorrect because '\n",
      "                                           'the context explicitly mentions '\n",
      "                                           'that the human brain is '\n",
      "                                           'responsible for intelligence, '\n",
      "                                           'memory, and emotions, not just '\n",
      "                                           'physical movement.',\n",
      "                            'label': 'No'}],\n",
      "              'scores': [0.0, 0.0, 0.0],\n",
      "              'votes': ['no', 'no', 'no']}],\n",
      "  'root': <uniflow.node.Node object at 0x7f45f194c1c0>}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output = client.run(data)\n",
    "pprint.pprint(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that model response is a JSON object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'explanation': 'The answer directly addresses the question by correctly '\n",
      "                'identifying the Pacific Ocean as the largest ocean on Earth, '\n",
      "                'matching the information provided in the context.',\n",
      " 'label': 'Yes'}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(output[0][\"output\"][0][\"response\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model's responses can be distilled into majority votes, as shown below. Given the non-deterministic nature of the LLM (where each inference could yield a different output), we've enhanced stability and self-consistency by averaging results from three LLM output samplings, a notable improvement over single-output scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data 0 has majority vote \u001b[31myes\u001b[0m and average score \u001b[34m1.0\u001b[0m\n",
      "data 1 has majority vote \u001b[31mno\u001b[0m and average score \u001b[34m0.0\u001b[0m\n",
      "data 2 has majority vote \u001b[31mno\u001b[0m and average score \u001b[34m0.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "for idx, o in enumerate(output):\n",
    "    majority_vote = o['output'][0]['majority_vote']\n",
    "    average_score = o['output'][0]['average_score']\n",
    "    print(f\"data {idx} has majority vote \\033[31m{majority_vote}\\033[0m and average score \\033[34m{average_score}\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the config: Text format\n",
    "\n",
    "Following the previous settings, we changed `response_format={\"type\": \"text\"}` passed to `OpenAIModelConfig`, so the model will output plain text instead of a JSON object. In this case, AutoRater will use a regex to match the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RaterConfig(flow_name='RaterFlow', model_config={'model_name': 'gpt-3.5-turbo-1106', 'model_server': 'OpenAIModelServer', 'num_call': 3, 'temperature': 0.9, 'response_format': {'type': 'text'}}, label2score={'Yes': 1.0, 'No': 0.0}, guided_prompt_template=GuidedPrompt(instruction='\\n        # Task: Evaluate the appropriateness of a given answer based on a provided context and question.\\n        ## Input:\\n        1. context: A brief text containing key information.\\n        2. question: A query related to the context, testing knowledge that can be inferred or directly obtained from it.\\n        3. answer: A response to the question.\\n        ## Evaluation Criteria: Decide if the answer correctly addresses the context and question. Use these labels: \"Yes\" or \"No\".\\n        ## Response Format:\\n        1. label: Your judgment (Yes or No).\\n        2. explanation: Reasoning behind your judgment, explaining why the answer is appropriate or not.\\n        ## Note: Use the below example only for demonstration, do not include in the final response. And your response should only include two fields: \"explanation\" and \"label\".\\n        ', examples=[Context(context='The Eiffel Tower, located in Paris, France, is one of the most famous landmarks in the world. It was constructed in 1889 and stands at a height of 324 meters.', question='When was the Eiffel Tower constructed?', answer='The Eiffel Tower was constructed in 1889.', explanation='The context explicitly mentions that the Eiffel Tower was constructed in 1889, so the answer is correct.', label='Yes'), Context(context='Photosynthesis is a process used by plants to convert light energy into chemical energy. This process primarily occurs in the chloroplasts of plant cells.', question='Where does photosynthesis primarily occur in plant cells?', answer='Photosynthesis primarily occurs in the mitochondria of plant cells.', explanation='The context mentions that photosynthesis primarily occurs in the chloroplasts of plant cells, so the answer is incorrect.', label='No')]), num_thread=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:20<00:00,  7.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'output': [{'average_score': 1.0,\n",
      "              'error': 'No errors.',\n",
      "              'majority_vote': 'yes',\n",
      "              'response': ['explanation: The answer correctly addresses the '\n",
      "                           'question by stating that the largest ocean on '\n",
      "                           'Earth is the Pacific Ocean, which is explicitly '\n",
      "                           'mentioned in the context.\\n'\n",
      "                           'label: Yes',\n",
      "                           'explanation: The answer directly addresses the '\n",
      "                           'question by correctly stating that the Pacific '\n",
      "                           'Ocean is the largest ocean on Earth, which matches '\n",
      "                           'the information provided in the context. \\n'\n",
      "                           'label: Yes',\n",
      "                           'explanation: The answer correctly identifies the '\n",
      "                           'Pacific Ocean as the largest ocean on Earth, which '\n",
      "                           'is explicitly stated in the context. \\n'\n",
      "                           'label: Yes'],\n",
      "              'scores': [1.0, 1.0, 1.0],\n",
      "              'votes': ['yes', 'yes', 'yes']}],\n",
      "  'root': <uniflow.node.Node object at 0x7f460a07fcd0>},\n",
      " {'output': [{'average_score': 0.0,\n",
      "              'error': 'No errors.',\n",
      "              'majority_vote': 'no',\n",
      "              'response': ['explanation: The context states that Shakespeare '\n",
      "                           'wrote 39 plays during his lifetime, so the answer '\n",
      "                           'is incorrect.\\n'\n",
      "                           'label: No',\n",
      "                           'explanation: The context clearly states that '\n",
      "                           'Shakespeare wrote 39 plays, so the answer is '\n",
      "                           'incorrect.\\n'\n",
      "                           'label: No',\n",
      "                           'explanation: The context states that Shakespeare '\n",
      "                           'wrote 39 plays during his lifetime, so the answer '\n",
      "                           'is incorrect.\\n'\n",
      "                           'label: No'],\n",
      "              'scores': [0.0, 0.0, 0.0],\n",
      "              'votes': ['no', 'no', 'no']}],\n",
      "  'root': <uniflow.node.Node object at 0x7f460a07fa30>},\n",
      " {'output': [{'average_score': 0.0,\n",
      "              'error': 'No errors.',\n",
      "              'majority_vote': 'no',\n",
      "              'response': ['explanation: The context mentions that the human '\n",
      "                           'brain is responsible for intelligence, memory, and '\n",
      "                           'emotions, not just physical movement. Therefore, '\n",
      "                           'the answer is not appropriate.\\n'\n",
      "                           'label: No',\n",
      "                           'explanation: The context mentions that the human '\n",
      "                           'brain is responsible for intelligence, memory, and '\n",
      "                           'emotions, not just physical movement. The answer '\n",
      "                           'is incorrect.\\n'\n",
      "                           'label: No',\n",
      "                           'explanation: The context mentions that the human '\n",
      "                           'brain is responsible for intelligence, memory, and '\n",
      "                           'emotions, not just physical movement. Therefore, '\n",
      "                           'the answer is incorrect.\\n'\n",
      "                           'label: No'],\n",
      "              'scores': [0.0, 0.0, 0.0],\n",
      "              'votes': ['no', 'no', 'no']}],\n",
      "  'root': <uniflow.node.Node object at 0x7f460a07ea10>}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "config = RaterClassificationConfig(\n",
    "    flow_name=\"RaterFlow\",\n",
    "    model_config=OpenAIModelConfig(num_call=3, response_format={\"type\": \"text\"}),\n",
    "    label2score={\"Yes\": 1.0, \"No\": 0.0})\n",
    "\n",
    "with OpScope(name=\"TextFlow\"):\n",
    "    client = RaterClient(config)\n",
    "\n",
    "output = client.run(data)\n",
    "pprint.pprint(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model's responses can be distilled into majority votes, as shown below. Given the non-deterministic nature of the LLM (where each inference could yield a different output), we've enhanced stability and self-consistency by averaging results from three LLM output samplings, a notable improvement over single-output scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data 0 has majority vote \u001b[31myes\u001b[0m and average score \u001b[34m1.0\u001b[0m\n",
      "data 1 has majority vote \u001b[31mno\u001b[0m and average score \u001b[34m0.0\u001b[0m\n",
      "data 2 has majority vote \u001b[31mno\u001b[0m and average score \u001b[34m0.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "for idx, o in enumerate(output):\n",
    "    majority_vote = o['output'][0]['majority_vote']\n",
    "    average_score = o['output'][0]['average_score']\n",
    "    print(f\"data {idx} has majority vote \\033[31m{majority_vote}\\033[0m and average score \\033[34m{average_score}\\033[0m\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uniflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
