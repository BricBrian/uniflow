{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use AutoRater to Compare Answers to a Given Question from a Jupyter Notebook\n",
    "\n",
    "In this example, we will show you how to use autorater to compare a generated answer to a Given Question from a given jupyter notebook.\n",
    "\n",
    "### Before running the code\n",
    "\n",
    "You will need to `uniflow` conda environment to run this notebook. You can set up the environment following the instruction: https://github.com/CambioML/uniflow/tree/main#installation.\n",
    "\n",
    "Next, you will need a valid [OpenAI API key](https://platform.openai.com/api-keys) to run the code. Once you have the key, set it as the environment variable `OPENAI_API_KEY` within a `.env` file in the root directory of this repository. For more details, see this [instruction](https://github.com/CambioML/uniflow/tree/main#api-keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dependency\n",
    "First, we set system paths and import libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayn/miniconda3/envs/uniflow/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\".\")\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "import pprint\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display\n",
    "\n",
    "from uniflow.flow.client import RaterClient\n",
    "from uniflow.flow.config  import RaterForGeneratedAnswerConfig\n",
    "from uniflow.op.model.model_config  import OpenAIModelConfig\n",
    "from uniflow.op.prompt_schema import Context\n",
    "\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the input data\n",
    "\n",
    "We use 3 example data. Each one is a tuple with context, question, grounding answer and generated answer to be labeled.  Then we use `Context` class to wrap them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_input = [\n",
    "    (\"Reddit is an American social news aggregation, content rating, and discussion website. Registered users submit content to the site such as links, text posts, images, and videos, which are then voted up or down by other members.\",\n",
    "     \"What type of content can users submit on Reddit?\",\n",
    "     \"Users can post comments on Reddit.\",\n",
    "     \"Users on Reddit can submit various types of content including links, text posts, images, and videos.\"), # Better\n",
    "    (\"League of Legends (LoL), commonly referred to as League, is a 2009 multiplayer online battle arena video game developed and published by Riot Games. \",\n",
    "     \"When was League of Legends released?\",\n",
    "     \"League of Legends was released in 2009.\",\n",
    "     \"League of Legends was released in the early 2000s.\"), # Worse\n",
    "    (\"Vitamin C (also known as ascorbic acid and ascorbate) is a water-soluble vitamin found in citrus and other fruits, berries and vegetables, also sold as a dietary supplement and as a topical serum ingredient to treat melasma (dark pigment spots) and wrinkles on the face.\",\n",
    "     \"Is Vitamin C water-soluble?\",\n",
    "     \"Yes, Vitamin C is a very water-soluble vitamin.\",\n",
    "     \"Yes, Vitamin C can be dissolved in water well.\"), # Equally good\n",
    "]\n",
    "data = [\n",
    "    Context(context=c[0], question=c[1], grounding_answer=c[2], generated_answer=c[3])\n",
    "    for c in raw_input\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up config\n",
    "\n",
    "In this example, we will use the [`OpenAIModelConfig`](https://github.com/CambioML/uniflow/blob/main/uniflow/model/config.py#L17) as the default LLM to generate questions and answers. If you want to use open-source models, you can replace the `OpenAIConfig` and `OpenAIModelConfig` with `HuggingfaceConfig` and [`HuggingfaceModelConfig`](https://github.com/CambioML/uniflow/blob/main/uniflow/model/config.py#L27).\n",
    "\n",
    "We use the default `guided_prompt` in `RaterForGeneratedAnswerConfig`, which contains five examples(one shot per class), labeled by `Strong accept`, `Accept`, `Equivalent`, `Reject` and `Strong reject`. The default examples are also wrap by `Context` class with fileds of context, question, grounding answer, generated answer (and label), consistent with input data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RaterConfig(flow_name='RaterFlow', model_config={'model_name': 'gpt-3.5-turbo-1106', 'model_server': 'OpenAIModelServer', 'num_call': 3, 'temperature': 0.9, 'response_format': {'type': 'text'}}, label2score={'Strong accept': 2.0, 'Accept': 1.0, 'Equivalent': 0.0, 'Reject': -1.0, 'Strong reject': -2.0}, guided_prompt_template=GuidedPrompt(instruction='Rate the generated answer compared to the grounding answer to the question. Accept means the generated answer is better than the grounding answer and reject means worse.\\n        Follow the format of the examples below to include context, question, grounding answer, generated answer and label in the response.\\n        The response should not include examples in the prompt.', examples=[Context(context='Basic operating system features were developed in the 1950s, and more complex functions were introduced in the 1960s.', question='When were basic operating system features developed?', grounding_answer='In the 1960s, people developed some basic operating system functions.', generated_answer='Basic operating system features were developed in the 1950s.', explanation='The generated answer is much better because it correctly identifies the 1950s as the time when basic operating system features were developed', label='Strong accept'), Context(context='Early computers were built to perform a series of single tasks, like a calculator. Basic operating system could automatically run different programs in succession to speed up processing.', question='Did early computers function like modern calculators?', grounding_answer='No. Early computers were used primarily for complex calculating.', generated_answer='Yes. Early computers were built to perform a series of single tasks, similar to a calculator.', explanation=\"The generated answer is better as it correctly captures the essence of the early computers' functionality, which was to perform single tasks akin to calculators.\", label='Accept'), Context(context='Operating systems did not exist in their modern and more complex forms until the early 1960s. Hardware features were added, that enabled use of runtime libraries, interrupts, and parallel processing.', question='When did operating systems start to resemble their modern forms?', grounding_answer='Operating systems started to resemble their modern forms in the early 1960s.', generated_answer='Modern and more complex forms of operating systems began to emerge in the early 1960s.', explanation='Both answers are equally good as they accurately pinpoint the early 1960s as the period when modern operating systems began to develop.', label='Equivalent'), Context(context='Operating systems did not exist in their modern and more complex forms until the early 1960s. Hardware features were added, that enabled use of runtime libraries, interrupts, and parallel processing.', question='What features were added to hardware in the 1960s?', grounding_answer='Hardware in the 1960s saw the addition of features like runtime libraries and parallel processing.', generated_answer='The 1960s saw the addition of input output control and compatible timesharing capabilities in hardware.', explanation='The generated answer is worse because it inaccurately suggests the addition of capabilities of hardware in 1960s which is not supported by the context.', label='Reject'), Context(context='Operating systems did not exist in their modern and more complex forms until the early 1960s. When personal computers became popular in the 1980s, operating systems were made for them similar in concept to those used on larger computers.', question='When did operating systems in personal computer were similar to those used on larger computers?', grounding_answer='In 1980s, as personal computers became popular.', generated_answer='In the early 1960s, as operating system became more complex.', explanation='The generated answer is much worse as it incorrectly states the early 1960s as the period of popularity for personal computers, contradicting the context which indicates the 1980s.', label='Strong reject')]), num_thread=1)\n"
     ]
    }
   ],
   "source": [
    "config = RaterForGeneratedAnswerConfig(\n",
    "    flow_name=\"RaterFlow\",\n",
    "    model_config=OpenAIModelConfig(num_call=3, response_format={\"type\": \"text\"}),\n",
    "    label2score={\n",
    "        \"Strong accept\": 2.0,\n",
    "        \"Accept\": 1.0,\n",
    "        \"Equivalent\": 0.0,\n",
    "        \"Reject\": -1.0,\n",
    "        \"Strong reject\": -2.0,\n",
    "    }\n",
    ")\n",
    "client = RaterClient(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run client\n",
    "\n",
    "Then we can run the client. For each item in the raw_input, the Client will generate an explanation and a final label [`Strong accept`, `Accept`, `Equivalent`, `Reject`, `Strong reject`] . The label is decided by taking the majority votes from sampling the LLM output 3 times, which improved stability compared with outputting 1 time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:03<00:00,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'output': [{'average_score': 1.0,\n",
      "              'error': 'No errors.',\n",
      "              'majority_vote': 'accept',\n",
      "              'response': ['explanation: The generated answer is better as it '\n",
      "                           'accurately lists the various types of content that '\n",
      "                           'users can submit on Reddit, whereas the grounding '\n",
      "                           'answer only mentions comments.\\n'\n",
      "                           'label: Accept',\n",
      "                           'explanation: The generated answer is better as it '\n",
      "                           'provides a more comprehensive list of the types of '\n",
      "                           'content that users can submit on Reddit, compared '\n",
      "                           'to the grounding answer which only mentions '\n",
      "                           'comments.\\n'\n",
      "                           'label: Accept',\n",
      "                           'explanation: The generated answer is better as it '\n",
      "                           'provides a more comprehensive list of the types of '\n",
      "                           'content that users can submit on Reddit, compared '\n",
      "                           'to just comments mentioned in the grounding '\n",
      "                           'answer.\\n'\n",
      "                           'label: Accept'],\n",
      "              'scores': [1.0, 1.0, 1.0],\n",
      "              'votes': ['accept', 'accept', 'accept']}],\n",
      "  'root': <uniflow.node.Node object at 0x7f6d955a6890>},\n",
      " {'output': [{'average_score': -1.6666666666666667,\n",
      "              'error': 'No errors.',\n",
      "              'majority_vote': 'strong reject',\n",
      "              'response': ['explanation: The generated answer is worse as it '\n",
      "                           'inaccurately suggests the early 2000s as the '\n",
      "                           'release date of League of Legends, which '\n",
      "                           'contradicts the grounding answer and the context.\\n'\n",
      "                           'label: Strong reject',\n",
      "                           'explanation: The generated answer is worse as it '\n",
      "                           'inaccurately states the early 2000s as the release '\n",
      "                           'date of League of Legends, which contradicts the '\n",
      "                           'grounding answer. \\n'\n",
      "                           'label: Reject',\n",
      "                           'explanation: The generated answer is worse as it '\n",
      "                           'inaccurately states the release of League of '\n",
      "                           'Legends in the early 2000s, which contradicts the '\n",
      "                           'context stating it was released in 2009.\\n'\n",
      "                           'label: Strong reject'],\n",
      "              'scores': [-2.0, -1.0, -2.0],\n",
      "              'votes': ['strong reject', 'reject', 'strong reject']}],\n",
      "  'root': <uniflow.node.Node object at 0x7f6d96f2f8e0>},\n",
      " {'output': [{'average_score': 0.3333333333333333,\n",
      "              'error': 'No errors.',\n",
      "              'majority_vote': 'equivalent',\n",
      "              'response': ['explanation: Both answers correctly state that '\n",
      "                           'Vitamin C is water-soluble, but the grounding '\n",
      "                           'answer provides more emphasis on the high '\n",
      "                           'solubility of Vitamin C in water.\\n'\n",
      "                           'label: Accept',\n",
      "                           'explanation: Both answers are equally good as they '\n",
      "                           'accurately confirm that Vitamin C is indeed '\n",
      "                           'water-soluble.\\n'\n",
      "                           'label: Equivalent',\n",
      "                           'explanation: Both answers are equivalent as they '\n",
      "                           'both correctly confirm that Vitamin C is '\n",
      "                           'water-soluble.\\n'\n",
      "                           'label: Equivalent'],\n",
      "              'scores': [1.0, 0.0, 0.0],\n",
      "              'votes': ['accept', 'equivalent', 'equivalent']}],\n",
      "  'root': <uniflow.node.Node object at 0x7f6d955a68c0>}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output = client.run(data)\n",
    "pprint.pprint(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data 0 has majority vote \u001b[31maccept\u001b[0m and average score \u001b[34m1.0\u001b[0m\n",
      "data 1 has majority vote \u001b[31mstrong reject\u001b[0m and average score \u001b[34m-1.6666666666666667\u001b[0m\n",
      "data 2 has majority vote \u001b[31mequivalent\u001b[0m and average score \u001b[34m0.3333333333333333\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "for idx, o in enumerate(output):\n",
    "    majority_vote = o['output'][0]['majority_vote']\n",
    "    average_score = o['output'][0]['average_score']\n",
    "    print(f\"data {idx} has majority vote \\033[31m{majority_vote}\\033[0m and average score \\033[34m{average_score}\\033[0m\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uniflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
