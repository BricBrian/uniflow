{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use `AutoRater` to Compare Answers to Given Questions\n",
    "\n",
    "Do you need to evaluate the completeness and accuracy of an answer generated by a Large Language Model (LLM) compared to a pre-fomulated answer? In this example, we demonstrate how to use AutoRater for verifying the correctness of a generated answers compared to the grounding answer in relation to given question and context.\n",
    "\n",
    "### Before running the code\n",
    "\n",
    "You will need to `uniflow` conda environment to run this notebook. You can set up the environment following the instruction: https://github.com/CambioML/uniflow/tree/main#installation.\n",
    "\n",
    "Next, you will need a valid [OpenAI API key](https://platform.openai.com/api-keys) to run the code. Once you have the key, set it as the environment variable `OPENAI_API_KEY` within a `.env` file in the root directory of this repository. For more details, see this [instruction](https://github.com/CambioML/uniflow/tree/main#api-keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the dependency\n",
    "First, we set system paths and import libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayn/miniconda3/envs/uniflow/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\".\")\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "import pprint\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display\n",
    "\n",
    "from uniflow.flow.client import RaterClient\n",
    "from uniflow.flow.config  import (\n",
    "    RaterForGeneratedAnswerOpenAIGPT4Config,\n",
    "    RaterForGeneratedAnswerOpenAIGPT3p5Config\n",
    ")\n",
    "from uniflow.op.prompt_schema import Context\n",
    "from uniflow.op.op import OpScope\n",
    "\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the input data\n",
    "\n",
    "We use three sample raw inputs. Each one is a tuple consisting of context, question, ground truth answer and generated answer to be labeled. Then we use the `Context` class to wrap them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_input = [\n",
    "    (\"Reddit is an American social news aggregation, content rating, and discussion website. Registered users submit content to the site such as links, text posts, images, and videos, which are then voted up or down by other members.\",\n",
    "     \"What type of content can users submit on Reddit?\",\n",
    "     \"Users can only post text on Reddit.\",\n",
    "     \"Users on Reddit can submit various types of content including links, text posts, images, and videos.\"), # Better\n",
    "    (\"League of Legends (LoL), commonly referred to as League, is a 2009 multiplayer online battle arena video game developed and published by Riot Games. \",\n",
    "     \"When was League of Legends released?\",\n",
    "     \"League of Legends was released in 2009.\",\n",
    "     \"League of Legends was released in the early 2000s.\"), # Worse\n",
    "    (\"Vitamin C (also known as ascorbic acid and ascorbate) is a water-soluble vitamin found in citrus and other fruits, berries and vegetables, also sold as a dietary supplement and as a topical serum ingredient to treat melasma (dark pigment spots) and wrinkles on the face.\",\n",
    "     \"Is Vitamin C water-soluble?\",\n",
    "     \"Yes, Vitamin C is a very water-soluble vitamin.\",\n",
    "     \"Yes, Vitamin C can be dissolved in water well.\"), # Equally good\n",
    "]\n",
    "\n",
    "data = [\n",
    "    Context(context=c[0], question=c[1], grounding_answer=c[2], generated_answer=c[3])\n",
    "    for c in raw_input\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Output JSON format using GPT4\n",
    "\n",
    "In this example, we will use the OpenAI GPT4 Model as the default LLM. If you want to use open-source models, you can replace with Huggingface models in the Uniflow.\n",
    "\n",
    "We use the default `guided_prompt_template` in `RaterForGeneratedAnswerOpenAIGPT4Config`, which includes the four attributes:\n",
    "- `flow_name` (str): Name of the rating flow, default is \"RaterFlow\".\n",
    "- `model_config` (ModelConfig): Configuration for the GPT-4 model. Includes model name (\"gpt-4\"), the server (\"OpenAIModelServer\"), number of calls (1), temperature (0), and the response format (plain text).\n",
    "- `label2score` (Dict[str, float]): Mapping of labels to scores, default is {\"accept\": 1.0, \"equivalent\": 0.0, \"reject\": -1.0}.\n",
    "- `guided_prompt_template` (GuidedPrompt): Template for guided prompts used in rating. Includes instructions for rating, along with examples that detail the context, question, grounding answer, generated answer, label, and explanation for each case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The label2score label ['reject', 'equivalent'] not in example label.\n",
      "RaterForGeneratedAnswerOpenAIGPT4Config(flow_name='RaterFlow',\n",
      "                                        model_config=OpenAIModelConfig(model_name='gpt-4',\n",
      "                                                                       model_server='OpenAIModelServer',\n",
      "                                                                       num_call=1,\n",
      "                                                                       temperature=0,\n",
      "                                                                       response_format={'type': 'text'}),\n",
      "                                        label2score={'accept': 1.0,\n",
      "                                                     'equivalent': 0.0,\n",
      "                                                     'reject': -1.0},\n",
      "                                        guided_prompt_template=GuidedPrompt(instruction=\"\\n            Compare two answers: a generated answer and a grounding answer based on a provided context and question.\\n            There are few annotated examples below, consisting of context, question, grounding answer, generated answer, explanation and label.\\n            If generated answer is better, you should give a higher score and vise versa. Check label to score dictionary: [('accept', 1.0), ('equivalent', 0.0), ('reject', -1.0)].\\n            Your response should only focus on the unlabeled sample, including two fields: explanation and label (one of ['accept', 'equivalent', 'reject']).\\n            \", examples=[Context(context='Early computers were built to perform a series of single tasks, like a calculator.', question='Did early computers function like modern calculators?', grounding_answer='No. Early computers were used primarily for complex calculating.', generated_answer='Yes. Early computers were built to perform a series of single tasks, similar to a calculator.', explanation='The generated answer is better because it correctly figures out early computers was used to perform single tasks akin to calculators and asigned a score of 1.0.', label='accept')]),\n",
      "                                        num_thread=1)\n"
     ]
    }
   ],
   "source": [
    "config = RaterForGeneratedAnswerOpenAIGPT4Config()\n",
    "pprint.pprint(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want the response format to be JSON, we need to update two aspects of the default config:\n",
    "1. Change the `model_name` to \"gpt-4-1106-preview\", which is the only GPT-4 model that supports the JSON format.\n",
    "1. Change the `response_format` to a `json_object`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.model_config.model_name = \"gpt-4-1106-preview\"\n",
    "config.model_config.response_format = {\"type\": \"json_object\"}\n",
    "config.model_config.num_call = 1\n",
    "config.model_config.temperature = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can initialize a client. Since we will demonstrate multiple raters in the notebook, we will initialize them under different operation name scopes.\n",
    "\n",
    "NOTE: The printed information `\"The label2score label ['reject', 'equivalent'] not in example label.\"` is because we only pass one example (label=`accept`) in default `guided_prompt_template` to reduce token consumption when using GPT-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The label2score label ['reject', 'equivalent'] not in example label.\n",
      "RaterConfig(flow_name='RaterFlow', model_config={'model_name': 'gpt-4-1106-preview', 'model_server': 'OpenAIModelServer', 'num_call': 1, 'temperature': 0.0, 'response_format': {'type': 'json_object'}}, label2score={'accept': 1.0, 'equivalent': 0.0, 'reject': -1.0}, guided_prompt_template=GuidedPrompt(instruction=\"\\n            Compare two answers: a generated answer and a grounding answer based on a provided context and question.\\n            There are few annotated examples below, consisting of context, question, grounding answer, generated answer, explanation and label.\\n            If generated answer is better, you should give a higher score and vise versa. Check label to score dictionary: [('accept', 1.0), ('equivalent', 0.0), ('reject', -1.0)].\\n            Your response should only focus on the unlabeled sample, including two fields: explanation and label (one of ['accept', 'equivalent', 'reject']).\\n            \", examples=[Context(context='Early computers were built to perform a series of single tasks, like a calculator.', question='Did early computers function like modern calculators?', grounding_answer='No. Early computers were used primarily for complex calculating.', generated_answer='Yes. Early computers were built to perform a series of single tasks, similar to a calculator.', explanation='The generated answer is better because it correctly figures out early computers was used to perform single tasks akin to calculators and asigned a score of 1.0.', label='accept')]), num_thread=1)\n"
     ]
    }
   ],
   "source": [
    "with OpScope(name=\"JSONFlow\"):\n",
    "    client = RaterClient(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the client\n",
    "\n",
    "Then, we can run the client. For each item in the raw input, the Client will generate an explanation and a final label in [`Accept`, `Equivalent`, `Reject`]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:08<00:00,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'output': [{'average_score': 1.0,\n",
      "              'error': 'No errors.',\n",
      "              'majority_vote': 'accept',\n",
      "              'response': [{'explanation': 'The grounding answer is incorrect '\n",
      "                                           'because it states that users can '\n",
      "                                           'only post text on Reddit, which '\n",
      "                                           'contradicts the context provided '\n",
      "                                           'that clearly states users can '\n",
      "                                           'submit links, text posts, images, '\n",
      "                                           'and videos. The generated answer '\n",
      "                                           'accurately reflects the context by '\n",
      "                                           'listing all the types of content '\n",
      "                                           'that can be submitted on Reddit. '\n",
      "                                           'Therefore, the generated answer is '\n",
      "                                           'better.',\n",
      "                            'label': 'accept'}],\n",
      "              'scores': [1.0],\n",
      "              'votes': ['accept']}],\n",
      "  'root': <uniflow.node.Node object at 0x7f04e7caf910>},\n",
      " {'output': [{'average_score': -1.0,\n",
      "              'error': 'No errors.',\n",
      "              'majority_vote': 'reject',\n",
      "              'response': [{'explanation': 'The grounding answer is more '\n",
      "                                           'accurate because it provides the '\n",
      "                                           'specific year (2009) when League '\n",
      "                                           'of Legends was released, which '\n",
      "                                           'directly answers the question. The '\n",
      "                                           'generated answer is less precise, '\n",
      "                                           \"only stating 'the early 2000s,' \"\n",
      "                                           'which could imply a range of years '\n",
      "                                           'and does not pinpoint the exact '\n",
      "                                           'release year.',\n",
      "                            'label': 'reject'}],\n",
      "              'scores': [-1.0],\n",
      "              'votes': ['reject']}],\n",
      "  'root': <uniflow.node.Node object at 0x7f04e7cafb80>},\n",
      " {'output': [{'average_score': 0.0,\n",
      "              'error': 'No errors.',\n",
      "              'majority_vote': 'equivalent',\n",
      "              'response': [{'explanation': 'The generated answer is equivalent '\n",
      "                                           'to the grounding answer as both '\n",
      "                                           'correctly affirm that Vitamin C is '\n",
      "                                           'water-soluble. The phrasing is '\n",
      "                                           'slightly different but the meaning '\n",
      "                                           'is the same.',\n",
      "                            'label': 'equivalent'}],\n",
      "              'scores': [0.0],\n",
      "              'votes': ['equivalent']}],\n",
      "  'root': <uniflow.node.Node object at 0x7f04e7cd5270>}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output = client.run(data)\n",
    "pprint.pprint(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that model response is a JSON object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'explanation': 'The grounding answer is incorrect because it states that '\n",
      "                'users can only post text on Reddit, which contradicts the '\n",
      "                'context provided that clearly states users can submit links, '\n",
      "                'text posts, images, and videos. The generated answer '\n",
      "                'accurately reflects the context by listing all the types of '\n",
      "                'content that can be submitted on Reddit. Therefore, the '\n",
      "                'generated answer is better.',\n",
      " 'label': 'accept'}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(output[0][\"output\"][0][\"response\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only sample LLM once so the majority vote is the only label for each item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data 0 has label \u001b[31maccept\u001b[0m and score \u001b[34m1.0\u001b[0m\n",
      "data 1 has label \u001b[31mreject\u001b[0m and score \u001b[34m-1.0\u001b[0m\n",
      "data 2 has label \u001b[31mequivalent\u001b[0m and score \u001b[34m0.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "for idx, o in enumerate(output):\n",
    "    majority_vote = o['output'][0]['majority_vote']\n",
    "    average_score = o['output'][0]['average_score']\n",
    "    print(f\"data {idx} has label \\033[31m{majority_vote}\\033[0m and score \\033[34m{average_score}\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Output text format using GPT3.5\n",
    "\n",
    "Following the previous settings, we will keep the default config `response_format={\"type\": \"text\"}`, so the model will output plain text instead of a JSON object. In this case, AutoRater will use a regex to match the label. Furthermore, we will change `num_call` to 3. This means the model will perform inference on each example three times, allowing us to take the majority vote of the ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The label2score label ['reject', 'equivalent'] not in example label.\n",
      "The label2score label ['reject', 'equivalent'] not in example label.\n",
      "RaterConfig(flow_name='RaterFlow', model_config={'model_name': 'gpt-3.5-turbo-1106', 'model_server': 'OpenAIModelServer', 'num_call': 3, 'temperature': 0.9, 'response_format': {'type': 'text'}}, label2score={'accept': 1.0, 'equivalent': 0.0, 'reject': -1.0}, guided_prompt_template=GuidedPrompt(instruction=\"\\n            # Task: Evaluate and compare two answers: a generated answer and a grounding answer based on a provided context and question.\\n            ## Input: A sample to be labeled:\\n            1. context: A brief text containing key information.\\n            2. question: A query related to the context, testing knowledge that can be inferred or directly obtained from it.\\n            3. grounding Answer: Pre-formulated, usually from human.\\n            4. generated Answer: From a language model.\\n            ## Evaluation Criteria: If generated answer is better, you should give a higher score and vise versa. Check label to score dictionary: [('accept', 1.0), ('equivalent', 0.0), ('reject', -1.0)].\\n            ## Response Format: Your response should only include two fields below:\\n            1. explanatoin: Reasoning behind your judgment, detailing why the generated answer is better, equivalent or worse.\\n            2. label: Your judgment (one of ['accept', 'equivalent', 'reject']).\\n            ## Note:\\n            Only use the example below as a few shot demonstrate but not include them in the final response. Your response should only focus on the unlabeled sample.\\n            \", examples=[Context(context='Early computers were built to perform a series of single tasks, like a calculator.', question='Did early computers function like modern calculators?', grounding_answer='No. Early computers were used primarily for complex calculating.', generated_answer='Yes. Early computers were built to perform a series of single tasks, similar to a calculator.', explanation='The generated answer is better because it correctly figures out early computers was used to perform single tasks akin to calculators and asigned a score of 1.0.', label='accept')]), num_thread=1)\n"
     ]
    }
   ],
   "source": [
    "config2 = RaterForGeneratedAnswerOpenAIGPT3p5Config()\n",
    "config2.model_config.num_call = 3\n",
    "config2.model_config.temperature = 0.9\n",
    "\n",
    "with OpScope(name=\"TextFlow\"):\n",
    "    client2 = RaterClient(config2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the client\n",
    "\n",
    "Then we can run the client. For each item in the `raw_input`, the label is determined by taking the majority vote from three samples of the LLM's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:07<00:00,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'output': [{'average_score': 1.0,\n",
      "              'error': 'No errors.',\n",
      "              'majority_vote': 'accept',\n",
      "              'response': ['explanation: The generated answer is better '\n",
      "                           'because it provides a more comprehensive and '\n",
      "                           'accurate response by including all the types of '\n",
      "                           'content users can submit on Reddit, whereas the '\n",
      "                           'grounding answer incorrectly states that users can '\n",
      "                           'only post text. Therefore, the generated answer is '\n",
      "                           'more informative and accurate, warranting a score '\n",
      "                           'of 1.0.\\n'\n",
      "                           'label: accept',\n",
      "                           'explanation: The generated answer is better as it '\n",
      "                           'provides a more comprehensive and accurate '\n",
      "                           'description of the types of content users can '\n",
      "                           'submit on Reddit, including links, text posts, '\n",
      "                           'images, and videos, whereas the grounding answer '\n",
      "                           'is too restrictive. Therefore, the generated '\n",
      "                           'answer is more informative and accurate, and I '\n",
      "                           'would assign a score of 1.0.\\n'\n",
      "                           'label: accept',\n",
      "                           'explanation: The generated answer is better as it '\n",
      "                           'correctly identifies that users can submit various '\n",
      "                           'types of content on Reddit, not just text. '\n",
      "                           'Therefore, it provides a more comprehensive and '\n",
      "                           'accurate response compared to the grounding '\n",
      "                           'answer, which only mentions text. \\n'\n",
      "                           'label: accept'],\n",
      "              'scores': [1.0, 1.0, 1.0],\n",
      "              'votes': ['accept', 'accept', 'accept']}],\n",
      "  'root': <uniflow.node.Node object at 0x7f04e7d75f30>},\n",
      " {'output': [{'average_score': -1.0,\n",
      "              'error': 'No errors.',\n",
      "              'majority_vote': 'reject',\n",
      "              'response': ['explanation: The grounding answer is more accurate '\n",
      "                           'as it provides the specific year (2009) when '\n",
      "                           'League of Legends was released, while the '\n",
      "                           'generated answer is too vague with \"early 2000s\" '\n",
      "                           'and does not provide the precise information '\n",
      "                           'required.\\n'\n",
      "                           'label: reject',\n",
      "                           'explanation: The grounding answer correctly states '\n",
      "                           'that League of Legends was released in 2009, while '\n",
      "                           'the generated answer incorrectly suggests it was '\n",
      "                           'released in the early 2000s. This inaccuracy makes '\n",
      "                           'the generated answer worse, so it is labeled as '\n",
      "                           \"'reject'.\\n\"\n",
      "                           'label: reject',\n",
      "                           'explanation: The grounding answer correctly states '\n",
      "                           'that League of Legends was released in 2009, while '\n",
      "                           'the generated answer incorrectly states \"early '\n",
      "                           '2000s\". Therefore, the grounding answer is better '\n",
      "                           'and the generated answer is worse.\\n'\n",
      "                           'label: reject'],\n",
      "              'scores': [-1.0, -1.0, -1.0],\n",
      "              'votes': ['reject', 'reject', 'reject']}],\n",
      "  'root': <uniflow.node.Node object at 0x7f050a0068f0>},\n",
      " {'output': [{'average_score': 0.0,\n",
      "              'error': 'No errors.',\n",
      "              'majority_vote': 'equivalent',\n",
      "              'response': ['explanation: The grounding answer and the '\n",
      "                           'generated answer both correctly state that Vitamin '\n",
      "                           'C is water-soluble, so they are equivalent.\\n'\n",
      "                           'label: equivalent',\n",
      "                           'explanation: The generated answer is equivalent '\n",
      "                           'because it accurately states that Vitamin C can be '\n",
      "                           'dissolved in water, which aligns with the fact '\n",
      "                           'that it is a water-soluble vitamin. The slight '\n",
      "                           'difference in wording does not affect the accuracy '\n",
      "                           'of the information.\\n'\n",
      "                           'label: equivalent',\n",
      "                           'explanation: The generated answer is equivalent '\n",
      "                           'because it accurately states that Vitamin C is '\n",
      "                           'water-soluble, though phrased differently from the '\n",
      "                           'grounding answer.\\n'\n",
      "                           'label: equivalent'],\n",
      "              'scores': [0.0, 0.0, 0.0],\n",
      "              'votes': ['equivalent', 'equivalent', 'equivalent']}],\n",
      "  'root': <uniflow.node.Node object at 0x7f04e7d77760>}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output = client2.run(data)\n",
    "pprint.pprint(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the non-deterministic nature of the LLM (where each inference could yield a different output), we've enhanced stability and self-consistency by averaging results from multiple LLM output samplings, a notable improvement over single-output scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data 0 has majority vote \u001b[31maccept\u001b[0m and average score \u001b[34m1.0\u001b[0m\n",
      "data 1 has majority vote \u001b[31mreject\u001b[0m and average score \u001b[34m-1.0\u001b[0m\n",
      "data 2 has majority vote \u001b[31mequivalent\u001b[0m and average score \u001b[34m0.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "for idx, o in enumerate(output):\n",
    "    majority_vote = o['output'][0]['majority_vote']\n",
    "    average_score = o['output'][0]['average_score']\n",
    "    print(f\"data {idx} has majority vote \\033[31m{majority_vote}\\033[0m and average score \\033[34m{average_score}\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of the notebook\n",
    "\n",
    "Check more Uniflow use cases in the [example folder](https://github.com/CambioML/uniflow/tree/main/example/model#examples)!\n",
    "\n",
    "<a href=\"https://www.cambioml.com/\" title=\"Title\">\n",
    "    <img src=\"../image/cambioml_logo_large.png\" style=\"height: 100px; display: block; margin-left: auto; margin-right: auto;\"/>\n",
    "</a>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uniflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
