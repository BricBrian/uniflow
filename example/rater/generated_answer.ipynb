{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use `AutoRater` to Compare Answers to Given Questions\n",
    "\n",
    "Do you need to evaluate the completeness and accuracy of an answer generated by a Large Language Model (LLM) compared to a pre-fomulated answer? In this example, we demonstrate how to use AutoRater for verifying the correctness of a generated answers compared to the grounding answer in relation to given question and context.\n",
    "\n",
    "### Before running the code\n",
    "\n",
    "You will need to `uniflow` conda environment to run this notebook. You can set up the environment following the instruction: https://github.com/CambioML/uniflow/tree/main#installation.\n",
    "\n",
    "Next, you will need a valid [OpenAI API key](https://platform.openai.com/api-keys) to run the code. Once you have the key, set it as the environment variable `OPENAI_API_KEY` within a `.env` file in the root directory of this repository. For more details, see this [instruction](https://github.com/CambioML/uniflow/tree/main#api-keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the dependency\n",
    "First, we set system paths and import libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayn/miniconda3/envs/uniflow/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\".\")\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "import pprint\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display\n",
    "\n",
    "from uniflow.flow.client import RaterClient\n",
    "from uniflow.flow.config  import (\n",
    "    RaterForGeneratedAnswerOpenAIGPT4Config,\n",
    "    RaterForGeneratedAnswerOpenAIGPT3p5Config\n",
    ")\n",
    "from uniflow.op.prompt_schema import Context\n",
    "from uniflow.op.op import OpScope\n",
    "\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the input data\n",
    "\n",
    "We use three sample raw inputs. Each one is a tuple consisting of context, question, ground truth answer and generated answer to be labeled. Then we use the `Context` class to wrap them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_input = [\n",
    "    (\"Reddit is an American social news aggregation, content rating, and discussion website. Registered users submit content to the site such as links, text posts, images, and videos, which are then voted up or down by other members.\",\n",
    "     \"What type of content can users submit on Reddit?\",\n",
    "     \"Users can post text on Reddit.\",\n",
    "     \"Users on Reddit can submit various types of content including links, text posts, images, and videos.\"), # Better\n",
    "    (\"League of Legends (LoL), commonly referred to as League, is a 2009 multiplayer online battle arena video game developed and published by Riot Games. \",\n",
    "     \"When was League of Legends released?\",\n",
    "     \"League of Legends was released in 2009.\",\n",
    "     \"League of Legends was released in the early 2000s.\"), # Worse\n",
    "    (\"Vitamin C (also known as ascorbic acid and ascorbate) is a water-soluble vitamin found in citrus and other fruits, berries and vegetables, also sold as a dietary supplement and as a topical serum ingredient to treat melasma (dark pigment spots) and wrinkles on the face.\",\n",
    "     \"Is Vitamin C water-soluble?\",\n",
    "     \"Yes, Vitamin C is a very water-soluble vitamin.\",\n",
    "     \"Yes, Vitamin C can be dissolved in water well.\"), # Equally good\n",
    "]\n",
    "\n",
    "data = [\n",
    "    Context(context=c[0], question=c[1], grounding_answer=c[2], generated_answer=c[3])\n",
    "    for c in raw_input\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Output JSON format using GPT4\n",
    "\n",
    "In this example, we will use the OpenAI GPT4 Model as the default LLM. If you want to use open-source models, you can replace with Huggingface models in the Uniflow.\n",
    "\n",
    "We use the default `guided_prompt` in `RaterForGeneratedAnswerOpenAIGPT4Config`, which includes the four attributes:\n",
    "- `flow_name` (str): Name of the rating flow, default is \"RaterFlow\".\n",
    "- `model_config` (ModelConfig): Configuration for the GPT-4 model. Includes model name (\"gpt-4\"), the server (\"OpenAIModelServer\"), number of calls (1), temperature (0.2), and the response format (plain text).\n",
    "- `label2score` (Dict[str, float]): Mapping of labels to scores, default is {\"strong accept\": 2.0, \"accept\": 1.0, \"equivalent\": 0.0, \"reject\": -1.0, \"strong reject\": -2.0}.\n",
    "- `guided_prompt_template` (GuidedPrompt): Template for guided prompts used in rating. Includes instructions for rating, along with examples that detail the context, question, grounding answer, generated answer, label, and explanation for each case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RaterForGeneratedAnswerOpenAIGPT4Config(flow_name='RaterFlow',\n",
      "                                        model_config=OpenAIModelConfig(model_name='gpt-4',\n",
      "                                                                       model_server='OpenAIModelServer',\n",
      "                                                                       num_call=1,\n",
      "                                                                       temperature=0.2,\n",
      "                                                                       response_format={'type': 'text'}),\n",
      "                                        label2score={'accept': 1.0,\n",
      "                                                     'equivalent': 0.0,\n",
      "                                                     'reject': -1.0,\n",
      "                                                     'strong accept': 2.0,\n",
      "                                                     'strong reject': -2.0},\n",
      "                                        guided_prompt_template=GuidedPrompt(instruction='\\n            Compare two answers: a generated answer and a grounding answer based on a provided context and question. Accept means generated is better grounding and reject means worse.\\n            There are few examples below, consist of context, question, generated answer, grounding answer, explanation and label.\\n            Your response should only focus on the unlabeled sample, including two fields: label (\"strong accept\", \"accept\", \"equivalent\", \"reject\" or \"strong reject\") and explanation.\\n             The response label should be one of the following: [\\'strong accept\\', \\'accept\\', \\'equivalent\\', \\'reject\\', \\'strong reject\\'].', examples=[Context(context='Basic operating system features were developed in the 1950s, and more complex functions were introduced in the 1960s.', question='When were basic operating system features developed?', grounding_answer='In the 1960s, people developed some basic operating system functions.', generated_answer='Basic operating system features were developed in the 1950s.', explanation='The generated answer is much better because it correctly identifies the 1950s as the time when basic operating system features were developed', label='strong accept'), Context(context='Early computers were built to perform a series of single tasks, like a calculator. Basic operating system could automatically run different programs in succession to speed up processing.', question='Did early computers function like modern calculators?', grounding_answer='No. Early computers were used primarily for complex calculating.', generated_answer='Yes. Early computers were built to perform a series of single tasks, similar to a calculator.', explanation=\"The generated answer is better as it correctly captures the essence of the early computers' functionality, which was to perform single tasks akin to calculators.\", label='accept'), Context(context='Operating systems did not exist in their modern and more complex forms until the early 1960s. Hardware features were added, that enabled use of runtime libraries, interrupts, and parallel processing.', question='When did operating systems start to resemble their modern forms?', grounding_answer='Operating systems started to resemble their modern forms in the early 1960s.', generated_answer='Modern and more complex forms of operating systems began to emerge in the early 1960s.', explanation='Both answers are equally good as they accurately pinpoint the early 1960s as the period when modern operating systems began to develop.', label='equivalent'), Context(context='Operating systems did not exist in their modern and more complex forms until the early 1960s. Hardware features were added, that enabled use of runtime libraries, interrupts, and parallel processing.', question='What features were added to hardware in the 1960s?', grounding_answer='Hardware in the 1960s saw the addition of features like runtime libraries and parallel processing.', generated_answer='The 1960s saw the addition of input output control and compatible timesharing capabilities in hardware.', explanation='The generated answer is worse because it inaccurately suggests the addition of capabilities of hardware in 1960s which is not supported by the context.', label='reject'), Context(context='Operating systems did not exist in their modern and more complex forms until the early 1960s. When personal computers became popular in the 1980s, operating systems were made for them similar in concept to those used on larger computers.', question='When did operating systems in personal computer were similar to those used on larger computers?', grounding_answer='In 1980s, as personal computers became popular.', generated_answer='In the early 1960s, as operating system became more complex.', explanation='The generated answer is much worse as it incorrectly states the early 1960s as the period of popularity for personal computers, contradicting the context which indicates the 1980s.', label='strong reject')]),\n",
      "                                        num_thread=1)\n"
     ]
    }
   ],
   "source": [
    "config = RaterForGeneratedAnswerOpenAIGPT4Config()\n",
    "pprint.pprint(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want the response format to be JSON, we need to update two aspects of the default config:\n",
    "1. Change the `model_name` to \"gpt-4-1106-preview\", which is the only GPT-4 model that supports the JSON format.\n",
    "1. Change the `response_format` to a `json_object`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.model_config.model_name = \"gpt-4-1106-preview\"\n",
    "config.model_config.response_format = {\"type\": \"json_object\"}\n",
    "config.model_config.num_call = 1\n",
    "config.model_config.temperature = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can initialize a client. Since we will demonstrate multiple raters in the notebook, we will initialize them under different operation name scopes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RaterConfig(flow_name='RaterFlow', model_config={'model_name': 'gpt-4-1106-preview', 'model_server': 'OpenAIModelServer', 'num_call': 1, 'temperature': 0.0, 'response_format': {'type': 'json_object'}}, label2score={'strong accept': 2.0, 'accept': 1.0, 'equivalent': 0.0, 'reject': -1.0, 'strong reject': -2.0}, guided_prompt_template=GuidedPrompt(instruction='\\n            Compare two answers: a generated answer and a grounding answer based on a provided context and question. Accept means generated is better grounding and reject means worse.\\n            There are few examples below, consist of context, question, generated answer, grounding answer, explanation and label.\\n            Your response should only focus on the unlabeled sample, including two fields: label (\"strong accept\", \"accept\", \"equivalent\", \"reject\" or \"strong reject\") and explanation.\\n             The response label should be one of the following: [\\'strong accept\\', \\'accept\\', \\'equivalent\\', \\'reject\\', \\'strong reject\\'].', examples=[Context(context='Basic operating system features were developed in the 1950s, and more complex functions were introduced in the 1960s.', question='When were basic operating system features developed?', grounding_answer='In the 1960s, people developed some basic operating system functions.', generated_answer='Basic operating system features were developed in the 1950s.', explanation='The generated answer is much better because it correctly identifies the 1950s as the time when basic operating system features were developed', label='strong accept'), Context(context='Early computers were built to perform a series of single tasks, like a calculator. Basic operating system could automatically run different programs in succession to speed up processing.', question='Did early computers function like modern calculators?', grounding_answer='No. Early computers were used primarily for complex calculating.', generated_answer='Yes. Early computers were built to perform a series of single tasks, similar to a calculator.', explanation=\"The generated answer is better as it correctly captures the essence of the early computers' functionality, which was to perform single tasks akin to calculators.\", label='accept'), Context(context='Operating systems did not exist in their modern and more complex forms until the early 1960s. Hardware features were added, that enabled use of runtime libraries, interrupts, and parallel processing.', question='When did operating systems start to resemble their modern forms?', grounding_answer='Operating systems started to resemble their modern forms in the early 1960s.', generated_answer='Modern and more complex forms of operating systems began to emerge in the early 1960s.', explanation='Both answers are equally good as they accurately pinpoint the early 1960s as the period when modern operating systems began to develop.', label='equivalent'), Context(context='Operating systems did not exist in their modern and more complex forms until the early 1960s. Hardware features were added, that enabled use of runtime libraries, interrupts, and parallel processing.', question='What features were added to hardware in the 1960s?', grounding_answer='Hardware in the 1960s saw the addition of features like runtime libraries and parallel processing.', generated_answer='The 1960s saw the addition of input output control and compatible timesharing capabilities in hardware.', explanation='The generated answer is worse because it inaccurately suggests the addition of capabilities of hardware in 1960s which is not supported by the context.', label='reject'), Context(context='Operating systems did not exist in their modern and more complex forms until the early 1960s. When personal computers became popular in the 1980s, operating systems were made for them similar in concept to those used on larger computers.', question='When did operating systems in personal computer were similar to those used on larger computers?', grounding_answer='In 1980s, as personal computers became popular.', generated_answer='In the early 1960s, as operating system became more complex.', explanation='The generated answer is much worse as it incorrectly states the early 1960s as the period of popularity for personal computers, contradicting the context which indicates the 1980s.', label='strong reject')]), num_thread=1)\n"
     ]
    }
   ],
   "source": [
    "with OpScope(name=\"JSONFlow\"):\n",
    "    client = RaterClient(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the client\n",
    "\n",
    "Then, we can run the client. For each item in the raw input, the Client will generate an explanation and a final label in [`Strong accept`, `Accept`, `Equivalent`, `Reject`, `Strong reject`]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:20<00:00,  6.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'output': [{'average_score': 2.0,\n",
      "              'error': 'No errors.',\n",
      "              'majority_vote': 'strong accept',\n",
      "              'response': [{'explanation': 'The generated answer provides a '\n",
      "                                           'comprehensive list of the types of '\n",
      "                                           'content that users can submit on '\n",
      "                                           'Reddit, which includes links, text '\n",
      "                                           'posts, images, and videos. This '\n",
      "                                           'answer is more complete and '\n",
      "                                           'informative compared to the '\n",
      "                                           'grounding answer, which only '\n",
      "                                           'mentions text posts and omits the '\n",
      "                                           'other types of content that can '\n",
      "                                           'also be submitted.',\n",
      "                            'label': 'strong accept'}],\n",
      "              'scores': [2.0],\n",
      "              'votes': ['strong accept']}],\n",
      "  'root': <uniflow.node.Node object at 0x7f99b61c39a0>},\n",
      " {'output': [{'average_score': -1.0,\n",
      "              'error': 'No errors.',\n",
      "              'majority_vote': 'reject',\n",
      "              'response': [{'explanation': 'The generated answer is incorrect '\n",
      "                                           \"because it vaguely states 'the \"\n",
      "                                           \"early 2000s' as the release time \"\n",
      "                                           'for League of Legends, which could '\n",
      "                                           'imply any year from 2000 to 2005. '\n",
      "                                           'The grounding answer is precise '\n",
      "                                           'and matches the context provided, '\n",
      "                                           'stating the correct year of '\n",
      "                                           'release, 2009.',\n",
      "                            'label': 'reject'}],\n",
      "              'scores': [-1.0],\n",
      "              'votes': ['reject']}],\n",
      "  'root': <uniflow.node.Node object at 0x7f99d0591900>},\n",
      " {'output': [{'average_score': 0.0,\n",
      "              'error': 'No errors.',\n",
      "              'majority_vote': 'equivalent',\n",
      "              'response': [{'explanation': 'Both the grounding answer and the '\n",
      "                                           'generated answer correctly state '\n",
      "                                           'that Vitamin C is water-soluble. '\n",
      "                                           \"The grounding answer uses 'very \"\n",
      "                                           \"water-soluble' while the generated \"\n",
      "                                           \"answer uses 'can be dissolved in \"\n",
      "                                           \"water well,' both of which convey \"\n",
      "                                           'the same meaning that Vitamin C '\n",
      "                                           'dissolves well in water. '\n",
      "                                           'Therefore, the answers are '\n",
      "                                           'equivalent in terms of accuracy '\n",
      "                                           'and completeness.',\n",
      "                            'label': 'equivalent'}],\n",
      "              'scores': [0.0],\n",
      "              'votes': ['equivalent']}],\n",
      "  'root': <uniflow.node.Node object at 0x7f99b61c3970>}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output = client.run(data)\n",
    "pprint.pprint(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that model response is a JSON object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'explanation': 'The generated answer provides a comprehensive list of the '\n",
      "                'types of content that users can submit on Reddit, which '\n",
      "                'includes links, text posts, images, and videos. This answer '\n",
      "                'is more complete and informative compared to the grounding '\n",
      "                'answer, which only mentions text posts and omits the other '\n",
      "                'types of content that can also be submitted.',\n",
      " 'label': 'strong accept'}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(output[0][\"output\"][0][\"response\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only sample LLM once so the majority vote is the only label for each item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data 0 has label \u001b[31mstrong accept\u001b[0m and score \u001b[34m2.0\u001b[0m\n",
      "data 1 has label \u001b[31mreject\u001b[0m and score \u001b[34m-1.0\u001b[0m\n",
      "data 2 has label \u001b[31mequivalent\u001b[0m and score \u001b[34m0.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "for idx, o in enumerate(output):\n",
    "    majority_vote = o['output'][0]['majority_vote']\n",
    "    average_score = o['output'][0]['average_score']\n",
    "    print(f\"data {idx} has label \\033[31m{majority_vote}\\033[0m and score \\033[34m{average_score}\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Output text format using GPT3.5\n",
    "\n",
    "Following the previous settings, we will keep the default config `response_format={\"type\": \"text\"}`, so the model will output plain text instead of a JSON object. In this case, AutoRater will use a regex to match the label. Furthermore, we will change `num_call` to 3. This means the model will perform inference on each example three times, allowing us to take the majority vote of the ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RaterConfig(flow_name='RaterFlow', model_config={'model_name': 'gpt-3.5-turbo-1106', 'model_server': 'OpenAIModelServer', 'num_call': 3, 'temperature': 0.9, 'response_format': {'type': 'text'}}, label2score={'strong accept': 2.0, 'accept': 1.0, 'equivalent': 0.0, 'reject': -1.0, 'strong reject': -2.0}, guided_prompt_template=GuidedPrompt(instruction=\"\\n            # Task: Evaluate and compare two answers: a generated answer and a grounding answer based on a provided context and question.\\n            ## Input: A sample to be labeled:\\n            1. context: A brief text containing key information.\\n            2. question: A query related to the context, testing knowledge that can be inferred or directly obtained from it.\\n            3. grounding Answer: Pre-formulated, usually from human.\\n            4. generated Answer: From a language model.\\n            ## Evaluation Criteria: Decide which answer is better. Use labels:\\n            1. strong accept: Generated better than Grounding\\n            2. accept: Generated somewhat better than Grounding\\n            3. equivalent: Equal quality\\n            4. reject: Generated somewhat worse than Grounding\\n            5. strong reject: Generated worse than Grounding\\n            ## Response Format: Your response should only include two fields below:\\n            1. label: Your judgment (one of the five labels mentioned above).\\n            2. explanatoin: Reasoning behind your judgment, detailing why the generated answer is better, equivalent or worse.\\n            ## Note:\\n            Only use the example below as a few shot demonstrate but not include them in the final response. Do not repeat examples.\\n             The response label should be one of the following: ['strong accept', 'accept', 'equivalent', 'reject', 'strong reject'].\", examples=[Context(context='Basic operating system features were developed in the 1950s, and more complex functions were introduced in the 1960s.', question='When were basic operating system features developed?', grounding_answer='In the 1960s, people developed some basic operating system functions.', generated_answer='Basic operating system features were developed in the 1950s.', explanation='The generated answer is much better because it correctly identifies the 1950s as the time when basic operating system features were developed', label='strong accept'), Context(context='Early computers were built to perform a series of single tasks, like a calculator. Basic operating system could automatically run different programs in succession to speed up processing.', question='Did early computers function like modern calculators?', grounding_answer='No. Early computers were used primarily for complex calculating.', generated_answer='Yes. Early computers were built to perform a series of single tasks, similar to a calculator.', explanation=\"The generated answer is better as it correctly captures the essence of the early computers' functionality, which was to perform single tasks akin to calculators.\", label='accept'), Context(context='Operating systems did not exist in their modern and more complex forms until the early 1960s. Hardware features were added, that enabled use of runtime libraries, interrupts, and parallel processing.', question='When did operating systems start to resemble their modern forms?', grounding_answer='Operating systems started to resemble their modern forms in the early 1960s.', generated_answer='Modern and more complex forms of operating systems began to emerge in the early 1960s.', explanation='Both answers are equally good as they accurately pinpoint the early 1960s as the period when modern operating systems began to develop.', label='equivalent'), Context(context='Operating systems did not exist in their modern and more complex forms until the early 1960s. Hardware features were added, that enabled use of runtime libraries, interrupts, and parallel processing.', question='What features were added to hardware in the 1960s?', grounding_answer='Hardware in the 1960s saw the addition of features like runtime libraries and parallel processing.', generated_answer='The 1960s saw the addition of input output control and compatible timesharing capabilities in hardware.', explanation='The generated answer is worse because it inaccurately suggests the addition of capabilities of hardware in 1960s which is not supported by the context.', label='reject'), Context(context='Operating systems did not exist in their modern and more complex forms until the early 1960s. When personal computers became popular in the 1980s, operating systems were made for them similar in concept to those used on larger computers.', question='When did operating systems in personal computer were similar to those used on larger computers?', grounding_answer='In 1980s, as personal computers became popular.', generated_answer='In the early 1960s, as operating system became more complex.', explanation='The generated answer is much worse as it incorrectly states the early 1960s as the period of popularity for personal computers, contradicting the context which indicates the 1980s.', label='strong reject')]), num_thread=1)\n"
     ]
    }
   ],
   "source": [
    "config2 = RaterForGeneratedAnswerOpenAIGPT3p5Config()\n",
    "config2.model_config.num_call = 3\n",
    "\n",
    "with OpScope(name=\"TextFlow\"):\n",
    "    client2 = RaterClient(config2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the client\n",
    "\n",
    "Then we can run the client. For each item in the `raw_input`, the label is determined by taking the majority vote from three samples of the LLM's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:03<00:00,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'output': [{'average_score': 2.0,\n",
      "              'error': 'No errors.',\n",
      "              'majority_vote': 'strong accept',\n",
      "              'response': ['label: strong accept\\n'\n",
      "                           'explanation: The generated answer is much better '\n",
      "                           'as it provides a comprehensive list of the types '\n",
      "                           'of content that users can submit on Reddit, '\n",
      "                           'whereas the grounding answer only mentions one '\n",
      "                           'type of content (text). The generated answer is '\n",
      "                           'more thorough and accurate.',\n",
      "                           'label: strong accept\\n'\n",
      "                           'explanation: The generated answer is much better '\n",
      "                           'as it provides a comprehensive list of the types '\n",
      "                           'of content that users can submit on Reddit, '\n",
      "                           'including links, text posts, images, and videos, '\n",
      "                           'whereas the grounding answer only mentions text '\n",
      "                           'posts.',\n",
      "                           'label: strong accept\\n'\n",
      "                           'explanation: The generated answer is much better '\n",
      "                           'as it provides a comprehensive list of the types '\n",
      "                           'of content that users can submit on Reddit, '\n",
      "                           'whereas the grounding answer only mentions text '\n",
      "                           'posts. The generated answer demonstrates a more '\n",
      "                           'thorough understanding of the range of content '\n",
      "                           'submission options on Reddit.'],\n",
      "              'scores': [2.0, 2.0, 2.0],\n",
      "              'votes': ['strong accept', 'strong accept', 'strong accept']}],\n",
      "  'root': <uniflow.node.Node object at 0x7f99b509dd80>},\n",
      " {'output': [{'average_score': -2.0,\n",
      "              'error': 'No errors.',\n",
      "              'majority_vote': 'strong reject',\n",
      "              'response': ['label: strong reject\\n'\n",
      "                           'explanation: The generated answer is significantly '\n",
      "                           'worse as it inaccurately states the release time '\n",
      "                           'of League of Legends as the early 2000s, which '\n",
      "                           'contradicts the provided context clearly '\n",
      "                           'indicating its release in 2009.',\n",
      "                           'label: strong reject\\n'\n",
      "                           'explanation: The generated answer is significantly '\n",
      "                           'worse as it inaccurately states the early 2000s as '\n",
      "                           'the release date of League of Legends, which is '\n",
      "                           'contradicted by the context indicating 2009 as the '\n",
      "                           'correct year of release.',\n",
      "                           'label: strong reject\\n'\n",
      "                           'explanation: The generated answer is significantly '\n",
      "                           'worse as it inaccurately states the early 2000s as '\n",
      "                           'the release date for League of Legends, '\n",
      "                           'contradicting the context which clearly indicates '\n",
      "                           '2009 as the year of release.'],\n",
      "              'scores': [-2.0, -2.0, -2.0],\n",
      "              'votes': ['strong reject', 'strong reject', 'strong reject']}],\n",
      "  'root': <uniflow.node.Node object at 0x7f99b61e8610>},\n",
      " {'output': [{'average_score': 0.3333333333333333,\n",
      "              'error': 'No errors.',\n",
      "              'majority_vote': 'equivalent',\n",
      "              'response': ['label: accept\\n'\n",
      "                           'explanation: Both answers correctly identify '\n",
      "                           'Vitamin C as a water-soluble vitamin. The '\n",
      "                           'generated answer may be slightly more verbose, but '\n",
      "                           'it conveys the same information as the grounding '\n",
      "                           'answer. Therefore, the generated answer is '\n",
      "                           'slightly better but still acceptable.',\n",
      "                           'label: equivalent\\n'\n",
      "                           'explanation: Both answers correctly affirm that '\n",
      "                           'Vitamin C is water-soluble, so they are of equal '\n",
      "                           'quality in addressing the question.',\n",
      "                           'label: equivalent\\n'\n",
      "                           'explanation: Both answers correctly state that '\n",
      "                           'Vitamin C is water-soluble and can be dissolved in '\n",
      "                           'water, so they are of equal quality.'],\n",
      "              'scores': [1.0, 0.0, 0.0],\n",
      "              'votes': ['accept', 'equivalent', 'equivalent']}],\n",
      "  'root': <uniflow.node.Node object at 0x7f99b509f6a0>}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output = client2.run(data)\n",
    "pprint.pprint(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the non-deterministic nature of the LLM (where each inference could yield a different output), we've enhanced stability and self-consistency by averaging results from multiple LLM output samplings, a notable improvement over single-output scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data 0 has majority vote \u001b[31mstrong accept\u001b[0m and average score \u001b[34m2.0\u001b[0m\n",
      "data 1 has majority vote \u001b[31mstrong reject\u001b[0m and average score \u001b[34m-2.0\u001b[0m\n",
      "data 2 has majority vote \u001b[31mequivalent\u001b[0m and average score \u001b[34m0.3333333333333333\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "for idx, o in enumerate(output):\n",
    "    majority_vote = o['output'][0]['majority_vote']\n",
    "    average_score = o['output'][0]['average_score']\n",
    "    print(f\"data {idx} has majority vote \\033[31m{majority_vote}\\033[0m and average score \\033[34m{average_score}\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of the notebook\n",
    "\n",
    "Check more Uniflow use cases in the [example folder](https://github.com/CambioML/uniflow/tree/main/example/model#examples)!\n",
    "\n",
    "<a href=\"https://www.cambioml.com/\" title=\"Title\">\n",
    "    <img src=\"../image/cambioml_logo_large.png\" style=\"height: 100px; display: block; margin-left: auto; margin-right: auto;\"/>\n",
    "</a>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uniflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
