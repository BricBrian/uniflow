{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use `AutoRater` to Compare Answers to Given Questions\n",
    "\n",
    "In this example, we will show you how to use autorater to compare a generated answer to Given Questions.\n",
    "\n",
    "### Before running the code\n",
    "\n",
    "You will need to `uniflow` conda environment to run this notebook. You can set up the environment following the instruction: https://github.com/CambioML/uniflow/tree/main#installation.\n",
    "\n",
    "Next, you will need a valid [OpenAI API key](https://platform.openai.com/api-keys) to run the code. Once you have the key, set it as the environment variable `OPENAI_API_KEY` within a `.env` file in the root directory of this repository. For more details, see this [instruction](https://github.com/CambioML/uniflow/tree/main#api-keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the dependency\n",
    "First, we set system paths and import libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayn/miniconda3/envs/uniflow/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\".\")\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "import pprint\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display\n",
    "\n",
    "from uniflow.flow.client import RaterClient\n",
    "from uniflow.flow.config  import RaterForGeneratedAnswerConfig\n",
    "from uniflow.op.model.model_config  import OpenAIModelConfig\n",
    "from uniflow.op.prompt_schema import Context\n",
    "\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the input data\n",
    "\n",
    "We use 3 sample raw inputs. Each one is a tuple with context, question, ground truth answer and generated answer to be labeled.  Then we use the `Context` class to wrap them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_input = [\n",
    "    (\"Reddit is an American social news aggregation, content rating, and discussion website. Registered users submit content to the site such as links, text posts, images, and videos, which are then voted up or down by other members.\",\n",
    "     \"What type of content can users submit on Reddit?\",\n",
    "     \"Users can post comments on Reddit.\",\n",
    "     \"Users on Reddit can submit various types of content including links, text posts, images, and videos.\"), # Better\n",
    "    (\"League of Legends (LoL), commonly referred to as League, is a 2009 multiplayer online battle arena video game developed and published by Riot Games. \",\n",
    "     \"When was League of Legends released?\",\n",
    "     \"League of Legends was released in 2009.\",\n",
    "     \"League of Legends was released in the early 2000s.\"), # Worse\n",
    "    (\"Vitamin C (also known as ascorbic acid and ascorbate) is a water-soluble vitamin found in citrus and other fruits, berries and vegetables, also sold as a dietary supplement and as a topical serum ingredient to treat melasma (dark pigment spots) and wrinkles on the face.\",\n",
    "     \"Is Vitamin C water-soluble?\",\n",
    "     \"Yes, Vitamin C is a very water-soluble vitamin.\",\n",
    "     \"Yes, Vitamin C can be dissolved in water well.\"), # Equally good\n",
    "]\n",
    "\n",
    "data = [\n",
    "    Context(context=c[0], question=c[1], grounding_answer=c[2], generated_answer=c[3])\n",
    "    for c in raw_input\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the config\n",
    "\n",
    "In this example, we use the [`OpenAIModelConfig`](https://github.com/CambioML/uniflow/blob/main/uniflow/model/config.py#L17) as the default LLM to generate questions and answers. If you want to use open-source models, you can replace `OpenAIConfig` and `OpenAIModelConfig` with `HuggingfaceConfig` and [`HuggingfaceModelConfig`](https://github.com/CambioML/uniflow/blob/main/uniflow/model/config.py#L27).\n",
    "\n",
    "We use the default `guided_prompt` in `RaterForGeneratedAnswerConfig`, which contains five examples (one shot per class), labeled as `Strong accept`, `Accept`, `Equivalent`, `Reject` and `Strong reject`. The default examples are also wrapped in the `Context` class with fields of context, question, grounding answer, generated answer (and label), ensuring consistency with the input data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RaterConfig(flow_name='RaterFlow', model_config={'model_name': 'gpt-4-1106-preview', 'model_server': 'OpenAIModelServer', 'num_call': 3, 'temperature': 0.9, 'response_format': {'type': 'json_object'}}, label2score={'strong accept': 2.0, 'accept': 1.0, 'equivalent': 0.0, 'reject': -1.0, 'strong reject': -2.0}, guided_prompt_template=GuidedPrompt(instruction='\\n        # Task: Evaluate and compare two answers: a \"Generated Answer\" and a \"Grounding Answer\" based on a provided context and question.\\n        ## Input: A sample to be labeled:\\n        1. context: A brief text containing key information.\\n        2. question: A query related to the context, testing knowledge that can be inferred or directly obtained from it.\\n        3. grounding Answer: Pre-formulated, usually from human.\\n        4. generated Answer: From a language model.\\n        ## Evaluation Criteria: Decide which answer is better. Use labels:\\n        1. strong accept: Generated better than Grounding\\n        2. accept: Generated somewhat better than Grounding\\n        3. equivalent: Equal quality\\n        4. reject: Generated somewhat worse than Grounding\\n        5. strong reject: Generated worse than Grounding\\n        ## Response Format: Your response should only include two fields below:\\n        1. label: Your judgment (one of the five labels mentioned above).\\n        2. explanatoin: Reasoning behind your judgment, detailing why the generated answer is better, equivalent or worse.\\n        ## Note:\\n        Only use the example below as a few shot demonstrate but not include them in the final response. Do not repeat examples.\\n        # ', examples=[Context(context='Basic operating system features were developed in the 1950s, and more complex functions were introduced in the 1960s.', question='When were basic operating system features developed?', grounding_answer='In the 1960s, people developed some basic operating system functions.', generated_answer='Basic operating system features were developed in the 1950s.', explanation='The generated answer is much better because it correctly identifies the 1950s as the time when basic operating system features were developed', label='strong accept'), Context(context='Early computers were built to perform a series of single tasks, like a calculator. Basic operating system could automatically run different programs in succession to speed up processing.', question='Did early computers function like modern calculators?', grounding_answer='No. Early computers were used primarily for complex calculating.', generated_answer='Yes. Early computers were built to perform a series of single tasks, similar to a calculator.', explanation=\"The generated answer is better as it correctly captures the essence of the early computers' functionality, which was to perform single tasks akin to calculators.\", label='accept'), Context(context='Operating systems did not exist in their modern and more complex forms until the early 1960s. Hardware features were added, that enabled use of runtime libraries, interrupts, and parallel processing.', question='When did operating systems start to resemble their modern forms?', grounding_answer='Operating systems started to resemble their modern forms in the early 1960s.', generated_answer='Modern and more complex forms of operating systems began to emerge in the early 1960s.', explanation='Both answers are equally good as they accurately pinpoint the early 1960s as the period when modern operating systems began to develop.', label='equivalent'), Context(context='Operating systems did not exist in their modern and more complex forms until the early 1960s. Hardware features were added, that enabled use of runtime libraries, interrupts, and parallel processing.', question='What features were added to hardware in the 1960s?', grounding_answer='Hardware in the 1960s saw the addition of features like runtime libraries and parallel processing.', generated_answer='The 1960s saw the addition of input output control and compatible timesharing capabilities in hardware.', explanation='The generated answer is worse because it inaccurately suggests the addition of capabilities of hardware in 1960s which is not supported by the context.', label='reject'), Context(context='Operating systems did not exist in their modern and more complex forms until the early 1960s. When personal computers became popular in the 1980s, operating systems were made for them similar in concept to those used on larger computers.', question='When did operating systems in personal computer were similar to those used on larger computers?', grounding_answer='In 1980s, as personal computers became popular.', generated_answer='In the early 1960s, as operating system became more complex.', explanation='The generated answer is much worse as it incorrectly states the early 1960s as the period of popularity for personal computers, contradicting the context which indicates the 1980s.', label='strong reject')]), num_thread=1)\n"
     ]
    }
   ],
   "source": [
    "config = RaterForGeneratedAnswerConfig(\n",
    "    flow_name=\"RaterFlow\",\n",
    "    model_config=OpenAIModelConfig(model_name=\"gpt-4-1106-preview\", num_call=3, response_format={\"type\": \"json_object\"}),\n",
    "    label2score={\n",
    "        \"strong accept\": 2.0,\n",
    "        \"accept\": 1.0,\n",
    "        \"equivalent\": 0.0,\n",
    "        \"reject\": -1.0,\n",
    "        \"strong reject\": -2.0,\n",
    "    }\n",
    ")\n",
    "\n",
    "client = RaterClient(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the client\n",
    "\n",
    "Then, we can run the client. For each item in the raw input, the Client will generate an explanation and a final label [`Strong accept`, `Accept`, `Equivalent`, `Reject`, `Strong reject`]. The label is determined by taking the majority vote from three samples of the LLM output, which improves stability compared to generating a single output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:22<00:00,  7.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'output': [{'average_score': 2.0,\n",
      "              'error': 'No errors.',\n",
      "              'majority_vote': 'strong accept',\n",
      "              'response': [{'explanation': 'The generated answer provides a '\n",
      "                                           'comprehensive list of the types of '\n",
      "                                           'content users can submit on '\n",
      "                                           'Reddit, which includes links, text '\n",
      "                                           'posts, images, and videos. This '\n",
      "                                           'directly corresponds to the '\n",
      "                                           'context given. The grounding '\n",
      "                                           'answer is too narrow, mentioning '\n",
      "                                           'only comments, which may be a '\n",
      "                                           'feature of Reddit but does not '\n",
      "                                           'encompass the range of content '\n",
      "                                           'types described in the context.',\n",
      "                            'label': 'strong accept'},\n",
      "                           {'explanation': 'The generated answer is '\n",
      "                                           'significantly better than the '\n",
      "                                           'grounding answer because it '\n",
      "                                           'comprehensively lists the types of '\n",
      "                                           'content that users can submit on '\n",
      "                                           'Reddit—links, text posts, images, '\n",
      "                                           'and videos, which is in direct '\n",
      "                                           'alignment with the context '\n",
      "                                           'provided. The grounding answer '\n",
      "                                           'only mentions comments, which is a '\n",
      "                                           'subset of the interactions on '\n",
      "                                           'Reddit and not the primary content '\n",
      "                                           'forms stated in the context.',\n",
      "                            'label': 'strong accept'},\n",
      "                           {'explanation': 'The generated answer is more '\n",
      "                                           'comprehensive and detailed, '\n",
      "                                           'listing all the types of content '\n",
      "                                           'that can be submitted by users on '\n",
      "                                           'Reddit, which includes links, text '\n",
      "                                           'posts, images, and videos. On the '\n",
      "                                           'other hand, the grounding answer '\n",
      "                                           'provides an incomplete response by '\n",
      "                                           'only mentioning the ability to '\n",
      "                                           'post comments, failing to cover '\n",
      "                                           'the complete range of content '\n",
      "                                           'types as detailed in the context.',\n",
      "                            'label': 'strong accept'}],\n",
      "              'scores': [2.0, 2.0, 2.0],\n",
      "              'votes': ['strong accept', 'strong accept', 'strong accept']}],\n",
      "  'root': <uniflow.node.Node object at 0x7f9c6dab3310>},\n",
      " {'output': [{'average_score': -1.6666666666666667,\n",
      "              'error': 'No errors.',\n",
      "              'majority_vote': 'strong reject',\n",
      "              'response': [{'explanation': 'The generated answer is worse '\n",
      "                                           'because it provides an incorrect '\n",
      "                                           \"release timeframe for 'League of \"\n",
      "                                           \"Legends' by stating 'in the early \"\n",
      "                                           \"2000s,' which is a broader range \"\n",
      "                                           'and not as precise as the correct '\n",
      "                                           'year provided by the grounding '\n",
      "                                           'answer, which is 2009.',\n",
      "                            'label': 'reject'},\n",
      "                           {'explanation': 'The generated answer is much worse '\n",
      "                                           'because it provides an incorrect '\n",
      "                                           \"release timeframe for 'League of \"\n",
      "                                           \"Legends'. The grounding answer is \"\n",
      "                                           'precise and matches the context '\n",
      "                                           'which states that the game was '\n",
      "                                           'released in 2009, whereas the '\n",
      "                                           'generated answer vaguely mentions '\n",
      "                                           \"'the early 2000s' which could \"\n",
      "                                           'imply a range of years that does '\n",
      "                                           'not include the correct year of '\n",
      "                                           'release.',\n",
      "                            'label': 'strong reject'},\n",
      "                           {'explanation': 'The generated answer is much worse '\n",
      "                                           'because it provides an inaccurate '\n",
      "                                           \"release period of 'early 2000s' \"\n",
      "                                           'for League of Legends, while the '\n",
      "                                           'grounding answer correctly states '\n",
      "                                           'the game was released in 2009 as '\n",
      "                                           'per the context provided.',\n",
      "                            'label': 'strong reject'}],\n",
      "              'scores': [-1.0, -2.0, -2.0],\n",
      "              'votes': ['reject', 'strong reject', 'strong reject']}],\n",
      "  'root': <uniflow.node.Node object at 0x7f9c864db490>},\n",
      " {'output': [{'average_score': 0.0,\n",
      "              'error': 'No errors.',\n",
      "              'majority_vote': 'equivalent',\n",
      "              'response': [{'explanation': 'Both the grounding answer and the '\n",
      "                                           'generated answer correctly state '\n",
      "                                           'that Vitamin C is water-soluble, '\n",
      "                                           'with minor variations in wording '\n",
      "                                           'that do not affect the accuracy or '\n",
      "                                           'quality of the information '\n",
      "                                           'provided.',\n",
      "                            'label': 'equivalent'},\n",
      "                           {'explanation': 'Both the grounding answer and the '\n",
      "                                           'generated answer correctly '\n",
      "                                           'identify that Vitamin C is '\n",
      "                                           'water-soluble. The grounding '\n",
      "                                           \"answer uses the term 'very \"\n",
      "                                           \"water-soluble' while the generated \"\n",
      "                                           \"answer phrases it as 'can be \"\n",
      "                                           \"dissolved in water well,' both \"\n",
      "                                           'expressing the same factual '\n",
      "                                           'content in slightly different '\n",
      "                                           'words.',\n",
      "                            'label': 'equivalent'},\n",
      "                           {'explanation': 'Both answers correctly affirm that '\n",
      "                                           'Vitamin C is water-soluble. The '\n",
      "                                           \"grounding answer states 'Vitamin C \"\n",
      "                                           \"is a very water-soluble vitamin,' \"\n",
      "                                           'while the generated answer says '\n",
      "                                           \"'Vitamin C can be dissolved in \"\n",
      "                                           \"water well.' Both essentially \"\n",
      "                                           'convey the same information that '\n",
      "                                           'Vitamin C is water-soluble, '\n",
      "                                           'thereby making them equivalent in '\n",
      "                                           'accuracy and quality.',\n",
      "                            'label': 'equivalent'}],\n",
      "              'scores': [0.0, 0.0, 0.0],\n",
      "              'votes': ['equivalent', 'equivalent', 'equivalent']}],\n",
      "  'root': <uniflow.node.Node object at 0x7f9c6dab2fe0>}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output = client.run(data)\n",
    "pprint.pprint(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model's responses can be distilled into majority votes, as shown below. Given the non-deterministic nature of the LLM (where each inference could yield a different output), we've enhanced stability and self-consistency by averaging results from three LLM output samplings, a notable improvement over single-output scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data 0 has majority vote \u001b[31mstrong accept\u001b[0m and average score \u001b[34m2.0\u001b[0m\n",
      "data 1 has majority vote \u001b[31mstrong reject\u001b[0m and average score \u001b[34m-1.6666666666666667\u001b[0m\n",
      "data 2 has majority vote \u001b[31mequivalent\u001b[0m and average score \u001b[34m0.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "for idx, o in enumerate(output):\n",
    "    majority_vote = o['output'][0]['majority_vote']\n",
    "    average_score = o['output'][0]['average_score']\n",
    "    print(f\"data {idx} has majority vote \\033[31m{majority_vote}\\033[0m and average score \\033[34m{average_score}\\033[0m\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uniflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
