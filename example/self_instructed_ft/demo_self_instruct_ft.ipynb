{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cbc4c4a",
   "metadata": {},
   "source": [
    "## Example of generating self-instruct dataset for Paul Graham's essays\n",
    "http://www.paulgraham.com/articles.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb677037",
   "metadata": {},
   "source": [
    "### Prepare the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d84dd70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.getcwd(), os.pardir, os.pardir))\n",
    "from uniflow.flow.flow_self_instructed_gen import SelfInstructedGenFlow\n",
    "import uniflow.flow.constants as constants\n",
    "\n",
    "#html_file = \"do_things_that_dont_scale.html\" #from http://paulgraham.com/ds.html\n",
    "#html_file = \"makers_schedule_managers_schedule.html\" #from http://www.paulgraham.com/makersschedule.html\n",
    "html_file = \"life_is_short.html\" #http://www.paulgraham.com/vb.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd610184",
   "metadata": {},
   "source": [
    "### Run the Self Instructed Gen Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2a934c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "html_in:  22.11_information-theory.html\n",
      "Preprocess HTML Complete! [Document(page_content='Table Of Contents\\n\\nPreface\\n\\nInstallation\\n\\nNotation\\n\\n1. Introduction\\n\\n2. Preliminaries\\n2.1. Data Manipulation\\n2.2. Data Preprocessing\\n2.3. Linear Algebra\\n2.4. Calculus\\n2.5. Automatic Differentiation\\n2.6. Probability and Statistics\\n2.7. Documentation\\n\\n3. Linear Neural Networks for Regression\\n3.1. Linear Regression\\n3.2. Object-Oriented Design for Implementation\\n3.3. Synthetic Regression Data\\n3.4. Linear Regression Implementation from Scratch\\n3.5. Concise Implementation of Linear Regression\\n3.6. Generalization\\n3.7. Weight Decay\\n\\n4. Linear Neural Networks for Classification\\n4.1. Softmax Regression\\n4.2. The Image Classification Dataset\\n4.3. The Base Classification Model\\n4.4. Softmax Regression Implementation from Scratch\\n4.5. Concise Implementation of Softmax Regression\\n4.6. Generalization in Classification\\n4.7. Environment and Distribution Shift\\n\\n5. Multilayer Perceptrons\\n5.1. Multilayer Perceptrons\\n5.2. Implementation of Multilayer Perceptrons\\n5.3. Forward Propagation, Backward Propagation, and Computational Graphs\\n5.4. Numerical Stability and Initialization\\n5.5. Generalization in Deep Learning\\n5.6. Dropout\\n5.7. Predicting House Prices on Kaggle\\n\\n6. Builders’ Guide\\n6.1. Layers and Modules\\n6.2. Parameter Management\\n6.3. Parameter Initialization\\n6.4. Lazy Initialization\\n6.5. Custom Layers\\n6.6. File I/O\\n6.7. GPUs\\n\\n7. Convolutional Neural Networks\\n7.1. From Fully Connected Layers to Convolutions\\n7.2. Convolutions for Images\\n7.3. Padding and Stride\\n7.4. Multiple Input and Multiple Output Channels\\n7.5. Pooling\\n7.6. Convolutional Neural Networks (LeNet)\\n\\n8. Modern Convolutional Neural Networks\\n8.1. Deep Convolutional Neural Networks (AlexNet)\\n8.2. Networks Using Blocks (VGG)\\n8.3. Network in Network (NiN)\\n8.4. Multi-Branch Networks (GoogLeNet)\\n8.5. Batch Normalization\\n8.6. Residual Networks (ResNet) and ResNeXt\\n8.7. Densely Connected Networks (DenseNet)\\n8.8. Designing Convolution Network Architectures\\n\\n9. Recurrent Neural Networks\\n9.1. Working with Sequences\\n9.2. Converting Raw Text into Sequence Data\\n9.3. Language Models\\n9.4. Recurrent Neural Networks\\n9.5. Recurrent Neural Network Implementation from Scratch\\n9.6. Concise Implementation of Recurrent Neural Networks\\n9.7. Backpropagation Through Time\\n\\n10. Modern Recurrent Neural Networks\\n10.1. Long Short-Term Memory (LSTM)\\n10.2. Gated Recurrent Units (GRU)\\n10.3. Deep Recurrent Neural Networks\\n10.4. Bidirectional Recurrent Neural Networks\\n10.5. Machine Translation and the Dataset\\n10.6. The Encoder–Decoder Architecture\\n10.7. Sequence-to-Sequence Learning for Machine Translation\\n10.8. Beam Search\\n\\n11. Attention Mechanisms and Transformers\\n11.1. Queries, Keys, and Values\\n11.2. Attention Pooling by Similarity\\n11.3. Attention Scoring Functions\\n11.4. The Bahdanau Attention Mechanism\\n11.5. Multi-Head Attention\\n11.6. Self-Attention and Positional Encoding\\n11.7. The Transformer Architecture\\n11.8. Transformers for Vision\\n11.9. Large-Scale Pretraining with Transformers\\n\\n12. Optimization Algorithms\\n12.1. Optimization and Deep Learning\\n12.2. Convexity\\n12.3. Gradient Descent\\n12.4. Stochastic Gradient Descent\\n12.5. Minibatch Stochastic Gradient Descent\\n12.6. Momentum\\n12.7. Adagrad\\n12.8. RMSProp\\n12.9. Adadelta\\n12.10. Adam\\n12.11. Learning Rate Scheduling\\n\\n13. Computational Performance\\n13.1. Compilers and Interpreters\\n13.2. Asynchronous Computation\\n13.3. Automatic Parallelism\\n13.4. Hardware\\n13.5. Training on Multiple GPUs\\n13.6. Concise Implementation for Multiple GPUs\\n13.7. Parameter Servers\\n\\n14. Computer Vision\\n14.1. Image Augmentation\\n14.2. Fine-Tuning\\n14.3. Object Detection and Bounding Boxes\\n14.4. Anchor Boxes\\n14.5. Multiscale Object Detection\\n14.6. The Object Detection Dataset\\n14.7. Single Shot Multibox Detection\\n14.8. Region-based CNNs (R-CNNs)\\n14.9. Semantic Segmentation and the Dataset\\n14.10. Transposed Convolution\\n14.11. Fully Convolutional Networks\\n14.12. Neural Style Transfer\\n14.13. Image Classification (CIFAR-10) on Kaggle\\n14.14. Dog Breed Identification (ImageNet Dogs) on Kaggle', metadata={'source': './22.11_information-theory.html'}), Document(page_content='15. Natural Language Processing: Pretraining\\n15.1. Word Embedding (word2vec)\\n15.2. Approximate Training\\n15.3. The Dataset for Pretraining Word Embeddings\\n15.4. Pretraining word2vec\\n15.5. Word Embedding with Global Vectors (GloVe)\\n15.6. Subword Embedding\\n15.7. Word Similarity and Analogy\\n15.8. Bidirectional Encoder Representations from Transformers (BERT)\\n15.9. The Dataset for Pretraining BERT\\n15.10. Pretraining BERT\\n\\n16. Natural Language Processing: Applications\\n16.1. Sentiment Analysis and the Dataset\\n16.2. Sentiment Analysis: Using Recurrent Neural Networks\\n16.3. Sentiment Analysis: Using Convolutional Neural Networks\\n16.4. Natural Language Inference and the Dataset\\n16.5. Natural Language Inference: Using Attention\\n16.6. Fine-Tuning BERT for Sequence-Level and Token-Level Applications\\n16.7. Natural Language Inference: Fine-Tuning BERT\\n\\n17. Reinforcement Learning\\n17.1. Markov Decision Process (MDP)\\n17.2. Value Iteration\\n17.3. Q-Learning\\n\\n18. Gaussian Processes\\n18.1. Introduction to Gaussian Processes\\n18.2. Gaussian Process Priors\\n18.3. Gaussian Process Inference\\n\\n19. Hyperparameter Optimization\\n19.1. What Is Hyperparameter Optimization?\\n19.2. Hyperparameter Optimization API\\n19.3. Asynchronous Random Search\\n19.4. Multi-Fidelity Hyperparameter Optimization\\n19.5. Asynchronous Successive Halving\\n\\n20. Generative Adversarial Networks\\n20.1. Generative Adversarial Networks\\n20.2. Deep Convolutional Generative Adversarial Networks\\n\\n21. Recommender Systems\\n21.1. Overview of Recommender Systems\\n21.2. The MovieLens Dataset\\n21.3. Matrix Factorization\\n21.4. AutoRec: Rating Prediction with Autoencoders\\n21.5. Personalized Ranking for Recommender Systems\\n21.6. Neural Collaborative Filtering for Personalized Ranking\\n21.7. Sequence-Aware Recommender Systems\\n21.8. Feature-Rich Recommender Systems\\n21.9. Factorization Machines\\n21.10. Deep Factorization Machines\\n\\n22. Appendix: Mathematics for Deep Learning\\n22.1. Geometry and Linear Algebraic Operations\\n22.2. Eigendecompositions\\n22.3. Single Variable Calculus\\n22.4. Multivariable Calculus\\n22.5. Integral Calculus\\n22.6. Random Variables\\n22.7. Maximum Likelihood\\n22.8. Distributions\\n22.9. Naive Bayes\\n22.10. Statistics\\n22.11. Information Theory\\n\\n23. Appendix: Tools for Deep Learning\\n23.1. Using Jupyter Notebooks\\n23.2. Using Amazon SageMaker\\n23.3. Using AWS EC2 Instances\\n23.4. Using Google Colab\\n23.5. Selecting Servers and GPUs\\n23.6. Contributing to This Book\\n23.7. Utility Functions and Classes\\n23.8. The d2l API Document\\n\\nReferences\\n\\n22.11. Information Theory¶   Colab [pytorch]  Open the notebook in Colab   Colab [mxnet]  Open the notebook in Colab   Colab [jax]  Open the notebook in Colab   Colab [tensorflow]  Open the notebook in Colab   SageMaker Studio Lab  Open the notebook in SageMaker Studio Lab\\n\\nThe universe is overflowing with information. Information provides a\\ncommon language across disciplinary rifts: from Shakespeare’s Sonnet to\\nresearchers’ paper on Cornell ArXiv, from Van Gogh’s printing Starry\\nNight to Beethoven’s music Symphony No.\\xa05, from the first programming\\nlanguage Plankalkül to the state-of-the-art machine learning algorithms.\\nEverything must follow the rules of information theory, no matter the\\nformat. With information theory, we can measure and compare how much\\ninformation is present in different signals. In this section, we will\\ninvestigate the fundamental concepts of information theory and\\napplications of information theory in machine learning.', metadata={'source': './22.11_information-theory.html'}), Document(page_content='Before we get started, let’s outline the relationship between machine\\nlearning and information theory. Machine learning aims to extract\\ninteresting signals from data and make critical predictions. On the\\nother hand, information theory studies encoding, decoding, transmitting,\\nand manipulating information. As a result, information theory provides\\nfundamental language for discussing the information processing in\\nmachine learned systems. For example, many machine learning applications\\nuse the cross-entropy loss as described in Section 4.1. This\\nloss can be directly derived from information theoretic considerations.\\n\\n22.11.1. Information¶\\n\\nLet’s start with the “soul” of information theory: information.\\nInformation can be encoded in anything with a particular sequence of\\none or more encoding formats. Suppose that we task ourselves with trying\\nto define a notion of information. What could be our starting point?\\n\\nConsider the following thought experiment. We have a friend with a deck\\nof cards. They will shuffle the deck, flip over some cards, and tell us\\nstatements about the cards. We will try to assess the information\\ncontent of each statement.\\n\\nFirst, they flip over a card and tell us, “I see a card.” This provides\\nus with no information at all. We were already certain that this was the\\ncase so we hope the information should be zero.\\n\\nNext, they flip over a card and say, “I see a heart.” This provides us\\nsome information, but in reality there are only \\\\(4\\\\) different\\nsuits that were possible, each equally likely, so we are not surprised\\nby this outcome. We hope that whatever the measure of information, this\\nevent should have low information content.\\n\\nNext, they flip over a card and say, “This is the \\\\(3\\\\) of spades.”\\nThis is more information. Indeed there were \\\\(52\\\\) equally likely\\npossible outcomes, and our friend told us which one it was. This should\\nbe a medium amount of information.\\n\\nLet’s take this to the logical extreme. Suppose that finally they flip\\nover every card from the deck and read off the entire sequence of the\\nshuffled deck. There are \\\\(52!\\\\) different orders to the deck, again\\nall equally likely, so we need a lot of information to know which one it\\nis.\\n\\nAny notion of information we develop must conform to this intuition.\\nIndeed, in the next sections we will learn how to compute that these\\nevents have \\\\(0\\\\textrm{ bits}\\\\), \\\\(2\\\\textrm{ bits}\\\\),\\n\\\\(~5.7\\\\textrm{ bits}\\\\), and \\\\(~225.6\\\\textrm{ bits}\\\\) of\\ninformation respectively.\\n\\nIf we read through these thought experiments, we see a natural idea. As\\na starting point, rather than caring about the knowledge, we may build\\noff the idea that information represents the degree of surprise or the\\nabstract possibility of the event. For example, if we want to describe\\nan unusual event, we need a lot information. For a common event, we may\\nnot need much information.\\n\\nIn 1948, Claude E. Shannon published A Mathematical Theory of\\nCommunication (Shannon, 1948) establishing the theory of\\ninformation. In his article, Shannon introduced the concept of\\ninformation entropy for the first time. We will begin our journey here.\\n\\n22.11.1.1. Self-information¶\\n\\n\\\\(0\\\\) and\\n\\n\\\\(1\\\\). Indeed, binary\\nencoding is still in common use on all modern digital computers. In this\\nway, any information is encoded by a series of\\n\\n\\\\(0\\\\) and\\n\\n\\\\(1\\\\).\\nAnd hence, a series of binary digits of length\\n\\n\\\\(n\\\\) contains\\n\\n\\\\(n\\\\) bits of information.\\n\\n\\\\(0\\\\) or\\n\\n\\\\(1\\\\)\\noccurs with a probability of\\n\\n\\\\(\\\\frac{1}{2}\\\\). Hence, an event\\n\\n\\\\(X\\\\) with a series of codes of length\\n\\n\\\\(n\\\\), occurs with a\\nprobability of\\n\\n\\\\(\\\\frac{1}{2^n}\\\\). At the same time, as we mentioned\\nbefore, this series contains\\n\\n\\\\(n\\\\) bits of information. So, can we\\ngeneralize to a mathematical function which can transfer the probability\\n\\n\\\\(p\\\\) to the number of bits? Shannon gave the answer by defining\\n\\n(22.11.1)¶\\\\[I(X) = - \\\\log_2 (p),\\\\]', metadata={'source': './22.11_information-theory.html'}), Document(page_content='\\\\(p\\\\) to the number of bits? Shannon gave the answer by defining\\n\\n(22.11.1)¶\\\\[I(X) = - \\\\log_2 (p),\\\\]\\n\\nas the bits of information we have received for this event \\\\(X\\\\).\\nNote that we will always use base-2 logarithms in this section. For the\\nsake of simplicity, the rest of this section will omit the subscript 2\\nin the logarithm notation, i.e., \\\\(\\\\log(.)\\\\) always refers to\\n\\\\(\\\\log_2(.)\\\\). For example, the code “0010” has a self-information\\n\\n(22.11.2)¶\\\\[I(\\\\textrm{\"0010\"}) = - \\\\log (p(\\\\textrm{\"0010\"})) = - \\\\log \\\\left( \\\\frac{1}{2^4} \\\\right) = 4 \\\\textrm{ bits}.\\\\]\\n\\nWe can calculate self information as shown below. Before that, let’s\\nfirst import all the necessary packages in this section.\\n\\npytorch\\n\\nmxnet\\n\\ntensorflow\\n\\nimport\\n\\ntorch\\n\\nfrom\\n\\ntorch.nn\\n\\nimport\\n\\nNLLLoss\\n\\ndef\\n\\nnansum\\n\\n):\\n\\n# Define nansum, as pytorch does not offer it inbuilt.\\n\\nreturn\\n\\ntorch\\n\\nisnan\\n\\n)]\\n\\nsum\\n\\n()\\n\\ndef\\n\\nself_information\\n\\n):\\n\\nreturn\\n\\ntorch\\n\\nlog2\\n\\ntorch\\n\\ntensor\\n\\n))\\n\\nitem\\n\\n()\\n\\nself_information\\n\\n64\\n\\n6.0\\n\\nimport\\n\\nrandom\\n\\nfrom\\n\\nmxnet\\n\\nimport\\n\\nnp\\n\\nfrom\\n\\nmxnet.metric\\n\\nimport\\n\\nNegativeLogLikelihood\\n\\nfrom\\n\\nmxnet.ndarray\\n\\nimport\\n\\nnansum\\n\\ndef\\n\\nself_information\\n\\n):\\n\\nreturn\\n\\nnp\\n\\nlog2\\n\\nself_information\\n\\n64\\n\\n6.0\\n\\nimport\\n\\ntensorflow\\n\\nas\\n\\ntf\\n\\ndef\\n\\nlog2\\n\\n):\\n\\nreturn\\n\\ntf\\n\\nmath\\n\\nlog\\n\\ntf\\n\\nmath\\n\\nlog\\n\\n2.\\n\\ndef\\n\\nnansum\\n\\n):\\n\\nreturn\\n\\ntf\\n\\nreduce_sum\\n\\ntf\\n\\nwhere\\n\\ntf\\n\\nmath\\n\\nis_nan\\n\\n),\\n\\ntf\\n\\nzeros_like\\n\\n),\\n\\n),\\n\\naxis\\n\\n=-\\n\\ndef\\n\\nself_information\\n\\n):\\n\\nreturn\\n\\nlog2\\n\\ntf\\n\\nconstant\\n\\n))\\n\\nnumpy\\n\\n()\\n\\nself_information\\n\\n64\\n\\n6.0\\n\\n22.11.2. Entropy¶\\n\\nAs self-information only measures the information of a single discrete\\nevent, we need a more generalized measure for any random variable of\\neither discrete or continuous distribution.\\n\\n22.11.2.1. Motivating Entropy¶\\n\\nLet’s try to get specific about what we want. This will be an informal\\nstatement of what are known as the axioms of Shannon entropy. It will\\nturn out that the following collection of common-sense statements force\\nus to a unique definition of information. A formal version of these\\naxioms, along with several others may be found in\\nCsiszár (2008).\\n\\nThe information we gain by observing a random variable does not\\ndepend on what we call the elements, or the presence of additional\\nelements which have probability zero.\\n\\nThe information we gain by observing two random variables is no more\\nthan the sum of the information we gain by observing them separately.\\nIf they are independent, then it is exactly the sum.\\n\\nThe information gained when observing (nearly) certain events is\\n(nearly) zero.\\n\\nWhile proving this fact is beyond the scope of our text, it is important\\nto know that this uniquely determines the form that entropy must take.\\nThe only ambiguity that these allow is in the choice of fundamental\\nunits, which is most often normalized by making the choice we saw before\\nthat the information provided by a single fair coin flip is one bit.\\n\\n22.11.2.2. Definition¶\\n\\nFor any random variable \\\\(X\\\\) that follows a probability\\ndistribution \\\\(P\\\\) with a probability density function (p.d.f.) or a\\nprobability mass function (p.m.f.) \\\\(p(x)\\\\), we measure the expected\\namount of information through entropy (or Shannon entropy)\\n\\n(22.11.3)¶\\\\[H(X) = - E_{x \\\\sim P} [\\\\log p(x)].\\\\]\\n\\nTo be specific, if \\\\(X\\\\) is discrete,\\n\\n(22.11.4)¶\\\\[H(X) = - \\\\sum_i p_i \\\\log p_i \\\\textrm{, where } p_i = P(X_i).\\\\]\\n\\nOtherwise, if \\\\(X\\\\) is continuous, we also refer entropy as\\ndifferential entropy\\n\\n(22.11.5)¶\\\\[H(X) = - \\\\int_x p(x) \\\\log p(x) \\\\; dx.\\\\]\\n\\nWe can define entropy as below.\\n\\npytorch\\n\\nmxnet\\n\\ntensorflow\\n\\ndef\\n\\nentropy\\n\\n):\\n\\nentropy\\n\\ntorch\\n\\nlog2\\n\\n# Operator `nansum` will sum up the non-nan number\\n\\nout\\n\\nnansum\\n\\nentropy\\n\\nreturn\\n\\nout\\n\\nentropy\\n\\ntorch\\n\\ntensor\\n\\n([\\n\\n0.1\\n\\n0.5\\n\\n0.1\\n\\n0.3\\n\\n]))\\n\\ntensor\\n\\n1.6855\\n\\ndef\\n\\nentropy\\n\\n):\\n\\nentropy\\n\\nnp\\n\\nlog2\\n\\n# Operator `nansum` will sum up the non-nan number\\n\\nout\\n\\nnansum\\n\\nentropy\\n\\nas_nd_ndarray\\n\\n())\\n\\nreturn\\n\\nout\\n\\nentropy\\n\\nnp\\n\\narray\\n\\n([\\n\\n0.1\\n\\n0.5\\n\\n0.1\\n\\n0.3\\n\\n]))\\n\\n21\\n\\n59\\n\\n56\\n\\n../\\n\\nsrc\\n\\nstorage\\n\\nstorage\\n\\ncc\\n\\n196\\n\\nUsing\\n\\nPooled\\n\\nNaive\\n\\nStorageManager\\n\\nfor\\n\\nCPU\\n\\n1.6854753\\n\\nNDArray', metadata={'source': './22.11_information-theory.html'}), Document(page_content='as_nd_ndarray\\n\\n())\\n\\nreturn\\n\\nout\\n\\nentropy\\n\\nnp\\n\\narray\\n\\n([\\n\\n0.1\\n\\n0.5\\n\\n0.1\\n\\n0.3\\n\\n]))\\n\\n21\\n\\n59\\n\\n56\\n\\n../\\n\\nsrc\\n\\nstorage\\n\\nstorage\\n\\ncc\\n\\n196\\n\\nUsing\\n\\nPooled\\n\\nNaive\\n\\nStorageManager\\n\\nfor\\n\\nCPU\\n\\n1.6854753\\n\\nNDArray\\n\\n@cpu\\n\\ndef\\n\\nentropy\\n\\n):\\n\\nreturn\\n\\nnansum\\n\\nlog2\\n\\n))\\n\\nentropy\\n\\ntf\\n\\nconstant\\n\\n([\\n\\n0.1\\n\\n0.5\\n\\n0.1\\n\\n0.3\\n\\n]))\\n\\ntf\\n\\nTensor\\n\\nshape\\n\\n(),\\n\\ndtype\\n\\nfloat32\\n\\nnumpy\\n\\n1.6854753\\n\\n22.11.2.3. Interpretations¶\\n\\nYou may be curious: in the entropy definition (22.11.3), why\\ndo we use an expectation of a negative logarithm? Here are some\\nintuitions.\\n\\n\\\\(\\\\log\\\\)? Suppose that\\n\\n\\\\(p(x) = f_1(x) f_2(x) \\\\ldots, f_n(x)\\\\), where each component\\nfunction\\n\\n\\\\(f_i(x)\\\\) is independent from each other. This means that\\neach\\n\\n\\\\(f_i(x)\\\\) contributes independently to the total information\\nobtained from\\n\\n\\\\(p(x)\\\\). As discussed above, we want the entropy\\nformula to be additive over independent random variables. Luckily,\\n\\n\\\\(\\\\log\\\\) can naturally turn a product of probability distributions\\nto a summation of the individual terms.\\n\\nNext, why do we use a negative \\\\(\\\\log\\\\)? Intuitively, more\\nfrequent events should contain less information than less common events,\\nsince we often gain more information from an unusual case than from an\\nordinary one. However, \\\\(\\\\log\\\\) is monotonically increasing with the\\nprobabilities, and indeed negative for all values in \\\\([0, 1]\\\\). We\\nneed to construct a monotonically decreasing relationship between the\\nprobability of events and their entropy, which will ideally be always\\npositive (for nothing we observe should force us to forget what we have\\nknown). Hence, we add a negative sign in front of \\\\(\\\\log\\\\) function.\\n\\n\\\\(X\\\\). We can interpret the self-information\\n(\\n\\n\\\\(-\\\\log(p)\\\\)) as the amount of\\n\\n\\\\(X\\\\). For example,\\nimagine that a slot machine system emits statistical independently\\nsymbols\\n\\n\\\\({s_1, \\\\ldots, s_k}\\\\) with probabilities\\n\\n\\\\({p_1, \\\\ldots, p_k}\\\\) respectively. Then the entropy of this system\\nequals to the average self-information from observing each output, i.e.,\\n\\n(22.11.6)¶\\\\[H(S) = \\\\sum_i {p_i \\\\cdot I(s_i)} = - \\\\sum_i {p_i \\\\cdot \\\\log p_i}.\\\\]\\n\\n22.11.2.4. Properties of Entropy¶\\n\\nBy the above examples and interpretations, we can derive the following\\nproperties of entropy (22.11.3). Here, we refer to \\\\(X\\\\)\\nas an event and \\\\(P\\\\) as the probability distribution of \\\\(X\\\\).\\n\\n\\\\(H(X) \\\\geq 0\\\\) for all discrete \\\\(X\\\\) (entropy can be\\nnegative for continuous \\\\(X\\\\)).\\n\\nIf \\\\(X \\\\sim P\\\\) with a p.d.f. or a p.m.f. \\\\(p(x)\\\\), and we\\ntry to estimate \\\\(P\\\\) by a new probability distribution \\\\(Q\\\\)\\nwith a p.d.f. or a p.m.f. \\\\(q(x)\\\\), then\\n\\n(22.11.7)¶\\\\[H(X) = - E_{x \\\\sim P} [\\\\log p(x)] \\\\leq  - E_{x \\\\sim P} [\\\\log q(x)], \\\\textrm{ with equality if and only if } P = Q.\\\\]\\nAlternatively, \\\\(H(X)\\\\) gives a lower bound of the average\\nnumber of bits needed to encode symbols drawn from \\\\(P\\\\).\\n\\nIf \\\\(X \\\\sim P\\\\), then \\\\(x\\\\) conveys the maximum amount of\\ninformation if it spreads evenly among all possible outcomes.\\nSpecifically, if the probability distribution \\\\(P\\\\) is discrete\\nwith \\\\(k\\\\)-class \\\\(\\\\{p_1, \\\\ldots, p_k \\\\}\\\\), then\\n\\n(22.11.8)¶\\\\[H(X) \\\\leq \\\\log(k), \\\\textrm{ with equality if and only if } p_i = \\\\frac{1}{k}, \\\\forall i.\\\\]\\nIf \\\\(P\\\\) is a continuous random variable, then the story\\nbecomes much more complicated. However, if we additionally impose\\nthat \\\\(P\\\\) is supported on a finite interval (with all values\\nbetween \\\\(0\\\\) and \\\\(1\\\\)), then \\\\(P\\\\) has the highest\\nentropy if it is the uniform distribution on that interval.\\n\\n22.11.3. Mutual Information¶\\n\\nPreviously we defined entropy of a single random variable \\\\(X\\\\), how\\nabout the entropy of a pair random variables \\\\((X, Y)\\\\)? We can\\nthink of these techniques as trying to answer the following type of\\nquestion, “What information is contained in \\\\(X\\\\) and \\\\(Y\\\\)\\ntogether compared to each separately? Is there redundant information, or\\nis it all unique?”\\n\\n\\\\((X, Y)\\\\) as a pair of\\nrandom variables that follows a joint probability distribution\\n\\n\\\\(P\\\\)\\nwith a p.d.f. or a p.m.f.\\n\\n\\\\(p_{X, Y}(x, y)\\\\), while\\n\\n\\\\(X\\\\) and\\n\\n\\\\(Y\\\\) follow probability distribution', metadata={'source': './22.11_information-theory.html'}), Document(page_content='\\\\((X, Y)\\\\) as a pair of\\nrandom variables that follows a joint probability distribution\\n\\n\\\\(P\\\\)\\nwith a p.d.f. or a p.m.f.\\n\\n\\\\(p_{X, Y}(x, y)\\\\), while\\n\\n\\\\(X\\\\) and\\n\\n\\\\(Y\\\\) follow probability distribution\\n\\n\\\\(p_X(x)\\\\) and\\n\\n\\\\(p_Y(y)\\\\), respectively.\\n\\n22.11.3.1. Joint Entropy¶\\n\\nSimilar to entropy of a single random variable (22.11.3), we\\ndefine the joint entropy \\\\(H(X, Y)\\\\) of a pair random variables\\n\\\\((X, Y)\\\\) as\\n\\n(22.11.9)¶\\\\[H(X, Y) = -E_{(x, y) \\\\sim P} [\\\\log p_{X, Y}(x, y)].\\\\]\\n\\nPrecisely, on the one hand, if \\\\((X, Y)\\\\) is a pair of discrete\\nrandom variables, then\\n\\n(22.11.10)¶\\\\[H(X, Y) = - \\\\sum_{x} \\\\sum_{y} p_{X, Y}(x, y) \\\\log p_{X, Y}(x, y).\\\\]\\n\\nOn the other hand, if \\\\((X, Y)\\\\) is a pair of continuous random\\nvariables, then we define the differential joint entropy as\\n\\n(22.11.11)¶\\\\[H(X, Y) = - \\\\int_{x, y} p_{X, Y}(x, y) \\\\ \\\\log p_{X, Y}(x, y) \\\\;dx \\\\;dy.\\\\]\\n\\n(22.11.9) as telling us the total\\nrandomness in the pair of random variables. As a pair of extremes, if\\n\\n\\\\(X = Y\\\\) are two identical random variables, then the information\\nin the pair is exactly the information in one and we have\\n\\n\\\\(H(X, Y) = H(X) = H(Y)\\\\). On the other extreme, if\\n\\n\\\\(X\\\\) and\\n\\n\\\\(Y\\\\) are independent then\\n\\n\\\\(H(X, Y) = H(X) + H(Y)\\\\). Indeed we\\nwill always have that the information contained in a pair of random\\nvariables is no smaller than the entropy of either random variable and\\nno more than the sum of both.\\n\\n(22.11.12)¶\\\\[H(X), H(Y) \\\\le H(X, Y) \\\\le H(X) + H(Y).\\\\]\\n\\nLet’s implement joint entropy from scratch.\\n\\npytorch\\n\\nmxnet\\n\\ntensorflow\\n\\ndef\\n\\njoint_entropy\\n\\np_xy\\n\\n):\\n\\njoint_ent\\n\\np_xy\\n\\ntorch\\n\\nlog2\\n\\np_xy\\n\\n# Operator `nansum` will sum up the non-nan number\\n\\nout\\n\\nnansum\\n\\njoint_ent\\n\\nreturn\\n\\nout\\n\\njoint_entropy\\n\\ntorch\\n\\ntensor\\n\\n([[\\n\\n0.1\\n\\n0.5\\n\\n],\\n\\n0.1\\n\\n0.3\\n\\n]]))\\n\\ntensor\\n\\n1.6855\\n\\ndef\\n\\njoint_entropy\\n\\np_xy\\n\\n):\\n\\njoint_ent\\n\\np_xy\\n\\nnp\\n\\nlog2\\n\\np_xy\\n\\n# Operator `nansum` will sum up the non-nan number\\n\\nout\\n\\nnansum\\n\\njoint_ent\\n\\nas_nd_ndarray\\n\\n())\\n\\nreturn\\n\\nout\\n\\njoint_entropy\\n\\nnp\\n\\narray\\n\\n([[\\n\\n0.1\\n\\n0.5\\n\\n],\\n\\n0.1\\n\\n0.3\\n\\n]]))\\n\\n1.6854753\\n\\nNDArray\\n\\n@cpu\\n\\ndef\\n\\njoint_entropy\\n\\np_xy\\n\\n):\\n\\njoint_ent\\n\\np_xy\\n\\nlog2\\n\\np_xy\\n\\n# Operator `nansum` will sum up the non-nan number\\n\\nout\\n\\nnansum\\n\\njoint_ent\\n\\nreturn\\n\\nout\\n\\njoint_entropy\\n\\ntf\\n\\nconstant\\n\\n([[\\n\\n0.1\\n\\n0.5\\n\\n],\\n\\n0.1\\n\\n0.3\\n\\n]]))\\n\\ntf\\n\\nTensor\\n\\nshape\\n\\n,),\\n\\ndtype\\n\\nfloat32\\n\\nnumpy\\n\\narray\\n\\n([\\n\\n0.8321928\\n\\n0.8532826\\n\\n],\\n\\ndtype\\n\\nfloat32\\n\\nNotice that this is the same code as before, but now we interpret it\\ndifferently as working on the joint distribution of the two random\\nvariables.\\n\\n22.11.3.2. Conditional Entropy¶\\n\\nThe joint entropy defined above the amount of information contained in a\\npair of random variables. This is useful, but oftentimes it is not what\\nwe care about. Consider the setting of machine learning. Let’s take\\n\\\\(X\\\\) to be the random variable (or vector of random variables) that\\ndescribes the pixel values of an image, and \\\\(Y\\\\) to be the random\\nvariable which is the class label. \\\\(X\\\\) should contain substantial\\ninformation—a natural image is a complex thing. However, the information\\ncontained in \\\\(Y\\\\) once the image has been show should be low.\\nIndeed, the image of a digit should already contain the information\\nabout what digit it is unless the digit is illegible. Thus, to continue\\nto extend our vocabulary of information theory, we need to be able to\\nreason about the information content in a random variable conditional on\\nanother.\\n\\nIn the probability theory, we saw the definition of the conditional\\nprobability to measure the relationship between variables. We now want\\nto analogously define the conditional entropy \\\\(H(Y \\\\mid X)\\\\). We\\ncan write this as\\n\\n(22.11.13)¶\\\\[H(Y \\\\mid X) = - E_{(x, y) \\\\sim P} [\\\\log p(y \\\\mid x)],\\\\]\\n\\nwhere \\\\(p(y \\\\mid x) = \\\\frac{p_{X, Y}(x, y)}{p_X(x)}\\\\) is the\\nconditional probability. Specifically, if \\\\((X, Y)\\\\) is a pair of\\ndiscrete random variables, then\\n\\n(22.11.14)¶\\\\[H(Y \\\\mid X) = - \\\\sum_{x} \\\\sum_{y} p(x, y) \\\\log p(y \\\\mid x).\\\\]', metadata={'source': './22.11_information-theory.html'}), Document(page_content='(22.11.14)¶\\\\[H(Y \\\\mid X) = - \\\\sum_{x} \\\\sum_{y} p(x, y) \\\\log p(y \\\\mid x).\\\\]\\n\\nIf \\\\((X, Y)\\\\) is a pair of continuous random variables, then the\\ndifferential conditional entropy is similarly defined as\\n\\n(22.11.15)¶\\\\[H(Y \\\\mid X) = - \\\\int_x \\\\int_y p(x, y) \\\\ \\\\log p(y \\\\mid x) \\\\;dx \\\\;dy.\\\\]\\n\\nIt is now natural to ask, how does the conditional entropy\\n\\\\(H(Y \\\\mid X)\\\\) relate to the entropy \\\\(H(X)\\\\) and the joint\\nentropy \\\\(H(X, Y)\\\\)? Using the definitions above, we can express\\nthis cleanly:\\n\\n(22.11.16)¶\\\\[H(Y \\\\mid X) = H(X, Y) - H(X).\\\\]\\n\\n\\\\(Y\\\\) given\\n\\n\\\\(X\\\\) (\\n\\n\\\\(H(Y \\\\mid X)\\\\)) is the same as the information in both\\n\\n\\\\(X\\\\) and\\n\\n\\\\(Y\\\\) together (\\n\\n\\\\(H(X, Y)\\\\)) minus the information\\nalready contained in\\n\\n\\\\(X\\\\). This gives us the information in\\n\\n\\\\(Y\\\\) which is not also represented in\\n\\n\\\\(X\\\\).\\n\\nNow, let’s implement conditional entropy (22.11.13) from\\nscratch.\\n\\npytorch\\n\\nmxnet\\n\\ntensorflow\\n\\ndef\\n\\nconditional_entropy\\n\\np_xy\\n\\np_x\\n\\n):\\n\\np_y_given_x\\n\\np_xy\\n\\np_x\\n\\ncond_ent\\n\\np_xy\\n\\ntorch\\n\\nlog2\\n\\np_y_given_x\\n\\n# Operator `nansum` will sum up the non-nan number\\n\\nout\\n\\nnansum\\n\\ncond_ent\\n\\nreturn\\n\\nout\\n\\nconditional_entropy\\n\\ntorch\\n\\ntensor\\n\\n([[\\n\\n0.1\\n\\n0.5\\n\\n],\\n\\n0.2\\n\\n0.3\\n\\n]]),\\n\\ntorch\\n\\ntensor\\n\\n([\\n\\n0.2\\n\\n0.8\\n\\n]))\\n\\ntensor\\n\\n0.8635\\n\\ndef\\n\\nconditional_entropy\\n\\np_xy\\n\\np_x\\n\\n):\\n\\np_y_given_x\\n\\np_xy\\n\\np_x\\n\\ncond_ent\\n\\np_xy\\n\\nnp\\n\\nlog2\\n\\np_y_given_x\\n\\n# Operator `nansum` will sum up the non-nan number\\n\\nout\\n\\nnansum\\n\\ncond_ent\\n\\nas_nd_ndarray\\n\\n())\\n\\nreturn\\n\\nout\\n\\nconditional_entropy\\n\\nnp\\n\\narray\\n\\n([[\\n\\n0.1\\n\\n0.5\\n\\n],\\n\\n0.2\\n\\n0.3\\n\\n]]),\\n\\nnp\\n\\narray\\n\\n([\\n\\n0.2\\n\\n0.8\\n\\n]))\\n\\n0.8635472\\n\\nNDArray\\n\\n@cpu\\n\\ndef\\n\\nconditional_entropy\\n\\np_xy\\n\\np_x\\n\\n):\\n\\np_y_given_x\\n\\np_xy\\n\\np_x\\n\\ncond_ent\\n\\np_xy\\n\\nlog2\\n\\np_y_given_x\\n\\n# Operator `nansum` will sum up the non-nan number\\n\\nout\\n\\nnansum\\n\\ncond_ent\\n\\nreturn\\n\\nout\\n\\nconditional_entropy\\n\\ntf\\n\\nconstant\\n\\n([[\\n\\n0.1\\n\\n0.5\\n\\n],\\n\\n0.2\\n\\n0.3\\n\\n]]),\\n\\ntf\\n\\nconstant\\n\\n([\\n\\n0.2\\n\\n0.8\\n\\n]))\\n\\ntf\\n\\nTensor\\n\\nshape\\n\\n,),\\n\\ndtype\\n\\nfloat32\\n\\nnumpy\\n\\narray\\n\\n([\\n\\n0.43903595\\n\\n0.42451128\\n\\n],\\n\\ndtype\\n\\nfloat32\\n\\n22.11.3.3. Mutual Information¶\\n\\n\\\\((X, Y)\\\\), you may\\nwonder: “Now that we know how much information is contained in\\n\\n\\\\(Y\\\\)\\nbut not in\\n\\n\\\\(X\\\\), can we similarly ask how much information is\\nshared between\\n\\n\\\\(X\\\\) and\\n\\n\\\\(Y\\\\)?” The answer will be the\\n\\n\\\\((X, Y)\\\\), which we will write as\\n\\n\\\\(I(X, Y)\\\\).\\n\\n\\\\(X\\\\) and\\n\\n\\\\(Y\\\\) together, and then we take off the parts\\nthat are not shared. The information contained in both\\n\\n\\\\(X\\\\) and\\n\\n\\\\(Y\\\\) together is written as\\n\\n\\\\(H(X, Y)\\\\). We want to subtract\\nfrom this the information contained in\\n\\n\\\\(X\\\\) but not in\\n\\n\\\\(Y\\\\),\\nand the information contained in\\n\\n\\\\(Y\\\\) but not in\\n\\n\\\\(X\\\\). As we\\nsaw in the previous section, this is given by\\n\\n\\\\(H(X \\\\mid Y)\\\\) and\\n\\n\\\\(H(Y \\\\mid X)\\\\) respectively. Thus, we have that the mutual\\ninformation should be\\n\\n(22.11.17)¶\\\\[I(X, Y) = H(X, Y) - H(Y \\\\mid X) - H(X \\\\mid Y).\\\\]\\n\\nIndeed, this is a valid definition for the mutual information. If we\\nexpand out the definitions of these terms and combine them, a little\\nalgebra shows that this is the same as\\n\\n(22.11.18)¶\\\\[I(X, Y) = E_{x} E_{y} \\\\left\\\\{ p_{X, Y}(x, y) \\\\log\\\\frac{p_{X, Y}(x, y)}{p_X(x) p_Y(y)} \\\\right\\\\}.\\\\]\\n\\nWe can summarize all of these relationships in image\\nFig. 22.11.1. It is an excellent test of intuition\\nto see why the following statements are all also equivalent to\\n\\\\(I(X, Y)\\\\).\\n\\n\\\\(H(X) - H(X \\\\mid Y)\\\\)\\n\\n\\\\(H(Y) - H(Y \\\\mid X)\\\\)\\n\\n\\\\(H(X) + H(Y) - H(X, Y)\\\\)\\n\\nFig. 22.11.1 Mutual information’s relationship with joint entropy and conditional\\nentropy.¶\\n\\nIn many ways we can think of the mutual information\\n(22.11.18) as principled extension of correlation\\ncoefficient we saw in Section 22.6. This allows us\\nto ask not only for linear relationships between variables, but for the\\nmaximum information shared between the two random variables of any kind.\\n\\nNow, let’s implement mutual information from scratch.\\n\\npytorch\\n\\nmxnet\\n\\ntensorflow\\n\\ndef\\n\\nmutual_information\\n\\np_xy\\n\\np_x\\n\\np_y\\n\\n):\\n\\np_xy\\n\\np_x\\n\\np_y\\n\\nmutual\\n\\np_xy\\n\\ntorch\\n\\nlog2\\n\\n# Operator `nansum` will sum up the non-nan number\\n\\nout\\n\\nnansum\\n\\nmutual\\n\\nreturn\\n\\nout\\n\\nmutual_information\\n\\ntorch\\n\\ntensor\\n\\n([[\\n\\n0.1\\n\\n0.5\\n\\n],', metadata={'source': './22.11_information-theory.html'}), Document(page_content='p_x\\n\\np_y\\n\\n):\\n\\np_xy\\n\\np_x\\n\\np_y\\n\\nmutual\\n\\np_xy\\n\\ntorch\\n\\nlog2\\n\\n# Operator `nansum` will sum up the non-nan number\\n\\nout\\n\\nnansum\\n\\nmutual\\n\\nreturn\\n\\nout\\n\\nmutual_information\\n\\ntorch\\n\\ntensor\\n\\n([[\\n\\n0.1\\n\\n0.5\\n\\n],\\n\\n0.1\\n\\n0.3\\n\\n]]),\\n\\ntorch\\n\\ntensor\\n\\n([\\n\\n0.2\\n\\n0.8\\n\\n]),\\n\\ntorch\\n\\ntensor\\n\\n([[\\n\\n0.75\\n\\n0.25\\n\\n]]))\\n\\ntensor\\n\\n0.7195\\n\\ndef\\n\\nmutual_information\\n\\np_xy\\n\\np_x\\n\\np_y\\n\\n):\\n\\np_xy\\n\\np_x\\n\\np_y\\n\\nmutual\\n\\np_xy\\n\\nnp\\n\\nlog2\\n\\n# Operator `nansum` will sum up the non-nan number\\n\\nout\\n\\nnansum\\n\\nmutual\\n\\nas_nd_ndarray\\n\\n())\\n\\nreturn\\n\\nout\\n\\nmutual_information\\n\\nnp\\n\\narray\\n\\n([[\\n\\n0.1\\n\\n0.5\\n\\n],\\n\\n0.1\\n\\n0.3\\n\\n]]),\\n\\nnp\\n\\narray\\n\\n([\\n\\n0.2\\n\\n0.8\\n\\n]),\\n\\nnp\\n\\narray\\n\\n([[\\n\\n0.75\\n\\n0.25\\n\\n]]))\\n\\n0.7194603\\n\\nNDArray\\n\\n@cpu\\n\\ndef\\n\\nmutual_information\\n\\np_xy\\n\\np_x\\n\\np_y\\n\\n):\\n\\np_xy\\n\\np_x\\n\\np_y\\n\\nmutual\\n\\np_xy\\n\\nlog2\\n\\n# Operator `nansum` will sum up the non-nan number\\n\\nout\\n\\nnansum\\n\\nmutual\\n\\nreturn\\n\\nout\\n\\nmutual_information\\n\\ntf\\n\\nconstant\\n\\n([[\\n\\n0.1\\n\\n0.5\\n\\n],\\n\\n0.1\\n\\n0.3\\n\\n]]),\\n\\ntf\\n\\nconstant\\n\\n([\\n\\n0.2\\n\\n0.8\\n\\n]),\\n\\ntf\\n\\nconstant\\n\\n([[\\n\\n0.75\\n\\n0.25\\n\\n]]))\\n\\ntf\\n\\nTensor\\n\\nshape\\n\\n,),\\n\\ndtype\\n\\nfloat32\\n\\nnumpy\\n\\narray\\n\\n([\\n\\n0.60246783\\n\\n0.1169925\\n\\n],\\n\\ndtype\\n\\nfloat32\\n\\n22.11.3.4. Properties of Mutual Information¶\\n\\nRather than memorizing the definition of mutual information\\n(22.11.18), you only need to keep in mind its notable\\nproperties:\\n\\nMutual information is symmetric, i.e., \\\\(I(X, Y) = I(Y, X)\\\\).\\n\\nMutual information is non-negative, i.e., \\\\(I(X, Y) \\\\geq 0\\\\).\\n\\n\\\\(I(X, Y) = 0\\\\) if and only if \\\\(X\\\\) and \\\\(Y\\\\) are\\nindependent. For example, if \\\\(X\\\\) and \\\\(Y\\\\) are independent,\\nthen knowing \\\\(Y\\\\) does not give any information about \\\\(X\\\\)\\nand vice versa, so their mutual information is zero.\\n\\nAlternatively, if \\\\(X\\\\) is an invertible function of \\\\(Y\\\\),\\nthen \\\\(Y\\\\) and \\\\(X\\\\) share all information and\\n\\n(22.11.19)¶\\\\[I(X, Y) = H(Y) = H(X).\\\\]\\n\\n22.11.3.5. Pointwise Mutual Information¶\\n\\nWhen we worked with entropy at the beginning of this chapter, we were\\nable to provide an interpretation of \\\\(-\\\\log(p_X(x))\\\\) as how\\nsurprised we were with the particular outcome. We may give a similar\\ninterpretation to the logarithmic term in the mutual information, which\\nis often referred to as the pointwise mutual information:\\n\\n(22.11.20)¶\\\\[\\\\textrm{pmi}(x, y) = \\\\log\\\\frac{p_{X, Y}(x, y)}{p_X(x) p_Y(y)}.\\\\]\\n\\nWe can think of (22.11.20) as measuring how much more or less\\nlikely the specific combination of outcomes \\\\(x\\\\) and \\\\(y\\\\) are\\ncompared to what we would expect for independent random outcomes. If it\\nis large and positive, then these two specific outcomes occur much more\\nfrequently than they would compared to random chance (note: the\\ndenominator is \\\\(p_X(x) p_Y(y)\\\\) which is the probability of the two\\noutcomes were independent), whereas if it is large and negative it\\nrepresents the two outcomes happening far less than we would expect by\\nrandom chance.\\n\\nThis allows us to interpret the mutual information\\n(22.11.18) as the average amount that we were surprised\\nto see two outcomes occurring together compared to what we would expect\\nif they were independent.\\n\\n22.11.3.6. Applications of Mutual Information¶\\n\\nMutual information may be a little abstract in it pure definition, so\\nhow does it related to machine learning? In natural language processing,\\none of the most difficult problems is the ambiguity resolution, or the\\nissue of the meaning of a word being unclear from context. For example,\\nrecently a headline in the news reported that “Amazon is on fire”. You\\nmay wonder whether the company Amazon has a building on fire, or the\\nAmazon rain forest is on fire.', metadata={'source': './22.11_information-theory.html'}), Document(page_content='In this case, mutual information can help us resolve this ambiguity. We\\nfirst find the group of words that each has a relatively large mutual\\ninformation with the company Amazon, such as e-commerce, technology, and\\nonline. Second, we find another group of words that each has a\\nrelatively large mutual information with the Amazon rain forest, such as\\nrain, forest, and tropical. When we need to disambiguate “Amazon”, we\\ncan compare which group has more occurrence in the context of the word\\nAmazon. In this case the article would go on to describe the forest, and\\nmake the context clear.\\n\\n22.11.4. Kullback–Leibler Divergence¶\\n\\nAs what we have discussed in Section 2.3, we can use\\nnorms to measure distance between two points in space of any\\ndimensionality. We would like to be able to do a similar task with\\nprobability distributions. There are many ways to go about this, but\\ninformation theory provides one of the nicest. We now explore the\\nKullback–Leibler (KL) divergence, which provides a way to measure if\\ntwo distributions are close together or not.\\n\\n22.11.4.1. Definition¶\\n\\n\\\\(X\\\\) that follows the probability\\ndistribution\\n\\n\\\\(P\\\\) with a p.d.f. or a p.m.f.\\n\\n\\\\(p(x)\\\\), and we\\nestimate\\n\\n\\\\(P\\\\) by another probability distribution\\n\\n\\\\(Q\\\\) with a\\np.d.f. or a p.m.f.\\n\\n\\\\(q(x)\\\\). Then the\\n\\n\\\\(P\\\\) and\\n\\n\\\\(Q\\\\) is\\n\\n(22.11.21)¶\\\\[D_{\\\\textrm{KL}}(P\\\\|Q) = E_{x \\\\sim P} \\\\left[ \\\\log \\\\frac{p(x)}{q(x)} \\\\right].\\\\]\\n\\n(22.11.20), we can\\nagain provide an interpretation of the logarithmic term:\\n\\n\\\\(-\\\\log \\\\frac{q(x)}{p(x)} = -\\\\log(q(x)) - (-\\\\log(p(x)))\\\\) will be\\nlarge and positive if we see\\n\\n\\\\(x\\\\) far more often under\\n\\n\\\\(P\\\\)\\nthan we would expect for\\n\\n\\\\(Q\\\\), and large and negative if we see the\\noutcome far less than expected. In this way, we can interpret it as our\\n\\nLet’s implement the KL divergence from Scratch.\\n\\npytorch\\n\\nmxnet\\n\\ntensorflow\\n\\ndef\\n\\nkl_divergence\\n\\n):\\n\\nkl\\n\\ntorch\\n\\nlog2\\n\\nout\\n\\nnansum\\n\\nkl\\n\\nreturn\\n\\nout\\n\\nabs\\n\\n()\\n\\nitem\\n\\n()\\n\\ndef\\n\\nkl_divergence\\n\\n):\\n\\nkl\\n\\nnp\\n\\nlog2\\n\\nout\\n\\nnansum\\n\\nkl\\n\\nas_nd_ndarray\\n\\n())\\n\\nreturn\\n\\nout\\n\\nabs\\n\\n()\\n\\nasscalar\\n\\n()\\n\\ndef\\n\\nkl_divergence\\n\\n):\\n\\nkl\\n\\nlog2\\n\\nout\\n\\nnansum\\n\\nkl\\n\\nreturn\\n\\ntf\\n\\nabs\\n\\nout\\n\\nnumpy\\n\\n()\\n\\n22.11.4.2. KL Divergence Properties¶\\n\\nLet’s take a look at some properties of the KL divergence\\n(22.11.21).\\n\\nKL divergence is non-symmetric, i.e., there are \\\\(P,Q\\\\) such that\\n\\n(22.11.22)¶\\\\[D_{\\\\textrm{KL}}(P\\\\|Q) \\\\neq D_{\\\\textrm{KL}}(Q\\\\|P).\\\\]\\n\\nKL divergence is non-negative, i.e.,\\n\\n(22.11.23)¶\\\\[D_{\\\\textrm{KL}}(P\\\\|Q) \\\\geq 0.\\\\]\\nNote that the equality holds only when \\\\(P = Q\\\\).\\n\\nIf there exists an \\\\(x\\\\) such that \\\\(p(x) > 0\\\\) and\\n\\\\(q(x) = 0\\\\), then \\\\(D_{\\\\textrm{KL}}(P\\\\|Q) = \\\\infty\\\\).\\n\\nThere is a close relationship between KL divergence and mutual\\ninformation. Besides the relationship shown in\\nFig. 22.11.1, \\\\(I(X, Y)\\\\) is also\\nnumerically equivalent with the following terms:\\n\\n\\\\(D_{\\\\textrm{KL}}(P(X, Y) \\\\ \\\\| \\\\ P(X)P(Y))\\\\);\\n\\\\(E_Y \\\\{ D_{\\\\textrm{KL}}(P(X \\\\mid Y) \\\\ \\\\| \\\\ P(X)) \\\\}\\\\);\\n\\\\(E_X \\\\{ D_{\\\\textrm{KL}}(P(Y \\\\mid X) \\\\ \\\\| \\\\ P(Y)) \\\\}\\\\).\\n\\nFor the first term, we interpret mutual information as the KL\\ndivergence between \\\\(P(X, Y)\\\\) and the product of \\\\(P(X)\\\\)\\nand \\\\(P(Y)\\\\), and thus is a measure of how different the joint\\ndistribution is from the distribution if they were independent. For\\nthe second term, mutual information tells us the average reduction in\\nuncertainty about \\\\(Y\\\\) that results from learning the value of\\nthe \\\\(X\\\\)’s distribution. Similarly to the third term.\\n\\n22.11.4.3. Example¶\\n\\nLet’s go through a toy example to see the non-symmetry explicitly.\\n\\n\\\\(10,000\\\\):\\nan objective tensor\\n\\n\\\\(p\\\\) which follows a normal distribution\\n\\n\\\\(N(0, 1)\\\\), and two candidate tensors\\n\\n\\\\(q_1\\\\) and\\n\\n\\\\(q_2\\\\)\\nwhich follow normal distributions\\n\\n\\\\(N(-1, 1)\\\\) and\\n\\n\\\\(N(1, 1)\\\\)\\nrespectively.\\n\\npytorch\\n\\nmxnet\\n\\ntensorflow\\n\\ntorch\\n\\nmanual_seed\\n\\ntensor_len\\n\\n10000\\n\\ntorch\\n\\nnormal\\n\\ntensor_len\\n\\n))\\n\\nq1\\n\\ntorch\\n\\nnormal\\n\\ntensor_len\\n\\n))\\n\\nq2\\n\\ntorch\\n\\nnormal\\n\\ntensor_len\\n\\n))\\n\\ntorch\\n\\nsort\\n\\n)[\\n\\nq1\\n\\ntorch\\n\\nsort\\n\\nq1\\n\\n)[\\n\\nq2\\n\\ntorch\\n\\nsort\\n\\nq2\\n\\n)[\\n\\nrandom\\n\\nseed\\n\\nnd_len\\n\\n10000\\n\\nnp\\n\\nrandom\\n\\nnormal\\n\\nloc\\n\\nscale', metadata={'source': './22.11_information-theory.html'}), Document(page_content='q1\\n\\ntorch\\n\\nnormal\\n\\ntensor_len\\n\\n))\\n\\nq2\\n\\ntorch\\n\\nnormal\\n\\ntensor_len\\n\\n))\\n\\ntorch\\n\\nsort\\n\\n)[\\n\\nq1\\n\\ntorch\\n\\nsort\\n\\nq1\\n\\n)[\\n\\nq2\\n\\ntorch\\n\\nsort\\n\\nq2\\n\\n)[\\n\\nrandom\\n\\nseed\\n\\nnd_len\\n\\n10000\\n\\nnp\\n\\nrandom\\n\\nnormal\\n\\nloc\\n\\nscale\\n\\nsize\\n\\nnd_len\\n\\n))\\n\\nq1\\n\\nnp\\n\\nrandom\\n\\nnormal\\n\\nloc\\n\\n=-\\n\\nscale\\n\\nsize\\n\\nnd_len\\n\\n))\\n\\nq2\\n\\nnp\\n\\nrandom\\n\\nnormal\\n\\nloc\\n\\nscale\\n\\nsize\\n\\nnd_len\\n\\n))\\n\\nnp\\n\\narray\\n\\nsorted\\n\\nasnumpy\\n\\n()))\\n\\nq1\\n\\nnp\\n\\narray\\n\\nsorted\\n\\nq1\\n\\nasnumpy\\n\\n()))\\n\\nq2\\n\\nnp\\n\\narray\\n\\nsorted\\n\\nq2\\n\\nasnumpy\\n\\n()))\\n\\ntensor_len\\n\\n10000\\n\\ntf\\n\\nrandom\\n\\nnormal\\n\\n((\\n\\ntensor_len\\n\\n),\\n\\nq1\\n\\ntf\\n\\nrandom\\n\\nnormal\\n\\n((\\n\\ntensor_len\\n\\n),\\n\\nq2\\n\\ntf\\n\\nrandom\\n\\nnormal\\n\\n((\\n\\ntensor_len\\n\\n),\\n\\ntf\\n\\nsort\\n\\nq1\\n\\ntf\\n\\nsort\\n\\nq1\\n\\nq2\\n\\ntf\\n\\nsort\\n\\nq2\\n\\n\\\\(q_1\\\\) and\\n\\n\\\\(q_2\\\\) are symmetric with respect to the\\ny-axis (i.e.,\\n\\n\\\\(x=0\\\\)), we expect a similar value of KL divergence\\nbetween\\n\\n\\\\(D_{\\\\textrm{KL}}(p\\\\|q_1)\\\\) and\\n\\n\\\\(D_{\\\\textrm{KL}}(p\\\\|q_2)\\\\). As you can see below, there is only a\\nless than 3% off between\\n\\n\\\\(D_{\\\\textrm{KL}}(p\\\\|q_1)\\\\) and\\n\\n\\\\(D_{\\\\textrm{KL}}(p\\\\|q_2)\\\\).\\n\\npytorch\\n\\nmxnet\\n\\ntensorflow\\n\\nkl_pq1\\n\\nkl_divergence\\n\\nq1\\n\\nkl_pq2\\n\\nkl_divergence\\n\\nq2\\n\\nsimilar_percentage\\n\\nabs\\n\\nkl_pq1\\n\\nkl_pq2\\n\\n((\\n\\nkl_pq1\\n\\nkl_pq2\\n\\n100\\n\\nkl_pq1\\n\\nkl_pq2\\n\\nsimilar_percentage\\n\\n8582.0341796875\\n\\n8828.3095703125\\n\\n2.8290698237936858\\n\\nkl_pq1\\n\\nkl_divergence\\n\\nq1\\n\\nkl_pq2\\n\\nkl_divergence\\n\\nq2\\n\\nsimilar_percentage\\n\\nabs\\n\\nkl_pq1\\n\\nkl_pq2\\n\\n((\\n\\nkl_pq1\\n\\nkl_pq2\\n\\n100\\n\\nkl_pq1\\n\\nkl_pq2\\n\\nsimilar_percentage\\n\\n8470.638\\n\\n8664.998\\n\\n2.268492904612395\\n\\nkl_pq1\\n\\nkl_divergence\\n\\nq1\\n\\nkl_pq2\\n\\nkl_divergence\\n\\nq2\\n\\nsimilar_percentage\\n\\nabs\\n\\nkl_pq1\\n\\nkl_pq2\\n\\n((\\n\\nkl_pq1\\n\\nkl_pq2\\n\\n100\\n\\nkl_pq1\\n\\nkl_pq2\\n\\nsimilar_percentage\\n\\n8652.75\\n\\n8690.211\\n\\n0.43200163611047165\\n\\nIn contrast, you may find that \\\\(D_{\\\\textrm{KL}}(q_2 \\\\|p)\\\\) and\\n\\\\(D_{\\\\textrm{KL}}(p \\\\| q_2)\\\\) are off a lot, with around 40% off as\\nshown below.\\n\\npytorch\\n\\nmxnet\\n\\ntensorflow\\n\\nkl_q2p\\n\\nkl_divergence\\n\\nq2\\n\\ndiffer_percentage\\n\\nabs\\n\\nkl_q2p\\n\\nkl_pq2\\n\\n((\\n\\nkl_q2p\\n\\nkl_pq2\\n\\n100\\n\\nkl_q2p\\n\\ndiffer_percentage\\n\\n14130.125\\n\\n46.18621024399691\\n\\nkl_q2p\\n\\nkl_divergence\\n\\nq2\\n\\ndiffer_percentage\\n\\nabs\\n\\nkl_q2p\\n\\nkl_pq2\\n\\n((\\n\\nkl_q2p\\n\\nkl_pq2\\n\\n100\\n\\nkl_q2p\\n\\ndiffer_percentage\\n\\n13536.835\\n\\n43.88680093791528\\n\\nkl_q2p\\n\\nkl_divergence\\n\\nq2\\n\\ndiffer_percentage\\n\\nabs\\n\\nkl_q2p\\n\\nkl_pq2\\n\\n((\\n\\nkl_q2p\\n\\nkl_pq2\\n\\n100\\n\\nkl_q2p\\n\\ndiffer_percentage\\n\\n13416.902\\n\\n42.761724211717066\\n\\n22.11.5. Cross-Entropy¶\\n\\nIf you are curious about applications of information theory in deep\\nlearning, here is a quick example. We define the true distribution\\n\\\\(P\\\\) with probability distribution \\\\(p(x)\\\\), and the estimated\\ndistribution \\\\(Q\\\\) with probability distribution \\\\(q(x)\\\\), and\\nwe will use them in the rest of this section.\\n\\n\\\\(n\\\\) data examples {\\n\\n\\\\(x_1, \\\\ldots, x_n\\\\)}. Assume that we\\nencode\\n\\n\\\\(1\\\\) and\\n\\n\\\\(0\\\\) as the positive and negative class label\\n\\n\\\\(y_i\\\\) respectively, and our neural network is parametrized by\\n\\n\\\\(\\\\theta\\\\). If we aim to find a best\\n\\n\\\\(\\\\theta\\\\) so that\\n\\n\\\\(\\\\hat{y}_i= p_{\\\\theta}(y_i \\\\mid x_i)\\\\), it is natural to apply the\\nmaximum log-likelihood approach as was seen in\\n\\nSection 22.7. To be specific, for true labels\\n\\n\\\\(y_i\\\\) and predictions\\n\\n\\\\(\\\\hat{y}_i= p_{\\\\theta}(y_i \\\\mid x_i)\\\\),\\nthe probability to be classified as positive is\\n\\n\\\\(\\\\pi_i= p_{\\\\theta}(y_i = 1 \\\\mid x_i)\\\\). Hence, the log-likelihood\\nfunction would be\\n\\n(22.11.24)¶\\\\[\\\\begin{split}\\\\begin{aligned}\\nl(\\\\theta) &= \\\\log L(\\\\theta) \\\\\\\\\\n  &= \\\\log \\\\prod_{i=1}^n \\\\pi_i^{y_i} (1 - \\\\pi_i)^{1 - y_i} \\\\\\\\\\n  &= \\\\sum_{i=1}^n y_i \\\\log(\\\\pi_i) + (1 - y_i) \\\\log (1 - \\\\pi_i). \\\\\\\\\\n\\\\end{aligned}\\\\end{split}\\\\]\\n\\n\\\\(l(\\\\theta)\\\\) is identical to\\nminimizing\\n\\n\\\\(- l(\\\\theta)\\\\), and hence we can find the best\\n\\n\\\\(\\\\theta\\\\) from here. To generalize the above loss to any\\ndistributions, we also called\\n\\n\\\\(-l(\\\\theta)\\\\) the\\n\\n\\\\(\\\\textrm{CE}(y, \\\\hat{y})\\\\), where\\n\\n\\\\(y\\\\) follows the true\\ndistribution\\n\\n\\\\(P\\\\) and\\n\\n\\\\(\\\\hat{y}\\\\) follows the estimated\\ndistribution\\n\\n\\\\(Q\\\\).', metadata={'source': './22.11_information-theory.html'}), Document(page_content='\\\\(-l(\\\\theta)\\\\) the\\n\\n\\\\(\\\\textrm{CE}(y, \\\\hat{y})\\\\), where\\n\\n\\\\(y\\\\) follows the true\\ndistribution\\n\\n\\\\(P\\\\) and\\n\\n\\\\(\\\\hat{y}\\\\) follows the estimated\\ndistribution\\n\\n\\\\(Q\\\\).\\n\\nThis was all derived by working from the maximum likelihood point of\\nview. However, if we look closely we can see that terms like\\n\\\\(\\\\log(\\\\pi_i)\\\\) have entered into our computation which is a solid\\nindication that we can understand the expression from an information\\ntheoretic point of view.\\n\\n22.11.5.1. Formal Definition¶\\n\\nLike KL divergence, for a random variable \\\\(X\\\\), we can also measure\\nthe divergence between the estimating distribution \\\\(Q\\\\) and the\\ntrue distribution \\\\(P\\\\) via cross-entropy,\\n\\n(22.11.25)¶\\\\[\\\\textrm{CE}(P, Q) = - E_{x \\\\sim P} [\\\\log(q(x))].\\\\]\\n\\nBy using properties of entropy discussed above, we can also interpret it\\nas the summation of the entropy \\\\(H(P)\\\\) and the KL divergence\\nbetween \\\\(P\\\\) and \\\\(Q\\\\), i.e.,\\n\\n(22.11.26)¶\\\\[\\\\textrm{CE} (P, Q) = H(P) + D_{\\\\textrm{KL}}(P\\\\|Q).\\\\]\\n\\nWe can implement the cross-entropy loss as below.\\n\\npytorch\\n\\nmxnet\\n\\ntensorflow\\n\\ndef\\n\\ncross_entropy\\n\\ny_hat\\n\\n):\\n\\nce\\n\\ntorch\\n\\nlog\\n\\ny_hat\\n\\nrange\\n\\nlen\\n\\ny_hat\\n\\n)),\\n\\n])\\n\\nreturn\\n\\nce\\n\\nmean\\n\\n()\\n\\ndef\\n\\ncross_entropy\\n\\ny_hat\\n\\n):\\n\\nce\\n\\nnp\\n\\nlog\\n\\ny_hat\\n\\nrange\\n\\nlen\\n\\ny_hat\\n\\n)),\\n\\n])\\n\\nreturn\\n\\nce\\n\\nmean\\n\\n()\\n\\ndef\\n\\ncross_entropy\\n\\ny_hat\\n\\n):\\n\\n# `tf.gather_nd` is used to select specific indices of a tensor.\\n\\nce\\n\\ntf\\n\\nmath\\n\\nlog\\n\\ntf\\n\\ngather_nd\\n\\ny_hat\\n\\nindices\\n\\n[[\\n\\nfor\\n\\nin\\n\\nzip\\n\\nrange\\n\\nlen\\n\\ny_hat\\n\\n)),\\n\\n)]))\\n\\nreturn\\n\\ntf\\n\\nreduce_mean\\n\\nce\\n\\nnumpy\\n\\n()\\n\\nNow define two tensors for the labels and predictions, and calculate the\\ncross-entropy loss of them.\\n\\npytorch\\n\\nmxnet\\n\\ntensorflow\\n\\nlabels\\n\\ntorch\\n\\ntensor\\n\\n([\\n\\n])\\n\\npreds\\n\\ntorch\\n\\ntensor\\n\\n([[\\n\\n0.3\\n\\n0.6\\n\\n0.1\\n\\n],\\n\\n0.2\\n\\n0.3\\n\\n0.5\\n\\n]])\\n\\ncross_entropy\\n\\npreds\\n\\nlabels\\n\\ntensor\\n\\n0.9486\\n\\nlabels\\n\\nnp\\n\\narray\\n\\n([\\n\\n])\\n\\npreds\\n\\nnp\\n\\narray\\n\\n([[\\n\\n0.3\\n\\n0.6\\n\\n0.1\\n\\n],\\n\\n0.2\\n\\n0.3\\n\\n0.5\\n\\n]])\\n\\ncross_entropy\\n\\npreds\\n\\nlabels\\n\\narray\\n\\n0.94856\\n\\nlabels\\n\\ntf\\n\\nconstant\\n\\n([\\n\\n])\\n\\npreds\\n\\ntf\\n\\nconstant\\n\\n([[\\n\\n0.3\\n\\n0.6\\n\\n0.1\\n\\n],\\n\\n0.2\\n\\n0.3\\n\\n0.5\\n\\n]])\\n\\ncross_entropy\\n\\npreds\\n\\nlabels\\n\\n0.94856\\n\\n22.11.5.2. Properties¶\\n\\nAs alluded in the beginning of this section, cross-entropy\\n(22.11.25) can be used to define a loss function in the\\noptimization problem. It turns out that the following are equivalent:\\n\\nMaximizing predictive probability of \\\\(Q\\\\) for distribution\\n\\\\(P\\\\), (i.e., \\\\(E_{x \\\\sim P} [\\\\log (q(x))]\\\\));\\n\\nMinimizing cross-entropy \\\\(\\\\textrm{CE} (P, Q)\\\\);\\n\\nMinimizing the KL divergence \\\\(D_{\\\\textrm{KL}}(P\\\\|Q)\\\\).\\n\\nThe definition of cross-entropy indirectly proves the equivalent\\nrelationship between objective 2 and objective 3, as long as the entropy\\nof true data \\\\(H(P)\\\\) is constant.\\n\\n22.11.5.3. Cross-Entropy as An Objective Function of Multi-class Classification¶\\n\\nIf we dive deep into the classification objective function with\\ncross-entropy loss \\\\(\\\\textrm{CE}\\\\), we will find minimizing\\n\\\\(\\\\textrm{CE}\\\\) is equivalent to maximizing the log-likelihood\\nfunction \\\\(L\\\\).\\n\\n\\\\(n\\\\)\\nexamples, and it can be classified into\\n\\n\\\\(k\\\\)-classes. For each data\\nexample\\n\\n\\\\(i\\\\), we represent any\\n\\n\\\\(k\\\\)-class label\\n\\n\\\\(\\\\mathbf{y}_i = (y_{i1}, \\\\ldots, y_{ik})\\\\) by\\n\\n\\\\(i\\\\) belongs to class\\n\\n\\\\(j\\\\),\\nthen we set the\\n\\n\\\\(j\\\\)-th entry to\\n\\n\\\\(1\\\\), and all other\\ncomponents to\\n\\n\\\\(0\\\\), i.e.,\\n\\n(22.11.27)¶\\\\[\\\\begin{split}y_{ij} = \\\\begin{cases}1 & j \\\\in J; \\\\\\\\ 0 &\\\\textrm{otherwise.}\\\\end{cases}\\\\end{split}\\\\]\\n\\nFor instance, if a multi-class classification problem contains three\\nclasses \\\\(A\\\\), \\\\(B\\\\), and \\\\(C\\\\), then the labels\\n\\\\(\\\\mathbf{y}_i\\\\) can be encoded in\\n{\\\\(A: (1, 0, 0); B: (0, 1, 0); C: (0, 0, 1)\\\\)}.\\n\\nAssume that our neural network is parametrized by \\\\(\\\\theta\\\\). For\\ntrue label vectors \\\\(\\\\mathbf{y}_i\\\\) and predictions\\n\\n(22.11.28)¶\\\\[\\\\hat{\\\\mathbf{y}}_i= p_{\\\\theta}(\\\\mathbf{y}_i \\\\mid \\\\mathbf{x}_i) = \\\\sum_{j=1}^k y_{ij} p_{\\\\theta} (y_{ij}  \\\\mid  \\\\mathbf{x}_i).\\\\]\\n\\nHence, the cross-entropy loss would be', metadata={'source': './22.11_information-theory.html'}), Document(page_content=\"(22.11.28)¶\\\\[\\\\hat{\\\\mathbf{y}}_i= p_{\\\\theta}(\\\\mathbf{y}_i \\\\mid \\\\mathbf{x}_i) = \\\\sum_{j=1}^k y_{ij} p_{\\\\theta} (y_{ij}  \\\\mid  \\\\mathbf{x}_i).\\\\]\\n\\nHence, the cross-entropy loss would be\\n\\n(22.11.29)¶\\\\[\\\\begin{split}\\\\textrm{CE}(\\\\mathbf{y}, \\\\hat{\\\\mathbf{y}}) = - \\\\sum_{i=1}^n \\\\mathbf{y}_i \\\\log \\\\hat{\\\\mathbf{y}}_i\\n = - \\\\sum_{i=1}^n \\\\sum_{j=1}^k y_{ij} \\\\log{p_{\\\\theta} (y_{ij}  \\\\mid  \\\\mathbf{x}_i)}.\\\\\\\\\\\\end{split}\\\\]\\n\\n\\\\(k\\\\)-class multinoulli distribution. It is an extension of the\\nBernoulli distribution from binary class to multi-class. If a random\\nvariable\\n\\n\\\\(\\\\mathbf{z} = (z_{1}, \\\\ldots, z_{k})\\\\) follows a\\n\\n\\\\(k\\\\)-class\\n\\n\\\\(\\\\mathbf{p} =\\\\) (\\n\\n\\\\(p_{1}, \\\\ldots, p_{k}\\\\)), i.e.,\\n\\n(22.11.30)¶\\\\[p(\\\\mathbf{z}) = p(z_1, \\\\ldots, z_k) = \\\\textrm{Multi} (p_1, \\\\ldots, p_k), \\\\textrm{ where } \\\\sum_{i=1}^k p_i = 1,\\\\]\\n\\nthen the joint probability mass function(p.m.f.) of \\\\(\\\\mathbf{z}\\\\)\\nis\\n\\n(22.11.31)¶\\\\[\\\\mathbf{p}^\\\\mathbf{z} = \\\\prod_{j=1}^k p_{j}^{z_{j}}.\\\\]\\n\\n\\\\(\\\\mathbf{y}_i\\\\), is following a\\n\\n\\\\(k\\\\)-class multinoulli\\ndistribution with probabilities\\n\\n\\\\(\\\\boldsymbol{\\\\pi} =\\\\)\\n(\\n\\n\\\\(\\\\pi_{1}, \\\\ldots, \\\\pi_{k}\\\\)). Therefore, the joint p.m.f. of each\\ndata example\\n\\n\\\\(\\\\mathbf{y}_i\\\\) is\\n\\n\\\\(\\\\mathbf{\\\\pi}^{\\\\mathbf{y}_i} = \\\\prod_{j=1}^k \\\\pi_{j}^{y_{ij}}.\\\\)\\nHence, the log-likelihood function would be\\n\\n(22.11.32)¶\\\\[\\\\begin{split}\\\\begin{aligned}\\nl(\\\\theta)\\n = \\\\log L(\\\\theta)\\n = \\\\log \\\\prod_{i=1}^n \\\\boldsymbol{\\\\pi}^{\\\\mathbf{y}_i}\\n = \\\\log \\\\prod_{i=1}^n \\\\prod_{j=1}^k \\\\pi_{j}^{y_{ij}}\\n = \\\\sum_{i=1}^n \\\\sum_{j=1}^k y_{ij} \\\\log{\\\\pi_{j}}.\\\\\\\\\\n\\\\end{aligned}\\\\end{split}\\\\]\\n\\nSince in maximum likelihood estimation, we maximizing the objective\\nfunction \\\\(l(\\\\theta)\\\\) by having\\n\\\\(\\\\pi_{j} = p_{\\\\theta} (y_{ij} \\\\mid \\\\mathbf{x}_i)\\\\). Therefore, for\\nany multi-class classification, maximizing the above log-likelihood\\nfunction \\\\(l(\\\\theta)\\\\) is equivalent to minimizing the CE loss\\n\\\\(\\\\textrm{CE}(y, \\\\hat{y})\\\\).\\n\\nTo test the above proof, let’s apply the built-in measure\\nNegativeLogLikelihood. Using the same labels and preds as in\\nthe earlier example, we will get the same numerical loss as the previous\\nexample up to the 5 decimal place.\\n\\npytorch\\n\\nmxnet\\n\\ntensorflow\\n\\n# Implementation of cross-entropy loss in PyTorch combines `nn.LogSoftmax()`\\n\\n# and `nn.NLLLoss()`\\n\\nnll_loss\\n\\nNLLLoss\\n\\n()\\n\\nloss\\n\\nnll_loss\\n\\ntorch\\n\\nlog\\n\\npreds\\n\\n),\\n\\nlabels\\n\\nloss\\n\\ntensor\\n\\n0.9486\\n\\nnll_loss\\n\\nNegativeLogLikelihood\\n\\n()\\n\\nnll_loss\\n\\nupdate\\n\\nlabels\\n\\nas_nd_ndarray\\n\\n(),\\n\\npreds\\n\\nas_nd_ndarray\\n\\n())\\n\\nnll_loss\\n\\nget\\n\\n()\\n\\n'nll-loss'\\n\\n0.9485599994659424\\n\\ndef\\n\\nnll_loss\\n\\ny_hat\\n\\n):\\n\\n# Convert labels to one-hot vectors.\\n\\ntf\\n\\nkeras\\n\\nutils\\n\\nto_categorical\\n\\nnum_classes\\n\\ny_hat\\n\\nshape\\n\\n])\\n\\n# We will not calculate negative log-likelihood from the definition.\\n\\n# Rather, we will follow a circular argument. Because NLL is same as\\n\\n# `cross_entropy`, if we calculate cross_entropy that would give us NLL\\n\\ncross_entropy\\n\\ntf\\n\\nkeras\\n\\nlosses\\n\\nCategoricalCrossentropy\\n\\nfrom_logits\\n\\nTrue\\n\\nreduction\\n\\ntf\\n\\nkeras\\n\\nlosses\\n\\nReduction\\n\\nNONE\\n\\nreturn\\n\\ntf\\n\\nreduce_mean\\n\\ncross_entropy\\n\\ny_hat\\n\\n))\\n\\nnumpy\\n\\n()\\n\\nloss\\n\\nnll_loss\\n\\ntf\\n\\nmath\\n\\nlog\\n\\npreds\\n\\n),\\n\\nlabels\\n\\nloss\\n\\n0.94856\\n\\n22.11.6. Summary¶\\n\\nInformation theory is a field of study about encoding, decoding,\\ntransmitting, and manipulating information.\\n\\nEntropy is the unit to measure how much information is presented in\\ndifferent signals.\\n\\nKL divergence can also measure the divergence between two\\ndistributions.\\n\\nCross-entropy can be viewed as an objective function of multi-class\\nclassification. Minimizing cross-entropy loss is equivalent to\\nmaximizing the log-likelihood function.\\n\\n22.11.7. Exercises¶\\n\\nVerify that the card examples from the first section indeed have the\\nclaimed entropy.\\n\\nShow that the KL divergence \\\\(D(p\\\\|q)\\\\) is nonnegative for all\\ndistributions \\\\(p\\\\) and \\\\(q\\\\). Hint: use Jensen’s inequality,\\ni.e., use the fact that \\\\(-\\\\log x\\\\) is a convex function.\\n\\nLet’s compute the entropy from a few data sources:\", metadata={'source': './22.11_information-theory.html'}), Document(page_content='Let’s compute the entropy from a few data sources:\\n\\nAssume that you are watching the output generated by a monkey at a\\ntypewriter. The monkey presses any of the \\\\(44\\\\) keys of the\\ntypewriter at random (you can assume that it has not discovered\\nany special keys or the shift key yet). How many bits of\\nrandomness per character do you observe?\\nBeing unhappy with the monkey, you replaced it by a drunk\\ntypesetter. It is able to generate words, albeit not coherently.\\nInstead, it picks a random word out of a vocabulary of\\n\\\\(2,000\\\\) words. Let’s assume that the average length of a\\nword is \\\\(4.5\\\\) letters in English. How many bits of\\nrandomness per character do you observe now?\\nStill being unhappy with the result, you replace the typesetter by\\na high quality language model. The language model can currently\\nobtain a perplexity as low as \\\\(15\\\\) points per word. The\\ncharacter perplexity of a language model is defined as the\\ninverse of the geometric mean of a set of probabilities, each\\nprobability is corresponding to a character in the word. To be\\nspecific, if the length of a given word is \\\\(l\\\\), then\\n\\\\(\\\\textrm{PPL}(\\\\textrm{word}) = \\\\left[\\\\prod_i p(\\\\textrm{character}_i)\\\\right]^{ -\\\\frac{1}{l}} = \\\\exp \\\\left[ - \\\\frac{1}{l} \\\\sum_i{\\\\log p(\\\\textrm{character}_i)} \\\\right].\\\\)\\nAssume that the test word has 4.5 letters, how many bits of\\nrandomness per character do you observe now?\\n\\nExplain intuitively why \\\\(I(X, Y) = H(X) - H(X \\\\mid Y)\\\\). Then,\\nshow this is true by expressing both sides as an expectation with\\nrespect to the joint distribution.\\n\\nWhat is the KL Divergence between the two Gaussian distributions\\n\\\\(\\\\mathcal{N}(\\\\mu_1, \\\\sigma_1^2)\\\\) and\\n\\\\(\\\\mathcal{N}(\\\\mu_2, \\\\sigma_2^2)\\\\)?\\n\\npytorch\\n\\nmxnet\\n\\ntensorflow\\n\\nDiscussions\\n\\nDiscussions\\n\\nDiscussions\\n\\nTable Of Contents\\n\\n22.11. Information Theory\\n22.11.1. Information\\n22.11.1.1. Self-information\\n\\n\\n22.11.2. Entropy\\n22.11.2.1. Motivating Entropy\\n22.11.2.2. Definition\\n22.11.2.3. Interpretations\\n22.11.2.4. Properties of Entropy\\n\\n\\n22.11.3. Mutual Information\\n22.11.3.1. Joint Entropy\\n22.11.3.2. Conditional Entropy\\n22.11.3.3. Mutual Information\\n22.11.3.4. Properties of Mutual Information\\n22.11.3.5. Pointwise Mutual Information\\n22.11.3.6. Applications of Mutual Information\\n\\n\\n22.11.4. Kullback–Leibler Divergence\\n22.11.4.1. Definition\\n22.11.4.2. KL Divergence Properties\\n22.11.4.3. Example\\n\\n\\n22.11.5. Cross-Entropy\\n22.11.5.1. Formal Definition\\n22.11.5.2. Properties\\n22.11.5.3. Cross-Entropy as An Objective Function of Multi-class Classification\\n\\n\\n22.11.6. Summary\\n22.11.7. Exercises\\n\\nPrevious\\n            22.10. Statistics\\n\\nNext\\n            23. Appendix: Tools for Deep Learning', metadata={'source': './22.11_information-theory.html'})]\n",
      "Starting SIModelInfOp!\n",
      "SIModelInfOp Complete!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<uniflow.node.node.Node at 0x1540cf3a0>]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Initiate flow\n",
    "flow = SelfInstructedGenFlow()\n",
    "\n",
    "# Run flow\n",
    "input_dict = {constants.HTML_KEY: html_file}\n",
    "output_dict = flow(input_dict)\n",
    "\n",
    "print(f\"root node name of flow is {flow.root.name}\\n\\n\")\n",
    "print(f\"output_dict keys: {output_dict.keys()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a395d3",
   "metadata": {},
   "source": [
    "### Print out the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1666d84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of output nodes\n",
    "len(output_dict[constants.OUTPUT_NAME])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5e719d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output dictionary keys\n",
    "output_dict[constants.OUTPUT_NAME][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aaea2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the first 50 entries in the generated question-answer pairs.\n",
    "output_dict[constants.OUTPUT_NAME][0][constants.QAPAIR_DF_KEY][:50]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "self-instruct-ft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
