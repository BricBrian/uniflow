{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\".\")\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/uniflow/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "from uniflow.transform.client import Client\n",
    "from uniflow.transform.config import TransformOpenAIConfig\n",
    "from uniflow.model.config import OpenAIModelConfig\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from uniflow.schema import Context, GuidedPrompt\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_file = \"README.md\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/uniflow/example/transform/README.md\n"
     ]
    }
   ],
   "source": [
    "dir_cur = os.getcwd()\n",
    "# input_file = \"../../\" + markdown_file\n",
    "input_file = os.path.join(f\"{dir_cur}\", markdown_file)\n",
    "print(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_str = \"\"\n",
    "with open(input_file, 'r') as file:\n",
    "   markdown_str = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Examples\\n## Base Config\\nThe base `Config` is the base configuration that all other configurations inherit from. Here are the default parameters:\\n\\n| Parameter | Type | Default | Description |\\n| --- | --- | --- | --- |\\n| `flow_name` | `str` | [ModelFlow] | The name of the flow to run. |\\n| `guided_prompt_template` | `GuidedPrompt` | [Default](../../README.md#2-prompting) | The template to use for the guided prompt. |\\n| `num_threads` | `int` | 1 | The number of threads to use. |\\n| `model_config` | `ModelConfig` | `ModelConfig` | The model configuration to use. |\\n\\nHere are the default parameters for the `ModelConfig`:\\n\\n| Parameter | Type | Default | Description |\\n| --- | --- | --- | --- |\\n| `model_name` | `str` | `gpt-3.5-turbo-1106` | The name of the model to use. |\\n\\nThe [model.ipynb notebook](./model.ipynb) shows a basic example of how to use the base `Config`, where it also passes the `OpenAIModelConfig` as a `model_config` argument.\\n\\n## OpenAIConfig\\nThe `OpenAIConfig` configuration runs the following default parameters:\\n\\n| Parameter | Type | Default | Description |\\n| --- | --- | --- | --- |\\n| `flow_name` | `str` | `OpenAIModelFlow` | The name of the flow to run. |\\n| `guided_prompt_template` | `GuidedPrompt` | [Default](../../README.md#2-prompting) | The template to use for the guided prompt. |\\n| `num_threads` | `int` | 1 | The number of threads to use. |\\n| `model_config` | `ModelConfig` | `OpenAIModelConfig` | The model configuration to use. |\\n\\nHere are the default parameters for the `OpenAIModelConfig`:\\n\\n| Parameter | Type | Default | Description |\\n| --- | --- | --- | --- |\\n| `model_name` | `str` | `gpt-3.5-turbo-1106` | The name of the model to use. |\\n| `num_call` | `int` | 1 | The number of calls to make to the OpenAI model |\\n| `temperature` | `float` | 1.5 | The temperature to use for the OpenAI model. |\\n| `response_format` | `Dict[str, str]` | {\"type\": \"text\"} | The response format to use for the OpenAI model. |\\n\\nSee the [openai_json_model.ipynb notebook](./openai_json_model.ipynb) for a working example.\\n\\n## HuggingfaceConfig\\nThe `HuggingfaceConfig` configuration has the following default parameters:\\n\\n| Parameter | Type | Default | Description |\\n| --- | --- | --- | --- |\\n| `flow_name` | `str` | [HuggingfaceModelFlow](../../README.md#model) | The name of the flow to run. |\\n| `guided_prompt_template` | `GuidedPrompt` | [Default](../../README.md#2-prompting) | The template to use for the guided prompt. |\\n| `num_threads` | `int` | 1 | The number of threads to use. |\\n| `model_config` | `ModelConfig` | `HuggingfaceModelConfig` | The model configuration to use. |\\n\\nHere are the default parameters for the `HuggingfaceModelConfig`:\\n\\n| Parameter | Type | Default | Description |\\n| --- | --- | --- | --- |\\n| `model_name` | `str` | `mistralai/Mistral-7B-Instruct-v0.1` | The name of the model to use. |\\n| `batch_size` | `int` | 1 | The batch size to use for the Huggingface model. |\\n\\nSee the [huggingface_model.ipynb notebook](./huggingface_model.ipynb) for a working example.\\n\\n## LMQGModelConfig\\nThe `LMQGModelConfig` configuration runs with the following default parameters:\\n\\n| Parameter | Type | Default | Description |\\n| --- | --- | --- | --- |\\n| `flow_name` | `str` | `LMQGModelFlow` | The name of the flow to run. |\\n| `guided_prompt_template` | `GuidedPrompt` | [Default](../../README.md#2-prompting) | The template to use for the guided prompt. |\\n| `num_threads` | `int` | 1 | The number of threads to use. |\\n| `model_config` | `ModelConfig` | `LMQGModelConfig` | The model configuration to use. |\\n\\nHere are the default parameters for the `LMQGModelConfig`:\\n\\n| Parameter | Type | Default | Description |\\n| --- | --- | --- | --- |\\n| `model_name` | `str` | `lmqg/t5-base-squad-qg-ae` | The name of the model to use. |\\n| `batch_size` | `int` | 1 | The batch size to use for the LMQG model. |\\n\\nSee the [lmqg_model.ipynb notebook](./lmqg_model.ipynb) for a working example.\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markdown_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import (Dict, List, TypedDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeaderType(TypedDict):\n",
    "    \"\"\"Header type as typed dict.\"\"\"\n",
    "\n",
    "    level: int\n",
    "    name: str\n",
    "    data: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LineType(TypedDict):\n",
    "    \"\"\"Line type as typed dict.\"\"\"\n",
    "\n",
    "    metadata: Dict[str, str]\n",
    "    content: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "    (\"####\", \"Header 4\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def markdown_splitter(markdown_str, headers_to_split_on=headers_to_split_on):\n",
    "    # Final output\n",
    "    lines_with_metadata: List[LineType] = []\n",
    "    # Content and metadata of the chunk currently being processed\n",
    "    current_content: List[str] = []\n",
    "    current_metadata: Dict[str, str] = {}\n",
    "    # Keep track of the nested header structure\n",
    "    header_stack: List[HeaderType] = []\n",
    "    initial_metadata: Dict[str, str] = {}\n",
    "\n",
    "    markdown_str += \"\\n# end\"\n",
    "    lines = markdown_str.split(\"\\n\")\n",
    "\n",
    "    for line in lines:\n",
    "        stripped_line = line.strip()\n",
    "\n",
    "        for sep, name in headers_to_split_on:\n",
    "            # Check if line starts with a header that we intend to split on\n",
    "            if stripped_line.startswith(sep) and (\n",
    "                # Header with no text OR header is followed by space\n",
    "                # Both are valid conditions that sep is being used a header\n",
    "                len(stripped_line) == len(sep) or stripped_line[len(sep)] == \" \"\n",
    "            ):\n",
    "                # Ensure we are tracking the header as metadata\n",
    "                if name is not None:\n",
    "                    # Get the current header level\n",
    "                    current_header_level = sep.count(\"#\")\n",
    "\n",
    "                    # Pop out headers of lower or same level from the stack\n",
    "                    while (\n",
    "                        header_stack\n",
    "                        and header_stack[-1][\"level\"] >= current_header_level\n",
    "                    ):\n",
    "                        # We have encountered a new header\n",
    "                        # at the same or higher level\n",
    "                        popped_header = header_stack.pop()\n",
    "                        # Clear the metadata for the\n",
    "                        # popped header in initial_metadata\n",
    "                        if popped_header[\"name\"] in initial_metadata:\n",
    "                            initial_metadata.pop(popped_header[\"name\"])\n",
    "\n",
    "                    # Push the current header to the stack\n",
    "                    header: HeaderType = {\n",
    "                        \"level\": current_header_level,\n",
    "                        \"name\": name,\n",
    "                        \"data\": stripped_line[len(sep) :].strip(),\n",
    "                    }\n",
    "                    header_stack.append(header)\n",
    "                    # Update initial_metadata with the current header\n",
    "                    initial_metadata[name] = header[\"data\"]\n",
    "\n",
    "                # Add the previous line to the lines_with_metadata\n",
    "                # only if current_content is not empty\n",
    "                if current_content:\n",
    "                    lines_with_metadata.append(\n",
    "                        {\n",
    "                            \"content\": \"\\n\".join(current_content),\n",
    "                            \"metadata\": current_metadata.copy(),\n",
    "                        }\n",
    "                    )\n",
    "                    current_content.clear()\n",
    "\n",
    "                break\n",
    "            else:\n",
    "                if stripped_line:\n",
    "                    current_content.append(stripped_line)\n",
    "                    current_metadata = initial_metadata.copy()\n",
    "    \n",
    "    return lines_with_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': '## Base Config', 'metadata': {'Header 1': 'Examples'}},\n",
       " {'content': 'The base `Config` is the base configuration that all other configurations inherit from. Here are the default parameters:\\nThe base `Config` is the base configuration that all other configurations inherit from. Here are the default parameters:\\nThe base `Config` is the base configuration that all other configurations inherit from. Here are the default parameters:\\nThe base `Config` is the base configuration that all other configurations inherit from. Here are the default parameters:\\n| Parameter | Type | Default | Description |\\n| Parameter | Type | Default | Description |\\n| Parameter | Type | Default | Description |\\n| Parameter | Type | Default | Description |\\n| --- | --- | --- | --- |\\n| --- | --- | --- | --- |\\n| --- | --- | --- | --- |\\n| --- | --- | --- | --- |\\n| `flow_name` | `str` | [ModelFlow] | The name of the flow to run. |\\n| `flow_name` | `str` | [ModelFlow] | The name of the flow to run. |\\n| `flow_name` | `str` | [ModelFlow] | The name of the flow to run. |\\n| `flow_name` | `str` | [ModelFlow] | The name of the flow to run. |\\n| `guided_prompt_template` | `GuidedPrompt` | [Default](../../README.md#2-prompting) | The template to use for the guided prompt. |\\n| `guided_prompt_template` | `GuidedPrompt` | [Default](../../README.md#2-prompting) | The template to use for the guided prompt. |\\n| `guided_prompt_template` | `GuidedPrompt` | [Default](../../README.md#2-prompting) | The template to use for the guided prompt. |\\n| `guided_prompt_template` | `GuidedPrompt` | [Default](../../README.md#2-prompting) | The template to use for the guided prompt. |\\n| `num_threads` | `int` | 1 | The number of threads to use. |\\n| `num_threads` | `int` | 1 | The number of threads to use. |\\n| `num_threads` | `int` | 1 | The number of threads to use. |\\n| `num_threads` | `int` | 1 | The number of threads to use. |\\n| `model_config` | `ModelConfig` | `ModelConfig` | The model configuration to use. |\\n| `model_config` | `ModelConfig` | `ModelConfig` | The model configuration to use. |\\n| `model_config` | `ModelConfig` | `ModelConfig` | The model configuration to use. |\\n| `model_config` | `ModelConfig` | `ModelConfig` | The model configuration to use. |\\nHere are the default parameters for the `ModelConfig`:\\nHere are the default parameters for the `ModelConfig`:\\nHere are the default parameters for the `ModelConfig`:\\nHere are the default parameters for the `ModelConfig`:\\n| Parameter | Type | Default | Description |\\n| Parameter | Type | Default | Description |\\n| Parameter | Type | Default | Description |\\n| Parameter | Type | Default | Description |\\n| --- | --- | --- | --- |\\n| --- | --- | --- | --- |\\n| --- | --- | --- | --- |\\n| --- | --- | --- | --- |\\n| `model_name` | `str` | `gpt-3.5-turbo-1106` | The name of the model to use. |\\n| `model_name` | `str` | `gpt-3.5-turbo-1106` | The name of the model to use. |\\n| `model_name` | `str` | `gpt-3.5-turbo-1106` | The name of the model to use. |\\n| `model_name` | `str` | `gpt-3.5-turbo-1106` | The name of the model to use. |\\nThe [model.ipynb notebook](./model.ipynb) shows a basic example of how to use the base `Config`, where it also passes the `OpenAIModelConfig` as a `model_config` argument.\\nThe [model.ipynb notebook](./model.ipynb) shows a basic example of how to use the base `Config`, where it also passes the `OpenAIModelConfig` as a `model_config` argument.\\nThe [model.ipynb notebook](./model.ipynb) shows a basic example of how to use the base `Config`, where it also passes the `OpenAIModelConfig` as a `model_config` argument.\\nThe [model.ipynb notebook](./model.ipynb) shows a basic example of how to use the base `Config`, where it also passes the `OpenAIModelConfig` as a `model_config` argument.\\n## OpenAIConfig',\n",
       "  'metadata': {'Header 1': 'Examples', 'Header 2': 'Base Config'}},\n",
       " {'content': 'The `OpenAIConfig` configuration runs the following default parameters:\\nThe `OpenAIConfig` configuration runs the following default parameters:\\nThe `OpenAIConfig` configuration runs the following default parameters:\\nThe `OpenAIConfig` configuration runs the following default parameters:\\n| Parameter | Type | Default | Description |\\n| Parameter | Type | Default | Description |\\n| Parameter | Type | Default | Description |\\n| Parameter | Type | Default | Description |\\n| --- | --- | --- | --- |\\n| --- | --- | --- | --- |\\n| --- | --- | --- | --- |\\n| --- | --- | --- | --- |\\n| `flow_name` | `str` | `OpenAIModelFlow` | The name of the flow to run. |\\n| `flow_name` | `str` | `OpenAIModelFlow` | The name of the flow to run. |\\n| `flow_name` | `str` | `OpenAIModelFlow` | The name of the flow to run. |\\n| `flow_name` | `str` | `OpenAIModelFlow` | The name of the flow to run. |\\n| `guided_prompt_template` | `GuidedPrompt` | [Default](../../README.md#2-prompting) | The template to use for the guided prompt. |\\n| `guided_prompt_template` | `GuidedPrompt` | [Default](../../README.md#2-prompting) | The template to use for the guided prompt. |\\n| `guided_prompt_template` | `GuidedPrompt` | [Default](../../README.md#2-prompting) | The template to use for the guided prompt. |\\n| `guided_prompt_template` | `GuidedPrompt` | [Default](../../README.md#2-prompting) | The template to use for the guided prompt. |\\n| `num_threads` | `int` | 1 | The number of threads to use. |\\n| `num_threads` | `int` | 1 | The number of threads to use. |\\n| `num_threads` | `int` | 1 | The number of threads to use. |\\n| `num_threads` | `int` | 1 | The number of threads to use. |\\n| `model_config` | `ModelConfig` | `OpenAIModelConfig` | The model configuration to use. |\\n| `model_config` | `ModelConfig` | `OpenAIModelConfig` | The model configuration to use. |\\n| `model_config` | `ModelConfig` | `OpenAIModelConfig` | The model configuration to use. |\\n| `model_config` | `ModelConfig` | `OpenAIModelConfig` | The model configuration to use. |\\nHere are the default parameters for the `OpenAIModelConfig`:\\nHere are the default parameters for the `OpenAIModelConfig`:\\nHere are the default parameters for the `OpenAIModelConfig`:\\nHere are the default parameters for the `OpenAIModelConfig`:\\n| Parameter | Type | Default | Description |\\n| Parameter | Type | Default | Description |\\n| Parameter | Type | Default | Description |\\n| Parameter | Type | Default | Description |\\n| --- | --- | --- | --- |\\n| --- | --- | --- | --- |\\n| --- | --- | --- | --- |\\n| --- | --- | --- | --- |\\n| `model_name` | `str` | `gpt-3.5-turbo-1106` | The name of the model to use. |\\n| `model_name` | `str` | `gpt-3.5-turbo-1106` | The name of the model to use. |\\n| `model_name` | `str` | `gpt-3.5-turbo-1106` | The name of the model to use. |\\n| `model_name` | `str` | `gpt-3.5-turbo-1106` | The name of the model to use. |\\n| `num_call` | `int` | 1 | The number of calls to make to the OpenAI model |\\n| `num_call` | `int` | 1 | The number of calls to make to the OpenAI model |\\n| `num_call` | `int` | 1 | The number of calls to make to the OpenAI model |\\n| `num_call` | `int` | 1 | The number of calls to make to the OpenAI model |\\n| `temperature` | `float` | 1.5 | The temperature to use for the OpenAI model. |\\n| `temperature` | `float` | 1.5 | The temperature to use for the OpenAI model. |\\n| `temperature` | `float` | 1.5 | The temperature to use for the OpenAI model. |\\n| `temperature` | `float` | 1.5 | The temperature to use for the OpenAI model. |\\n| `response_format` | `Dict[str, str]` | {\"type\": \"text\"} | The response format to use for the OpenAI model. |\\n| `response_format` | `Dict[str, str]` | {\"type\": \"text\"} | The response format to use for the OpenAI model. |\\n| `response_format` | `Dict[str, str]` | {\"type\": \"text\"} | The response format to use for the OpenAI model. |\\n| `response_format` | `Dict[str, str]` | {\"type\": \"text\"} | The response format to use for the OpenAI model. |\\nSee the [openai_json_model.ipynb notebook](./openai_json_model.ipynb) for a working example.\\nSee the [openai_json_model.ipynb notebook](./openai_json_model.ipynb) for a working example.\\nSee the [openai_json_model.ipynb notebook](./openai_json_model.ipynb) for a working example.\\nSee the [openai_json_model.ipynb notebook](./openai_json_model.ipynb) for a working example.\\n## HuggingfaceConfig',\n",
       "  'metadata': {'Header 1': 'Examples', 'Header 2': 'OpenAIConfig'}},\n",
       " {'content': 'The `HuggingfaceConfig` configuration has the following default parameters:\\nThe `HuggingfaceConfig` configuration has the following default parameters:\\nThe `HuggingfaceConfig` configuration has the following default parameters:\\nThe `HuggingfaceConfig` configuration has the following default parameters:\\n| Parameter | Type | Default | Description |\\n| Parameter | Type | Default | Description |\\n| Parameter | Type | Default | Description |\\n| Parameter | Type | Default | Description |\\n| --- | --- | --- | --- |\\n| --- | --- | --- | --- |\\n| --- | --- | --- | --- |\\n| --- | --- | --- | --- |\\n| `flow_name` | `str` | [HuggingfaceModelFlow](../../README.md#model) | The name of the flow to run. |\\n| `flow_name` | `str` | [HuggingfaceModelFlow](../../README.md#model) | The name of the flow to run. |\\n| `flow_name` | `str` | [HuggingfaceModelFlow](../../README.md#model) | The name of the flow to run. |\\n| `flow_name` | `str` | [HuggingfaceModelFlow](../../README.md#model) | The name of the flow to run. |\\n| `guided_prompt_template` | `GuidedPrompt` | [Default](../../README.md#2-prompting) | The template to use for the guided prompt. |\\n| `guided_prompt_template` | `GuidedPrompt` | [Default](../../README.md#2-prompting) | The template to use for the guided prompt. |\\n| `guided_prompt_template` | `GuidedPrompt` | [Default](../../README.md#2-prompting) | The template to use for the guided prompt. |\\n| `guided_prompt_template` | `GuidedPrompt` | [Default](../../README.md#2-prompting) | The template to use for the guided prompt. |\\n| `num_threads` | `int` | 1 | The number of threads to use. |\\n| `num_threads` | `int` | 1 | The number of threads to use. |\\n| `num_threads` | `int` | 1 | The number of threads to use. |\\n| `num_threads` | `int` | 1 | The number of threads to use. |\\n| `model_config` | `ModelConfig` | `HuggingfaceModelConfig` | The model configuration to use. |\\n| `model_config` | `ModelConfig` | `HuggingfaceModelConfig` | The model configuration to use. |\\n| `model_config` | `ModelConfig` | `HuggingfaceModelConfig` | The model configuration to use. |\\n| `model_config` | `ModelConfig` | `HuggingfaceModelConfig` | The model configuration to use. |\\nHere are the default parameters for the `HuggingfaceModelConfig`:\\nHere are the default parameters for the `HuggingfaceModelConfig`:\\nHere are the default parameters for the `HuggingfaceModelConfig`:\\nHere are the default parameters for the `HuggingfaceModelConfig`:\\n| Parameter | Type | Default | Description |\\n| Parameter | Type | Default | Description |\\n| Parameter | Type | Default | Description |\\n| Parameter | Type | Default | Description |\\n| --- | --- | --- | --- |\\n| --- | --- | --- | --- |\\n| --- | --- | --- | --- |\\n| --- | --- | --- | --- |\\n| `model_name` | `str` | `mistralai/Mistral-7B-Instruct-v0.1` | The name of the model to use. |\\n| `model_name` | `str` | `mistralai/Mistral-7B-Instruct-v0.1` | The name of the model to use. |\\n| `model_name` | `str` | `mistralai/Mistral-7B-Instruct-v0.1` | The name of the model to use. |\\n| `model_name` | `str` | `mistralai/Mistral-7B-Instruct-v0.1` | The name of the model to use. |\\n| `batch_size` | `int` | 1 | The batch size to use for the Huggingface model. |\\n| `batch_size` | `int` | 1 | The batch size to use for the Huggingface model. |\\n| `batch_size` | `int` | 1 | The batch size to use for the Huggingface model. |\\n| `batch_size` | `int` | 1 | The batch size to use for the Huggingface model. |\\nSee the [huggingface_model.ipynb notebook](./huggingface_model.ipynb) for a working example.\\nSee the [huggingface_model.ipynb notebook](./huggingface_model.ipynb) for a working example.\\nSee the [huggingface_model.ipynb notebook](./huggingface_model.ipynb) for a working example.\\nSee the [huggingface_model.ipynb notebook](./huggingface_model.ipynb) for a working example.\\n## LMQGModelConfig',\n",
       "  'metadata': {'Header 1': 'Examples', 'Header 2': 'HuggingfaceConfig'}},\n",
       " {'content': 'The `LMQGModelConfig` configuration runs with the following default parameters:\\nThe `LMQGModelConfig` configuration runs with the following default parameters:\\nThe `LMQGModelConfig` configuration runs with the following default parameters:\\nThe `LMQGModelConfig` configuration runs with the following default parameters:\\n| Parameter | Type | Default | Description |\\n| Parameter | Type | Default | Description |\\n| Parameter | Type | Default | Description |\\n| Parameter | Type | Default | Description |\\n| --- | --- | --- | --- |\\n| --- | --- | --- | --- |\\n| --- | --- | --- | --- |\\n| --- | --- | --- | --- |\\n| `flow_name` | `str` | `LMQGModelFlow` | The name of the flow to run. |\\n| `flow_name` | `str` | `LMQGModelFlow` | The name of the flow to run. |\\n| `flow_name` | `str` | `LMQGModelFlow` | The name of the flow to run. |\\n| `flow_name` | `str` | `LMQGModelFlow` | The name of the flow to run. |\\n| `guided_prompt_template` | `GuidedPrompt` | [Default](../../README.md#2-prompting) | The template to use for the guided prompt. |\\n| `guided_prompt_template` | `GuidedPrompt` | [Default](../../README.md#2-prompting) | The template to use for the guided prompt. |\\n| `guided_prompt_template` | `GuidedPrompt` | [Default](../../README.md#2-prompting) | The template to use for the guided prompt. |\\n| `guided_prompt_template` | `GuidedPrompt` | [Default](../../README.md#2-prompting) | The template to use for the guided prompt. |\\n| `num_threads` | `int` | 1 | The number of threads to use. |\\n| `num_threads` | `int` | 1 | The number of threads to use. |\\n| `num_threads` | `int` | 1 | The number of threads to use. |\\n| `num_threads` | `int` | 1 | The number of threads to use. |\\n| `model_config` | `ModelConfig` | `LMQGModelConfig` | The model configuration to use. |\\n| `model_config` | `ModelConfig` | `LMQGModelConfig` | The model configuration to use. |\\n| `model_config` | `ModelConfig` | `LMQGModelConfig` | The model configuration to use. |\\n| `model_config` | `ModelConfig` | `LMQGModelConfig` | The model configuration to use. |\\nHere are the default parameters for the `LMQGModelConfig`:\\nHere are the default parameters for the `LMQGModelConfig`:\\nHere are the default parameters for the `LMQGModelConfig`:\\nHere are the default parameters for the `LMQGModelConfig`:\\n| Parameter | Type | Default | Description |\\n| Parameter | Type | Default | Description |\\n| Parameter | Type | Default | Description |\\n| Parameter | Type | Default | Description |\\n| --- | --- | --- | --- |\\n| --- | --- | --- | --- |\\n| --- | --- | --- | --- |\\n| --- | --- | --- | --- |\\n| `model_name` | `str` | `lmqg/t5-base-squad-qg-ae` | The name of the model to use. |\\n| `model_name` | `str` | `lmqg/t5-base-squad-qg-ae` | The name of the model to use. |\\n| `model_name` | `str` | `lmqg/t5-base-squad-qg-ae` | The name of the model to use. |\\n| `model_name` | `str` | `lmqg/t5-base-squad-qg-ae` | The name of the model to use. |\\n| `batch_size` | `int` | 1 | The batch size to use for the LMQG model. |\\n| `batch_size` | `int` | 1 | The batch size to use for the LMQG model. |\\n| `batch_size` | `int` | 1 | The batch size to use for the LMQG model. |\\n| `batch_size` | `int` | 1 | The batch size to use for the LMQG model. |\\nSee the [lmqg_model.ipynb notebook](./lmqg_model.ipynb) for a working example.\\nSee the [lmqg_model.ipynb notebook](./lmqg_model.ipynb) for a working example.\\nSee the [lmqg_model.ipynb notebook](./lmqg_model.ipynb) for a working example.\\nSee the [lmqg_model.ipynb notebook](./lmqg_model.ipynb) for a working example.',\n",
       "  'metadata': {'Header 1': 'Examples', 'Header 2': 'LMQGModelConfig'}}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markdown_splitter(markdown_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import MarkdownHeaderTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='The base `Config` is the base configuration that all other configurations inherit from. Here are the default parameters:  \\n| Parameter | Type | Default | Description |\\n| --- | --- | --- | --- |\\n| `flow_name` | `str` | [ModelFlow] | The name of the flow to run. |\\n| `guided_prompt_template` | `GuidedPrompt` | [Default](../../README.md#2-prompting) | The template to use for the guided prompt. |\\n| `num_threads` | `int` | 1 | The number of threads to use. |\\n| `model_config` | `ModelConfig` | `ModelConfig` | The model configuration to use. |  \\nHere are the default parameters for the `ModelConfig`:  \\n| Parameter | Type | Default | Description |\\n| --- | --- | --- | --- |\\n| `model_name` | `str` | `gpt-3.5-turbo-1106` | The name of the model to use. |  \\nThe [model.ipynb notebook](./model.ipynb) shows a basic example of how to use the base `Config`, where it also passes the `OpenAIModelConfig` as a `model_config` argument.', metadata={'Header 1': 'Examples', 'Header 2': 'Base Config'}),\n",
       " Document(page_content='The `OpenAIConfig` configuration runs the following default parameters:  \\n| Parameter | Type | Default | Description |\\n| --- | --- | --- | --- |\\n| `flow_name` | `str` | `OpenAIModelFlow` | The name of the flow to run. |\\n| `guided_prompt_template` | `GuidedPrompt` | [Default](../../README.md#2-prompting) | The template to use for the guided prompt. |\\n| `num_threads` | `int` | 1 | The number of threads to use. |\\n| `model_config` | `ModelConfig` | `OpenAIModelConfig` | The model configuration to use. |  \\nHere are the default parameters for the `OpenAIModelConfig`:  \\n| Parameter | Type | Default | Description |\\n| --- | --- | --- | --- |\\n| `model_name` | `str` | `gpt-3.5-turbo-1106` | The name of the model to use. |\\n| `num_call` | `int` | 1 | The number of calls to make to the OpenAI model |\\n| `temperature` | `float` | 1.5 | The temperature to use for the OpenAI model. |\\n| `response_format` | `Dict[str, str]` | {\"type\": \"text\"} | The response format to use for the OpenAI model. |  \\nSee the [openai_json_model.ipynb notebook](./openai_json_model.ipynb) for a working example.', metadata={'Header 1': 'Examples', 'Header 2': 'OpenAIConfig'}),\n",
       " Document(page_content='The `HuggingfaceConfig` configuration has the following default parameters:  \\n| Parameter | Type | Default | Description |\\n| --- | --- | --- | --- |\\n| `flow_name` | `str` | [HuggingfaceModelFlow](../../README.md#model) | The name of the flow to run. |\\n| `guided_prompt_template` | `GuidedPrompt` | [Default](../../README.md#2-prompting) | The template to use for the guided prompt. |\\n| `num_threads` | `int` | 1 | The number of threads to use. |\\n| `model_config` | `ModelConfig` | `HuggingfaceModelConfig` | The model configuration to use. |  \\nHere are the default parameters for the `HuggingfaceModelConfig`:  \\n| Parameter | Type | Default | Description |\\n| --- | --- | --- | --- |\\n| `model_name` | `str` | `mistralai/Mistral-7B-Instruct-v0.1` | The name of the model to use. |\\n| `batch_size` | `int` | 1 | The batch size to use for the Huggingface model. |  \\nSee the [huggingface_model.ipynb notebook](./huggingface_model.ipynb) for a working example.', metadata={'Header 1': 'Examples', 'Header 2': 'HuggingfaceConfig'}),\n",
       " Document(page_content='The `LMQGModelConfig` configuration runs with the following default parameters:  \\n| Parameter | Type | Default | Description |\\n| --- | --- | --- | --- |\\n| `flow_name` | `str` | `LMQGModelFlow` | The name of the flow to run. |\\n| `guided_prompt_template` | `GuidedPrompt` | [Default](../../README.md#2-prompting) | The template to use for the guided prompt. |\\n| `num_threads` | `int` | 1 | The number of threads to use. |\\n| `model_config` | `ModelConfig` | `LMQGModelConfig` | The model configuration to use. |  \\nHere are the default parameters for the `LMQGModelConfig`:  \\n| Parameter | Type | Default | Description |\\n| --- | --- | --- | --- |\\n| `model_name` | `str` | `lmqg/t5-base-squad-qg-ae` | The name of the model to use. |\\n| `batch_size` | `int` | 1 | The batch size to use for the LMQG model. |  \\nSee the [lmqg_model.ipynb notebook](./lmqg_model.ipynb) for a working example.', metadata={'Header 1': 'Examples', 'Header 2': 'LMQGModelConfig'})]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "md_header_splits = markdown_splitter.split_text(markdown_str)\n",
    "md_header_splits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uniflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
